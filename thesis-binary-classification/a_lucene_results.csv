"1","1","359","44","Another non-reproducing failure, from my Jenkins:

Checking out Revision 85a27a231fdddb118ee178baac170da0097a02c0 (refs/remotes/origin/master)
[...]
   [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(7.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=61A674A498110EC0 -Dtests.slow=true -Dtests.locale=ja-JP -Dtests.timezone=Greenwich -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] FAILURE 0.07s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f0, reader=_4(7.0.0):c19 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([61A674A498110EC0:575A168B19E46DDC]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=false): {}, locale=ja-JP, timezone=Greenwich
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=220134656,total=306184192
   [junit4]   2> NOTE: All tests run in this JVM: [TestStringMSBRadixSorter, TestSpanTermQuery, TestOmitPositions, TestIndexableField, TestHighCompressionMode, TestDeterminizeLexicon, TestPackedTokenAttributeImpl, TestTopDocsCollector, TestIndexOrDocValuesQuery, TestDocValuesRewriteMethod, TestDocument, TestCrash, TestWildcardRandom, TestDocIdSetBuilder, TestFilterLeafReader, TestMergedIterator, TestMultiThreadTermVectors, TestAtomicUpdate, TestNorms, Test4GBStoredFields, TestFixedLengthBytesRefArray, TestFieldInvertState, TestBoolean2ScorerSupplier, TestLevenshteinAutomata, TestGraphTokenStreamFiniteStrings, TestStandardAnalyzer, TestSegmentReader, TestScorerPerf, TestBoostQuery, TestMergePolicyWrapper, TestComplexExplanations, TestPointQueries, TestMixedCodecs, TestPointValues, TestMultiMMap, TestLazyProxSkipping, TestTerms, TestIndexWriterThreadsToSegments, TestFilterWeight, TestDocumentsWriterDeleteQueue, TestCharFilter, TestDocInverterPerFieldErrorInfo, TestSimilarityProvider, LimitedFiniteStringsIteratorTest, TestNewestSegment, TestFSTs, TestClassicSimilarity, TestUnicodeUtil, TestQueryBuilder, TestSwappedIndexFiles, TestTimSorterWorstCase, TestBinaryDocValuesUpdates]

","Another non-reproducing failure, from my Jenkins:

Checking out Revision 85a27a231fdddb118ee178baac170da0097a02c0 (refs/remotes/origin/master)
[...]
   [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(7.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=61A674A498110EC0 -Dtests.slow=true -Dtests.locale=ja-JP -Dtests.timezone=Greenwich -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] FAILURE 0.07s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f0, reader=_4(7.0.0):c19 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([61A674A498110EC0:575A168B19E46DDC]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=false): {}, locale=ja-JP, timezone=Greenwich
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=220134656,total=306184192
   [junit4]   2> NOTE: All tests run in this JVM: [TestStringMSBRadixSorter, TestSpanTermQuery, TestOmitPositions, TestIndexableField, TestHighCompressionMode, TestDeterminizeLexicon, TestPackedTokenAttributeImpl, TestTopDocsCollector, TestIndexOrDocValuesQuery, TestDocValuesRewriteMethod, TestDocument, TestCrash, TestWildcardRandom, TestDocIdSetBuilder, TestFilterLeafReader, TestMergedIterator, TestMultiThreadTermVectors, TestAtomicUpdate, TestNorms, Test4GBStoredFields, TestFixedLengthBytesRefArray, TestFieldInvertState, TestBoolean2ScorerSupplier, TestLevenshteinAutomata, TestGraphTokenStreamFiniteStrings, TestStandardAnalyzer, TestSegmentReader, TestScorerPerf, TestBoostQuery, TestMergePolicyWrapper, TestComplexExplanations, TestPointQueries, TestMixedCodecs, TestPointValues, TestMultiMMap, TestLazyProxSkipping, TestTerms, TestIndexWriterThreadsToSegments, TestFilterWeight, TestDocumentsWriterDeleteQueue, TestCharFilter, TestDocInverterPerFieldErrorInfo, TestSimilarityProvider, LimitedFiniteStringsIteratorTest, TestNewestSegment, TestFSTs, TestClassicSimilarity, TestUnicodeUtil, TestQueryBuilder, TestSwappedIndexFiles, TestTimSorterWorstCase, TestBinaryDocValuesUpdates]","steve_rowe","NULL","1","issue","1","0","0","0","0"
"2","2","360","44","Another non-reproducing failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/19961/ (log - and commit sha - no longer available; the notification email arrived on June 24 at 10:37PM):

[...]
  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
  [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(7.0.0):C2)
  [junit4]   1> TEST: now reopen
  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=3A4BC284D906CE1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=hr-HR -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8
  [junit4] FAILURE 0.65s J2 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
  [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f4, reader=_28(7.0.0):C936 expected:<12> but was:<11>
  [junit4]    > 	at __randomizedtesting.SeedInfo.seed([3A4BC284D906CE1A:CB7A0AB58F3AD06]:0)
  [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
  [junit4]    > 	at java.lang.Thread.run(Thread.java:748)
  [junit4] IGNOR/A 0.00s J2 | TestBinaryDocValuesUpdates.testTonsOfUpdates
  [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
  [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, maxDocsPerChunk=1, blockSize=26), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, blockSize=26)), sim=RandomSimilarity(queryNorm=false): {}, locale=hr-HR, timezone=Pacific/Pitcairn
  [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=302943312,total=518979584
  [junit4]   2> NOTE: All tests run in this JVM: [TestDirectPacked, TestFieldCacheRewriteMethod, TestBagOfPostings, TestEarlyTermination, TestReaderWrapperDVTypeCheck, TestNeedsScores, TestRoaringDocIdSet, TestShardSearching, TestSpansEnum, TestSegmentTermEnum, TestLongPostings, TestIndexReaderClose, TestLucene70NormsFormat, TestReqExclBulkScorer, TestField, TestSegmentTermDocs, TestSimilarityBase, TestGeoEncodingUtils, TestPayloadsOnVectors, TestCharTermAttributeImpl, TestDisjunctionMaxQuery, TestTermRangeQuery, TestLongValuesSource, TestCachingTokenFilter, TestOfflineSorter, TestTopDocsCollector, TestBufferedIndexInput, TestTermScorer, TestPerFieldPostingsFormat2, TestConsistentFieldNumbers, TestFieldsReader, TestConjunctions, TestSloppyPhraseQuery2, TestSetOnce, TestRollingUpdates, TestIndexWriterLockRelease, TestIndexWriterMergePolicy, TestRollingBuffer, TestBinaryDocument, TestSimpleFSLockFactory, TestIndexingSequenceNumbers, FiniteStringsIteratorTest, TestGraphTokenStreamFiniteStrings, TestSentinelIntSet, TestHugeRamFile, TestSortedNumericSortField, TestMultiCollector, TestSpanNotQuery, TestAllFilesHaveCodecHeader, TestTrackingDirectoryWrapper, TestControlledRealTimeReopenThread, TestDirectoryReader, TestDocValues, TestDoubleRangeFieldQueries, TestSpanCollection, TestDemoParallelLeafReader, TestSpans, TestTerms, Test2BBinaryDocValues, TestParallelCompositeReader, TestArrayUtil, TestPrefixQuery, TestAttributeSource, TestByteBlockPool, TestCompiledAutomaton, TestSimpleExplanationsOfNonMatches, TestDocValuesScoring, TestExceedMaxTermLength, TestNRTThreads, TestLazyProxSkipping, TestSimilarity2, TestSearchWithThreads, TestPolygon2D, TestGrowableByteArrayDataOutput, TestIndexCommit, TestBasics, TestSearcherManager, TestNorms, TestStandardAnalyzer, TestTopDocsMerge, TestMinimize, TestNRTReaderWithThreads, TestIndexWriterForceMerge, TestPerFieldPostingsFormat, TestCollectionUtil, TestFastDecompressionMode, TestSort, TestMultiDocValues, TestCustomSearcherSort, TestTermsEnum2, Test2BDocs, TestMixedCodecs, TestSpanExplanations, TestFastCompressionMode, TestStressIndexing2, TestMultiPhraseQuery, TestDeterminism, TestMergeSchedulerExternal, TestForceMergeForever, TestSameScoresWithThreads, TestMultiFields, TestLiveFieldValues, TestSpanSearchEquivalence, TestPayloads, TestDoc, TestFieldMaskingSpanQuery, TestExternalCodecs, TestRegexpQuery, TestIntBlockPool, TestComplexExplanationsOfNonMatches, TestParallelReaderEmptyIndex, TestDocument, TestFileSwitchDirectory, TestDirectory, TestRegexpRandom, TestMultiLevelSkipList, TestCheckIndex, TestBooleanQueryVisitSubscorers, TestMatchAllDocsQuery, TestSubScorerFreqs, TestIndexWriterConfig, TestPositionIncrement, TestSpanExplanationsOfNonMatches, TestFilterLeafReader, TestSameTokenSamePosition, TestAutomatonQueryUnicode, TestRamUsageEstimator, TestSpanFirstQuery, TestIsCurrent, TestNoMergePolicy, TestNoMergeScheduler, TestNamedSPILoader, TestBytesRef, TestCharFilter, TestTwoPhaseCommitTool, TestCloseableThreadLocal, TestVersion, TestReaderClosed, TestNGramPhraseQuery, TestIntsRef, Test2BPositions, Test2BPostingsBytes, Test2BTerms, TestByteArrayDataInput, Test2BPagedBytes, TestCharArraySet, TestDelegatingAnalyzerWrapper, TestStopFilter, TestBlockPostingsFormat, TestLucene50TermVectorsFormat, Test2BSortedDocValuesOrds, TestAllFilesCheckIndexHeader, TestAllFilesHaveChecksumFooter, TestBinaryDocValuesUpdates]
  [junit4] Completed [362/453 (1!)] on J2 in 6.22s, 29 tests, 1 failure, 1 skipped <<< FAILURES!

","Another non-reproducing failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/19961/ (log - and commit sha - no longer available; the notification email arrived on June 24 at 10:37PM):

[...]
  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
  [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(7.0.0):C2)
  [junit4]   1> TEST: now reopen
  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=3A4BC284D906CE1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=hr-HR -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8
  [junit4] FAILURE 0.65s J2 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
  [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f4, reader=_28(7.0.0):C936 expected:<12> but was:<11>
  [junit4]    > 	at __randomizedtesting.SeedInfo.seed([3A4BC284D906CE1A:CB7A0AB58F3AD06]:0)
  [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
  [junit4]    > 	at java.lang.Thread.run(Thread.java:748)
  [junit4] IGNOR/A 0.00s J2 | TestBinaryDocValuesUpdates.testTonsOfUpdates
  [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
  [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, maxDocsPerChunk=1, blockSize=26), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, blockSize=26)), sim=RandomSimilarity(queryNorm=false): {}, locale=hr-HR, timezone=Pacific/Pitcairn
  [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=302943312,total=518979584
  [junit4]   2> NOTE: All tests run in this JVM: [TestDirectPacked, TestFieldCacheRewriteMethod, TestBagOfPostings, TestEarlyTermination, TestReaderWrapperDVTypeCheck, TestNeedsScores, TestRoaringDocIdSet, TestShardSearching, TestSpansEnum, TestSegmentTermEnum, TestLongPostings, TestIndexReaderClose, TestLucene70NormsFormat, TestReqExclBulkScorer, TestField, TestSegmentTermDocs, TestSimilarityBase, TestGeoEncodingUtils, TestPayloadsOnVectors, TestCharTermAttributeImpl, TestDisjunctionMaxQuery, TestTermRangeQuery, TestLongValuesSource, TestCachingTokenFilter, TestOfflineSorter, TestTopDocsCollector, TestBufferedIndexInput, TestTermScorer, TestPerFieldPostingsFormat2, TestConsistentFieldNumbers, TestFieldsReader, TestConjunctions, TestSloppyPhraseQuery2, TestSetOnce, TestRollingUpdates, TestIndexWriterLockRelease, TestIndexWriterMergePolicy, TestRollingBuffer, TestBinaryDocument, TestSimpleFSLockFactory, TestIndexingSequenceNumbers, FiniteStringsIteratorTest, TestGraphTokenStreamFiniteStrings, TestSentinelIntSet, TestHugeRamFile, TestSortedNumericSortField, TestMultiCollector, TestSpanNotQuery, TestAllFilesHaveCodecHeader, TestTrackingDirectoryWrapper, TestControlledRealTimeReopenThread, TestDirectoryReader, TestDocValues, TestDoubleRangeFieldQueries, TestSpanCollection, TestDemoParallelLeafReader, TestSpans, TestTerms, Test2BBinaryDocValues, TestParallelCompositeReader, TestArrayUtil, TestPrefixQuery, TestAttributeSource, TestByteBlockPool, TestCompiledAutomaton, TestSimpleExplanationsOfNonMatches, TestDocValuesScoring, TestExceedMaxTermLength, TestNRTThreads, TestLazyProxSkipping, TestSimilarity2, TestSearchWithThreads, TestPolygon2D, TestGrowableByteArrayDataOutput, TestIndexCommit, TestBasics, TestSearcherManager, TestNorms, TestStandardAnalyzer, TestTopDocsMerge, TestMinimize, TestNRTReaderWithThreads, TestIndexWriterForceMerge, TestPerFieldPostingsFormat, TestCollectionUtil, TestFastDecompressionMode, TestSort, TestMultiDocValues, TestCustomSearcherSort, TestTermsEnum2, Test2BDocs, TestMixedCodecs, TestSpanExplanations, TestFastCompressionMode, TestStressIndexing2, TestMultiPhraseQuery, TestDeterminism, TestMergeSchedulerExternal, TestForceMergeForever, TestSameScoresWithThreads, TestMultiFields, TestLiveFieldValues, TestSpanSearchEquivalence, TestPayloads, TestDoc, TestFieldMaskingSpanQuery, TestExternalCodecs, TestRegexpQuery, TestIntBlockPool, TestComplexExplanationsOfNonMatches, TestParallelReaderEmptyIndex, TestDocument, TestFileSwitchDirectory, TestDirectory, TestRegexpRandom, TestMultiLevelSkipList, TestCheckIndex, TestBooleanQueryVisitSubscorers, TestMatchAllDocsQuery, TestSubScorerFreqs, TestIndexWriterConfig, TestPositionIncrement, TestSpanExplanationsOfNonMatches, TestFilterLeafReader, TestSameTokenSamePosition, TestAutomatonQueryUnicode, TestRamUsageEstimator, TestSpanFirstQuery, TestIsCurrent, TestNoMergePolicy, TestNoMergeScheduler, TestNamedSPILoader, TestBytesRef, TestCharFilter, TestTwoPhaseCommitTool, TestCloseableThreadLocal, TestVersion, TestReaderClosed, TestNGramPhraseQuery, TestIntsRef, Test2BPositions, Test2BPostingsBytes, Test2BTerms, TestByteArrayDataInput, Test2BPagedBytes, TestCharArraySet, TestDelegatingAnalyzerWrapper, TestStopFilter, TestBlockPostingsFormat, TestLucene50TermVectorsFormat, Test2BSortedDocValuesOrds, TestAllFilesCheckIndexHeader, TestAllFilesHaveChecksumFooter, TestBinaryDocValuesUpdates]
  [junit4] Completed [362/453 (1!)]","steve_rowe","NULL","1","issue","1","0","0","0","0"
"3","3","360","44","Another non-reproducing failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/19961/ (log - and commit sha - no longer available; the notification email arrived on June 24 at 10:37PM):

[...]
  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
  [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(7.0.0):C2)
  [junit4]   1> TEST: now reopen
  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=3A4BC284D906CE1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=hr-HR -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8
  [junit4] FAILURE 0.65s J2 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
  [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f4, reader=_28(7.0.0):C936 expected:<12> but was:<11>
  [junit4]    > 	at __randomizedtesting.SeedInfo.seed([3A4BC284D906CE1A:CB7A0AB58F3AD06]:0)
  [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
  [junit4]    > 	at java.lang.Thread.run(Thread.java:748)
  [junit4] IGNOR/A 0.00s J2 | TestBinaryDocValuesUpdates.testTonsOfUpdates
  [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
  [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, maxDocsPerChunk=1, blockSize=26), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, blockSize=26)), sim=RandomSimilarity(queryNorm=false): {}, locale=hr-HR, timezone=Pacific/Pitcairn
  [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=302943312,total=518979584
  [junit4]   2> NOTE: All tests run in this JVM: [TestDirectPacked, TestFieldCacheRewriteMethod, TestBagOfPostings, TestEarlyTermination, TestReaderWrapperDVTypeCheck, TestNeedsScores, TestRoaringDocIdSet, TestShardSearching, TestSpansEnum, TestSegmentTermEnum, TestLongPostings, TestIndexReaderClose, TestLucene70NormsFormat, TestReqExclBulkScorer, TestField, TestSegmentTermDocs, TestSimilarityBase, TestGeoEncodingUtils, TestPayloadsOnVectors, TestCharTermAttributeImpl, TestDisjunctionMaxQuery, TestTermRangeQuery, TestLongValuesSource, TestCachingTokenFilter, TestOfflineSorter, TestTopDocsCollector, TestBufferedIndexInput, TestTermScorer, TestPerFieldPostingsFormat2, TestConsistentFieldNumbers, TestFieldsReader, TestConjunctions, TestSloppyPhraseQuery2, TestSetOnce, TestRollingUpdates, TestIndexWriterLockRelease, TestIndexWriterMergePolicy, TestRollingBuffer, TestBinaryDocument, TestSimpleFSLockFactory, TestIndexingSequenceNumbers, FiniteStringsIteratorTest, TestGraphTokenStreamFiniteStrings, TestSentinelIntSet, TestHugeRamFile, TestSortedNumericSortField, TestMultiCollector, TestSpanNotQuery, TestAllFilesHaveCodecHeader, TestTrackingDirectoryWrapper, TestControlledRealTimeReopenThread, TestDirectoryReader, TestDocValues, TestDoubleRangeFieldQueries, TestSpanCollection, TestDemoParallelLeafReader, TestSpans, TestTerms, Test2BBinaryDocValues, TestParallelCompositeReader, TestArrayUtil, TestPrefixQuery, TestAttributeSource, TestByteBlockPool, TestCompiledAutomaton, TestSimpleExplanationsOfNonMatches, TestDocValuesScoring, TestExceedMaxTermLength, TestNRTThreads, TestLazyProxSkipping, TestSimilarity2, TestSearchWithThreads, TestPolygon2D, TestGrowableByteArrayDataOutput, TestIndexCommit, TestBasics, TestSearcherManager, TestNorms, TestStandardAnalyzer, TestTopDocsMerge, TestMinimize, TestNRTReaderWithThreads, TestIndexWriterForceMerge, TestPerFieldPostingsFormat, TestCollectionUtil, TestFastDecompressionMode, TestSort, TestMultiDocValues, TestCustomSearcherSort, TestTermsEnum2, Test2BDocs, TestMixedCodecs, TestSpanExplanations, TestFastCompressionMode, TestStressIndexing2, TestMultiPhraseQuery, TestDeterminism, TestMergeSchedulerExternal, TestForceMergeForever, TestSameScoresWithThreads, TestMultiFields, TestLiveFieldValues, TestSpanSearchEquivalence, TestPayloads, TestDoc, TestFieldMaskingSpanQuery, TestExternalCodecs, TestRegexpQuery, TestIntBlockPool, TestComplexExplanationsOfNonMatches, TestParallelReaderEmptyIndex, TestDocument, TestFileSwitchDirectory, TestDirectory, TestRegexpRandom, TestMultiLevelSkipList, TestCheckIndex, TestBooleanQueryVisitSubscorers, TestMatchAllDocsQuery, TestSubScorerFreqs, TestIndexWriterConfig, TestPositionIncrement, TestSpanExplanationsOfNonMatches, TestFilterLeafReader, TestSameTokenSamePosition, TestAutomatonQueryUnicode, TestRamUsageEstimator, TestSpanFirstQuery, TestIsCurrent, TestNoMergePolicy, TestNoMergeScheduler, TestNamedSPILoader, TestBytesRef, TestCharFilter, TestTwoPhaseCommitTool, TestCloseableThreadLocal, TestVersion, TestReaderClosed, TestNGramPhraseQuery, TestIntsRef, Test2BPositions, Test2BPostingsBytes, Test2BTerms, TestByteArrayDataInput, Test2BPagedBytes, TestCharArraySet, TestDelegatingAnalyzerWrapper, TestStopFilter, TestBlockPostingsFormat, TestLucene50TermVectorsFormat, Test2BSortedDocValuesOrds, TestAllFilesCheckIndexHeader, TestAllFilesHaveChecksumFooter, TestBinaryDocValuesUpdates]
  [junit4] Completed [362/453 (1!)] on J2 in 6.22s, 29 tests, 1 failure, 1 skipped <<< FAILURES!

","on J2 in 6.22s, 29 tests, 1 failure, 1 skipped <<< FAILURES!","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"4","4","361","44","I'll tackle this.","I'll tackle this.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"5","5","362","44","Commit eaf1d45a1cad74a1037c7c4178fd2379a903f8cc in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45 ]
LUCENE-7888: fix concurrency hazards between merge completing and DV updates applying","Commit eaf1d45a1cad74a1037c7c4178fd2379a903f8cc in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45 ]
LUCENE-7888: fix concurrency hazards between merge completing and DV updates applying","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"6","6","363","44","I think these should be fixed now.  It was a tricky concurrency hazard, where an indexing thread that's resolving DV updates thinks it's done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.
Thanks Steve Rowe.","I think these should be fixed now.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"7","7","363","44","I think these should be fixed now.  It was a tricky concurrency hazard, where an indexing thread that's resolving DV updates thinks it's done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.
Thanks Steve Rowe.","It was a tricky concurrency hazard, where an indexing thread that's resolving DV updates thinks it's done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.","mikemccand","NULL","1","issue","1","0","0","0","0"
"8","8","363","44","I think these should be fixed now.  It was a tricky concurrency hazard, where an indexing thread that's resolving DV updates thinks it's done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.
Thanks Steve Rowe.","Thanks Steve Rowe.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"9","9","364","44","Mike, any reason not to backport to branch_7x and branch_7_0?  There was a recent failure on a Jenkins branch_7x job https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/9/:

Checking out Revision 758cbd98a7aa020ad67aea775028badf0be6418c (refs/remotes/origin/branch_7x)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D57106AE532F4164 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=qu-PE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.44s J2 | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f2, reader=_y(7.1.0):C435:fieldInfosGen=2:dvGen=2 expected:<7> but was:<6>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D57106AE532F4164:E38D6481D2DA2278]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   [junit4]    > 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   [junit4]    > 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
   [junit4]    > 	at java.base/java.lang.Thread.run(Thread.java:844)
   [junit4] IGNOR/A 0.00s J2 | TestMixedDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: leaving temporary files on disk at: /home/jenkins/workspace/Lucene-Solr-7.x-Linux/lucene/build/core/test/J2/temp/lucene.index.TestMixedDocValuesUpdates_D57106AE532F4164-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=qu-PE, timezone=America/Yakutat
   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 9 (64-bit)/cpus=8,threads=1,free=165322832,total=342360064

","Mike, any reason not to backport to branch_7x and branch_7_0?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"10","10","364","44","Mike, any reason not to backport to branch_7x and branch_7_0?  There was a recent failure on a Jenkins branch_7x job https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/9/:

Checking out Revision 758cbd98a7aa020ad67aea775028badf0be6418c (refs/remotes/origin/branch_7x)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D57106AE532F4164 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=qu-PE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.44s J2 | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f2, reader=_y(7.1.0):C435:fieldInfosGen=2:dvGen=2 expected:<7> but was:<6>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D57106AE532F4164:E38D6481D2DA2278]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   [junit4]    > 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   [junit4]    > 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
   [junit4]    > 	at java.base/java.lang.Thread.run(Thread.java:844)
   [junit4] IGNOR/A 0.00s J2 | TestMixedDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: leaving temporary files on disk at: /home/jenkins/workspace/Lucene-Solr-7.x-Linux/lucene/build/core/test/J2/temp/lucene.index.TestMixedDocValuesUpdates_D57106AE532F4164-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=qu-PE, timezone=America/Yakutat
   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 9 (64-bit)/cpus=8,threads=1,free=165322832,total=342360064

","There was a recent failure on a Jenkins branch_7x job https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/9/:

Checking out Revision 758cbd98a7aa020ad67aea775028badf0be6418c (refs/remotes/origin/branch_7x)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D57106AE532F4164 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=qu-PE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.44s J2 | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f2, reader=_y(7.1.0):C435:fieldInfosGen=2:dvGen=2 expected:<7> but was:<6>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D57106AE532F4164:E38D6481D2DA2278]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   [junit4]    > 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   [junit4]    > 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
   [junit4]    > 	at java.base/java.lang.Thread.run(Thread.java:844)
   [junit4] IGNOR/A 0.00s J2 | TestMixedDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: leaving temporary files on disk at: /home/jenkins/workspace/Lucene-Solr-7.x-Linux/lucene/build/core/test/J2/temp/lucene.index.TestMixedDocValuesUpdates_D57106AE532F4164-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=qu-PE, timezone=America/Yakutat
   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 9 (64-bit)/cpus=8,threads=1,free=165322832,total=342360064","steve_rowe","NULL","1","issue","1","0","0","0","0"
"11","11","365","44","More non-reproducing master failures from my Jenkins, commit shas are all after Mike's commit on this issue:

Checking out Revision 48b4960e0c093b480b8328f324992a7006054f17 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=6A2FDBE9B9C2F59C -Dtests.slow=true -Dtests.locale=ar-YE -Dtests.timezone=Asia/Beirut -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.31s J1 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=63, field=f1, reader=_l(8.0.0):c88:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([6A2FDBE9B9C2F59C:5CD3B9C638379680]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4] IGNOR/A 0.00s J1 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, f=PostingsFormat(name=LuceneVarGapDocFreqInterval), k1=PostingsFormat(name=LuceneVarGapDocFreqInterval), dvUpdateKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), k2=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), foo=PostingsFormat(name=LuceneVarGapDocFreqInterval), upd=PostingsFormat(name=Direct), updKey=FSTOrd50, id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), key=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{val=DocValuesFormat(name=Lucene70), ndv=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Asserting), sdv=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Asserting), control=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), sort=DocValuesFormat(name=Asserting), f2=DocValuesFormat(name=Direct), cf0=DocValuesFormat(name=Lucene70), f3=DocValuesFormat(name=Lucene70), f4=DocValuesFormat(name=Asserting), f5=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Asserting), bdv2=DocValuesFormat(name=Asserting), number=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Lucene70), bdv=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Lucene70), key=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1142, maxMBSortInHeap=7.285443710546513, sim=RandomSimilarity(queryNorm=false): {}, locale=ar-YE, timezone=Asia/Beirut
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=296202568,total=395313152



Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=en-ZA -Dtests.timezone=America/Inuvik -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.89s J6  | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f3, reader=_o(8.0.0):c417:fieldInfosGen=2:dvGen=2 expected:<5> but was:<4>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestBinaryDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, k1=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), f=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), dvUpdateKey=FSTOrd50, foo=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), k2=PostingsFormat(name=LuceneFixedGap), upd=Lucene50(blocksize=128), updKey=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), id=PostingsFormat(name=LuceneFixedGap), key=PostingsFormat(name=LuceneFixedGap)}, docValues:{ndv=DocValuesFormat(name=Memory), f10=DocValuesFormat(name=Asserting), f12=DocValuesFormat(name=Direct), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Asserting), f13=DocValuesFormat(name=Memory), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Direct), f1=DocValuesFormat(name=Direct), f15=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Memory), f18=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Asserting), f17=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Direct), bdv2=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Memory), number=DocValuesFormat(name=Direct), f7=DocValuesFormat(name=Asserting), f8=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Asserting), f9=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct), val=DocValuesFormat(name=Asserting), f21=DocValuesFormat(name=Asserting), f20=DocValuesFormat(name=Memory), f23=DocValuesFormat(name=Direct), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Asserting), f24=DocValuesFormat(name=Memory), sort=DocValuesFormat(name=Lucene70), cf0=DocValuesFormat(name=Asserting), cf2=DocValuesFormat(name=Direct), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Asserting), cf3=DocValuesFormat(name=Memory), cf6=DocValuesFormat(name=Direct), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Asserting), cf7=DocValuesFormat(name=Memory), cf9=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Lucene70), sdv=DocValuesFormat(name=Asserting), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Memory), cf24=DocValuesFormat(name=Asserting), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Direct), cf20=DocValuesFormat(name=Asserting), key=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Direct), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Direct), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Memory), cf17=DocValuesFormat(name=Asserting), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Direct), control=DocValuesFormat(name=Asserting), cf12=DocValuesFormat(name=Memory), cf13=DocValuesFormat(name=Asserting), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Direct), bdv=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=74, maxMBSortInHeap=7.963102974639169, sim=RandomSimilarity(queryNorm=true): {}, locale=en-ZA, timezone=America/Inuvik
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=73914040,total=520617984



Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=pt-BR -Dtests.timezone=Africa/Porto-Novo -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.17s J6  | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f1, reader=_7(8.0.0):c118:fieldInfosGen=2:dvGen=2 expected:<3> but was:<2>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestMixedDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {upd=Lucene50(blocksize=128), updKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), id=PostingsFormat(name=LuceneVarGapFixedInterval), key=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{f10=DocValuesFormat(name=Memory), f12=DocValuesFormat(name=Lucene70), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Memory), f13=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Lucene70), f15=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Asserting), f18=DocValuesFormat(name=Memory), f17=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Asserting), f7=DocValuesFormat(name=Memory), f8=DocValuesFormat(name=Lucene70), f9=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Lucene70), f21=DocValuesFormat(name=Memory), f20=DocValuesFormat(name=Asserting), f23=DocValuesFormat(name=Lucene70), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Memory), f24=DocValuesFormat(name=Asserting), cf0=DocValuesFormat(name=Memory), cf2=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Memory), cf3=DocValuesFormat(name=Asserting), cf6=DocValuesFormat(name=Lucene70), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Memory), cf7=DocValuesFormat(name=Asserting), cf9=DocValuesFormat(name=Lucene70), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Asserting), cf24=DocValuesFormat(name=Memory), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Lucene70), cf20=DocValuesFormat(name=Memory), key=DocValuesFormat(name=Lucene70), cf=DocValuesFormat(name=Lucene70), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Asserting), cf17=DocValuesFormat(name=Memory), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Lucene70), cf12=DocValuesFormat(name=Asserting), cf13=DocValuesFormat(name=Memory), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1397, maxMBSortInHeap=7.760564666966891, sim=RandomSimilarity(queryNorm=false): {}, locale=pt-BR, timezone=Africa/Porto-Novo
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=283409888,total=478150656



Checking out Revision 6c163658bbca15b1e4ff81d16b25e07df78468e8 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=85C41F1E0BBEB082 -Dtests.slow=true -Dtests.locale=ca-ES -Dtests.timezone=Kwajalein -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.71s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f2, reader=_d(8.0.0):c55:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([85C41F1E0BBEB082:B3387D318A4BD39E]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=true): {}, locale=ca-ES, timezone=Kwajalein
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=312487000,total=502792192

","More non-reproducing master failures from my Jenkins, commit shas are all after Mike's commit on this issue:

Checking out Revision 48b4960e0c093b480b8328f324992a7006054f17 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=6A2FDBE9B9C2F59C -Dtests.slow=true -Dtests.locale=ar-YE -Dtests.timezone=Asia/Beirut -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.31s J1 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=63, field=f1, reader=_l(8.0.0):c88:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([6A2FDBE9B9C2F59C:5CD3B9C638379680]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4] IGNOR/A 0.00s J1 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, f=PostingsFormat(name=LuceneVarGapDocFreqInterval), k1=PostingsFormat(name=LuceneVarGapDocFreqInterval), dvUpdateKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), k2=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), foo=PostingsFormat(name=LuceneVarGapDocFreqInterval), upd=PostingsFormat(name=Direct), updKey=FSTOrd50, id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), key=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{val=DocValuesFormat(name=Lucene70), ndv=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Asserting), sdv=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Asserting), control=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), sort=DocValuesFormat(name=Asserting), f2=DocValuesFormat(name=Direct), cf0=DocValuesFormat(name=Lucene70), f3=DocValuesFormat(name=Lucene70), f4=DocValuesFormat(name=Asserting), f5=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Asserting), bdv2=DocValuesFormat(name=Asserting), number=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Lucene70), bdv=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Lucene70), key=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1142, maxMBSortInHeap=7.285443710546513, sim=RandomSimilarity(queryNorm=false): {}, locale=ar-YE, timezone=Asia/Beirut
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=296202568,total=395313152



Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=en-ZA -Dtests.timezone=America/Inuvik -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.89s J6  | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f3, reader=_o(8.0.0):c417:fieldInfosGen=2:dvGen=2 expected:<5> but was:<4>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestBinaryDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, k1=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), f=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), dvUpdateKey=FSTOrd50, foo=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), k2=PostingsFormat(name=LuceneFixedGap), upd=Lucene50(blocksize=128), updKey=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), id=PostingsFormat(name=LuceneFixedGap), key=PostingsFormat(name=LuceneFixedGap)}, docValues:{ndv=DocValuesFormat(name=Memory), f10=DocValuesFormat(name=Asserting), f12=DocValuesFormat(name=Direct), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Asserting), f13=DocValuesFormat(name=Memory), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Direct), f1=DocValuesFormat(name=Direct), f15=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Memory), f18=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Asserting), f17=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Direct), bdv2=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Memory), number=DocValuesFormat(name=Direct), f7=DocValuesFormat(name=Asserting), f8=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Asserting), f9=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct), val=DocValuesFormat(name=Asserting), f21=DocValuesFormat(name=Asserting), f20=DocValuesFormat(name=Memory), f23=DocValuesFormat(name=Direct), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Asserting), f24=DocValuesFormat(name=Memory), sort=DocValuesFormat(name=Lucene70), cf0=DocValuesFormat(name=Asserting), cf2=DocValuesFormat(name=Direct), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Asserting), cf3=DocValuesFormat(name=Memory), cf6=DocValuesFormat(name=Direct), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Asserting), cf7=DocValuesFormat(name=Memory), cf9=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Lucene70), sdv=DocValuesFormat(name=Asserting), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Memory), cf24=DocValuesFormat(name=Asserting), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Direct), cf20=DocValuesFormat(name=Asserting), key=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Direct), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Direct), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Memory), cf17=DocValuesFormat(name=Asserting), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Direct), control=DocValuesFormat(name=Asserting), cf12=DocValuesFormat(name=Memory), cf13=DocValuesFormat(name=Asserting), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Direct), bdv=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=74, maxMBSortInHeap=7.963102974639169, sim=RandomSimilarity(queryNorm=true): {}, locale=en-ZA, timezone=America/Inuvik
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=73914040,total=520617984



Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=pt-BR -Dtests.timezone=Africa/Porto-Novo -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.17s J6  | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f1, reader=_7(8.0.0):c118:fieldInfosGen=2:dvGen=2 expected:<3> but was:<2>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestMixedDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {upd=Lucene50(blocksize=128), updKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), id=PostingsFormat(name=LuceneVarGapFixedInterval), key=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{f10=DocValuesFormat(name=Memory), f12=DocValuesFormat(name=Lucene70), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Memory), f13=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Lucene70), f15=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Asserting), f18=DocValuesFormat(name=Memory), f17=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Asserting), f7=DocValuesFormat(name=Memory), f8=DocValuesFormat(name=Lucene70), f9=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Lucene70), f21=DocValuesFormat(name=Memory), f20=DocValuesFormat(name=Asserting), f23=DocValuesFormat(name=Lucene70), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Memory), f24=DocValuesFormat(name=Asserting), cf0=DocValuesFormat(name=Memory), cf2=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Memory), cf3=DocValuesFormat(name=Asserting), cf6=DocValuesFormat(name=Lucene70), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Memory), cf7=DocValuesFormat(name=Asserting), cf9=DocValuesFormat(name=Lucene70), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Asserting), cf24=DocValuesFormat(name=Memory), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Lucene70), cf20=DocValuesFormat(name=Memory), key=DocValuesFormat(name=Lucene70), cf=DocValuesFormat(name=Lucene70), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Asserting), cf17=DocValuesFormat(name=Memory), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Lucene70), cf12=DocValuesFormat(name=Asserting), cf13=DocValuesFormat(name=Memory), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1397, maxMBSortInHeap=7.760564666966891, sim=RandomSimilarity(queryNorm=false): {}, locale=pt-BR, timezone=Africa/Porto-Novo
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=283409888,total=478150656



Checking out Revision 6c163658bbca15b1e4ff81d16b25e07df78468e8 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=85C41F1E0BBEB082 -Dtests.slow=true -Dtests.locale=ca-ES -Dtests.timezone=Kwajalein -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.71s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f2, reader=_d(8.0.0):c55:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([85C41F1E0BBEB082:B3387D318A4BD39E]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=true): {}, locale=ca-ES, timezone=Kwajalein
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=312487000,total=502792192","steve_rowe","NULL","1","issue","1","0","0","0","0"
"12","12","366","44","Mike, any reason not to backport to branch_7x and branch_7_0?
Ugh, I thought my commit went in before 7.x/7.0 branched; I'll back port tomorrow, and look into the new test failures!","Mike, any reason not to backport to branch_7x and branch_7_0?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"13","13","366","44","Mike, any reason not to backport to branch_7x and branch_7_0?
Ugh, I thought my commit went in before 7.x/7.0 branched; I'll back port tomorrow, and look into the new test failures!","Ugh, I thought my commit went in before 7.x/7.0 branched; I'll back port tomorrow, and look into the new test failures!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"14","14","367","44","Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
I'll dig on the new failures.","Mike, any reason not to backport to branch_7x and branch_7_0?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"15","15","367","44","Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
I'll dig on the new failures.","OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"16","16","367","44","Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
I'll dig on the new failures.","I'll dig on the new failures.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"17","17","368","44","
Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
Crap, sorry for wasting your time.  I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.  But looking now I see it on both branches' logs.","
Mike, any reason not to backport to branch_7x and branch_7_0?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"18","18","368","44","
Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
Crap, sorry for wasting your time.  I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.  But looking now I see it on both branches' logs.","OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"19","19","368","44","
Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
Crap, sorry for wasting your time.  I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.  But looking now I see it on both branches' logs.","Crap, sorry for wasting your time.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"20","20","368","44","
Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
Crap, sorry for wasting your time.  I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.  But looking now I see it on both branches' logs.","I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"21","21","368","44","
Mike, any reason not to backport to branch_7x and branch_7_0?
OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.
Crap, sorry for wasting your time.  I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.  But looking now I see it on both branches' logs.","But looking now I see it on both branches' logs.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"22","22","369","44","No worries Steve Rowe!","No worries Steve Rowe!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"23","23","370","44","Also, I think the 4 non-reproducing seeds above (https://issues.apache.org/jira/browse/LUCENE-7888?focusedCommentId=16077449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16077449) were before my last commit (7c704d5258b3be8c383ccb96bf4a30be441f091c) fixing a race ... so I'm hoping there are no more failures in this challenging test ","Also, I think the 4 non-reproducing seeds above (https://issues.apache.org/jira/browse/LUCENE-7888?focusedCommentId=16077449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16077449) were before my last commit (7c704d5258b3be8c383ccb96bf4a30be441f091c) fixing a race ... so I'm hoping there are no more failures in this challenging test","mikemccand","NULL","1","decision","0","0","0","0","1"
"24","24","379","48","I can look into the clustering plugin's use of it. I recall it was unfortunately required, but will have to go into this again to remind myself why.","I can look into the clustering plugin's use of it.","dweiss","NULL","1","issue","1","0","0","0","0"
"25","25","379","48","I can look into the clustering plugin's use of it. I recall it was unfortunately required, but will have to go into this again to remind myself why.","I recall it was unfortunately required, but will have to go into this again to remind myself why.","dweiss","NULL","1","issue","1","0","0","0","0"
"26","26","380","48","Patch removing context classloader usage. Tests seem to pass, unfortunately Solr trunk is very unstable. Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.
This patch also adds context class loaders on te forbidden api list. Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.","Patch removing context classloader usage.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"27","27","380","48","Patch removing context classloader usage. Tests seem to pass, unfortunately Solr trunk is very unstable. Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.
This patch also adds context class loaders on te forbidden api list. Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.","Tests seem to pass, unfortunately Solr trunk is very unstable.","thetaphi","NULL","1","issue","1","0","0","0","0"
"28","28","380","48","Patch removing context classloader usage. Tests seem to pass, unfortunately Solr trunk is very unstable. Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.
This patch also adds context class loaders on te forbidden api list. Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.","Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.","thetaphi","NULL","1","issue","1","0","0","0","0"
"29","29","380","48","Patch removing context classloader usage. Tests seem to pass, unfortunately Solr trunk is very unstable. Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.
This patch also adds context class loaders on te forbidden api list. Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.","This patch also adds context class loaders on te forbidden api list.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"30","30","380","48","Patch removing context classloader usage. Tests seem to pass, unfortunately Solr trunk is very unstable. Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.
This patch also adds context class loaders on te forbidden api list. Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.","Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"31","31","381","48","Looks good to me. I'll check why the context classloader is required in clustering later on. I think the case was that clustering was under shared libraries and resources loaded per-core couldn't figure where to load classes from.","Looks good to me.","dweiss","NULL","1","pro","0","0","1","0","0"
"32","32","381","48","Looks good to me. I'll check why the context classloader is required in clustering later on. I think the case was that clustering was under shared libraries and resources loaded per-core couldn't figure where to load classes from.","I'll check why the context classloader is required in clustering later on.","dweiss","NULL","1","alternative","0","1","0","0","0"
"33","33","381","48","Looks good to me. I'll check why the context classloader is required in clustering later on. I think the case was that clustering was under shared libraries and resources loaded per-core couldn't figure where to load classes from.","I think the case was that clustering was under shared libraries and resources loaded per-core couldn't figure where to load classes from.","dweiss","NULL","1","issue","1","0","0","0","0"
"34","34","382","48","I did some live test with the standalone techproducts example. I have seen no issues, so I think this should be fine to commit. I will add a CHANGES entry in both Lucene and Solr, because this affects both projects.","I did some live test with the standalone techproducts example.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"35","35","382","48","I did some live test with the standalone techproducts example. I have seen no issues, so I think this should be fine to commit. I will add a CHANGES entry in both Lucene and Solr, because this affects both projects.","I have seen no issues, so I think this should be fine to commit.","thetaphi","NULL","1","pro","0","0","1","0","0"
"36","36","382","48","I did some live test with the standalone techproducts example. I have seen no issues, so I think this should be fine to commit. I will add a CHANGES entry in both Lucene and Solr, because this affects both projects.","I will add a CHANGES entry in both Lucene and Solr, because this affects both projects.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"37","37","383","48","Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there. But all failures in tests that I see here, look like the ones Jenkins is drinking with his beers!
This is not a good state, the test suite should pass for a clean checkout.","Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there.","thetaphi","NULL","1","issue","1","0","0","0","0"
"38","38","383","48","Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there. But all failures in tests that I see here, look like the ones Jenkins is drinking with his beers!
This is not a good state, the test suite should pass for a clean checkout.","But all failures in tests that I see here, look like the ones Jenkins is drinking with his beers!","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"39","39","383","48","Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there. But all failures in tests that I see here, look like the ones Jenkins is drinking with his beers!
This is not a good state, the test suite should pass for a clean checkout.","This is not a good state, the test suite should pass for a clean checkout.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"40","40","384","48","Commit 5de15ff403fbf4afe68718151617e6104f7e3888 in lucene-solr's branch refs/heads/master from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5de15ff ]
LUCENE-7883: Lucene/Solr no longer uses the context class loader when resolving resources","Commit 5de15ff403fbf4afe68718151617e6104f7e3888 in lucene-solr's branch refs/heads/master from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5de15ff ]
LUCENE-7883: Lucene/Solr no longer uses the context class loader when resolving resources","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"41","41","385","48","I added changes and migrate entries and committed to master (7.0).","I added changes and migrate entries and committed to master (7.0).","thetaphi","NULL","1","decision","0","0","0","0","1"
"42","42","1114","147","Here is a patch.","Here is a patch.","jpountz","NULL","1","alternative","0","1","0","0","0"
"43","43","1115","147","+1 good catch!","+1 good catch!","martijn.v.groningen","NULL","1","pro","0","0","1","0","0"
"44","44","1116","147","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"45","45","1117","147","Commit 3a0c2a691d4fea1670b3d071032fc54c716b5d1a in lucene-solr's branch refs/heads/branch_6_5 from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a0c2a6 ]
LUCENE-7755: Join queries should not reference IndexReaders.","Commit 3a0c2a691d4fea1670b3d071032fc54c716b5d1a in lucene-solr's branch refs/heads/branch_6_5 from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a0c2a6 ]
LUCENE-7755: Join queries should not reference IndexReaders.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"46","46","1118","147","Commit bd2ec8e40e83e4712062c37ed121132054409918 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bd2ec8e ]
LUCENE-7755: Join queries should not reference IndexReaders.","Commit bd2ec8e40e83e4712062c37ed121132054409918 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bd2ec8e ]
LUCENE-7755: Join queries should not reference IndexReaders.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"47","47","1119","147","Commit edafcbad14482f3cd2f072fdca0c89600e72885d in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=edafcba ]
LUCENE-7755: Join queries should not reference IndexReaders.","Commit edafcbad14482f3cd2f072fdca0c89600e72885d in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=edafcba ]
LUCENE-7755: Join queries should not reference IndexReaders.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"48","48","1990","258","Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future. We can deal with fixing existing rawtypes or a few other classes of warning in the future.
David Smiley - you seemed interested in this conversation last time it came up.","Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future.","mdrob","NULL","1","alternative, pro","0","1","1","0","0"
"49","49","1990","258","Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future. We can deal with fixing existing rawtypes or a few other classes of warning in the future.
David Smiley - you seemed interested in this conversation last time it came up.","We can deal with fixing existing rawtypes or a few other classes of warning in the future.","mdrob","NULL","1","issue","1","0","0","0","0"
"50","50","1990","258","Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future. We can deal with fixing existing rawtypes or a few other classes of warning in the future.
David Smiley - you seemed interested in this conversation last time it came up.","David Smiley - you seemed interested in this conversation last time it came up.","mdrob","NULL","0",NULL,"0","0","0","0","0"
"51","51","1991","258","Thanks for filing this issue!  Are all the changes in this patch necessary to get the build to pass?  So to clarify... no code (outside what the patch touches) needs adjustments?","Thanks for filing this issue!","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"52","52","1991","258","Thanks for filing this issue!  Are all the changes in this patch necessary to get the build to pass?  So to clarify... no code (outside what the patch touches) needs adjustments?","Are all the changes in this patch necessary to get the build to pass?","dsmiley","NULL","1","issue","1","0","0","0","0"
"53","53","1991","258","Thanks for filing this issue!  Are all the changes in this patch necessary to get the build to pass?  So to clarify... no code (outside what the patch touches) needs adjustments?","So to clarify... no code (outside what the patch touches) needs adjustments?","dsmiley","NULL","1","issue","1","0","0","0","0"
"54","54","1992","258","Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.
Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.
I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.","Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.","mdrob","NULL","1","alternative, pro","0","1","1","0","0"
"55","55","1992","258","Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.
Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.
I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.","Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled.","mdrob","NULL","1","issue","1","0","0","0","0"
"56","56","1992","258","Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.
Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.
I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.","Each of those causes a lot of errors that I'd like to see eventually followed up on.","mdrob","NULL","1","issue","1","0","0","0","0"
"57","57","1992","258","Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.
Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.
I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.","The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.","mdrob","NULL","1","issue","1","0","0","0","0"
"58","58","1992","258","Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.
Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled. Each of those causes a lot of errors that I'd like to see eventually followed up on. The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.
I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.","I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.","mdrob","NULL","1","issue","1","0","0","0","0"
"59","59","1994","258","Yes on Solr your change is not enabled: https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30
We should also review Solr (maybe in a separate issue).","Yes on Solr your change is not enabled: https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30
We should also review Solr (maybe in a separate issue).","thetaphi","NULL","1","issue","1","0","0","0","0"
"60","60","1995","258","I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!
Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.
I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?","I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint.","mdrob","NULL","1","alternative, con","0","1","0","1","0"
"61","61","1995","258","I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!
Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.
I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?","But it's good to have them listed!","mdrob","NULL","1","pro","0","0","1","0","0"
"62","62","1995","258","I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!
Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.
I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?","Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.","mdrob","NULL","1","alternative, pro","0","1","1","0","0"
"63","63","1995","258","I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!
Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.
I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?","I don't have access to an IBM jdk to check if that produces different output or not.","mdrob","NULL","0",NULL,"0","0","0","0","0"
"64","64","1995","258","I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint. But it's good to have them listed!
Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.
I don't have access to an IBM jdk to check if that produces different output or not. Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?","Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?","mdrob","NULL","0",NULL,"0","0","0","0","0"
"65","65","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"66","66","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","I can try later, I have one installed.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"67","67","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","What do we do with Solr?","thetaphi","NULL","1","issue","1","0","0","0","0"
"68","68","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning).","thetaphi","NULL","1","decision","0","0","0","0","1"
"69","69","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","Otherwise I am fine with committing this.","thetaphi","NULL","1","pro","0","0","1","0","0"
"70","70","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","But we should really work on removing unsafe and rawtypes warnings from functions module.","thetaphi","NULL","1","issue","1","0","0","0","0"
"71","71","1996","258","Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options. I can try later, I have one installed.
What do we do with Solr? Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning). Otherwise I am fine with committing this. But we should really work on removing unsafe and rawtypes warnings from functions module. Now those are completely undetected (no warning, no error).","Now those are completely undetected (no warning, no error).","thetaphi","NULL","1","issue","1","0","0","0","0"
"72","72","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","Patch of 2 Jan 2017.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"73","73","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","This can be used as proximity subquery whenever SynonymQuery is used now, i.e.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"74","74","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","for synonym terms.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"75","75","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"76","76","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"77","77","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"78","78","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","Aside: how about renaming Terms to FieldTerms?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"79","79","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","This takes DisjunctionSpans out of SpanOrQuery.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"80","80","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"81","81","2138","273","Patch of 2 Jan 2017.
This can be used as proximity subquery whenever SynonymQuery is used now, i.e. for synonym terms.
I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.
Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.
Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.
Aside: how about renaming Terms to FieldTerms?
This takes DisjunctionSpans out of SpanOrQuery.
This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.
PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"82","82","2139","273","Some plans for using this:
In LUCENE-7580 to get real synonym scoring behaviour.
In Surround to score truncations.","Some plans for using this:
In LUCENE-7580 to get real synonym scoring behaviour.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"83","83","2139","273","Some plans for using this:
In LUCENE-7580 to get real synonym scoring behaviour.
In Surround to score truncations.","In Surround to score truncations.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"84","84","2140","273","In the patch of 2 Jan 2017 the term contexts are extracted twice, once in SynonymWeight and once to create the SpanSynonymWeight.
I'll post a fix later.","In the patch of 2 Jan 2017 the term contexts are extracted twice, once in SynonymWeight and once to create the SpanSynonymWeight.","paul.elschot@xs4all.nl","NULL","1","issue","1","0","0","0","0"
"85","85","2140","273","In the patch of 2 Jan 2017 the term contexts are extracted twice, once in SynonymWeight and once to create the SpanSynonymWeight.
I'll post a fix later.","I'll post a fix later.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"86","86","2141","273","Patch of 3 Jan 2017.
Compared to yesterday, this adds getTermContexts() in SynonymWeight for use in SpanSynonymQuery.","Patch of 3 Jan 2017.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"87","87","2141","273","Patch of 3 Jan 2017.
Compared to yesterday, this adds getTermContexts() in SynonymWeight for use in SpanSynonymQuery.","Compared to yesterday, this adds getTermContexts() in SynonymWeight for use in SpanSynonymQuery.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"88","88","2142","273","In SpanSynonymQuery.java here, this is not used:


import org.apache.lucene.search.similarities.Similarity;

","In SpanSynonymQuery.java here, this is not used:


import org.apache.lucene.search.similarities.Similarity;","paul.elschot@xs4all.nl","NULL","1","issue","1","0","0","0","0"
"89","89","2143","273","GitHub user PaulElschot opened a pull request:
 https://github.com/apache/lucene-solr/pull/165
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/165.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #165

commit 676c13c0c70e3f344ad6fb430eb5868270be83aa
Author: Paul Elschot <paul.j.elschot@gmail.com>
Date:   2017-03-08T22:10:40Z
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
","GitHub user PaulElschot opened a pull request:
 https://github.com/apache/lucene-solr/pull/165
 LUCENE-7615 of 8 March 2017.","githubbot","NULL","0",NULL,"0","0","0","0","0"
"90","90","2143","273","GitHub user PaulElschot opened a pull request:
 https://github.com/apache/lucene-solr/pull/165
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/165.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #165

commit 676c13c0c70e3f344ad6fb430eb5868270be83aa
Author: Paul Elschot <paul.j.elschot@gmail.com>
Date:   2017-03-08T22:10:40Z
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
","Adds support for SpanSynonymQuery in xml queryparser.","githubbot","NULL","0",NULL,"0","0","0","0","0"
"91","91","2143","273","GitHub user PaulElschot opened a pull request:
 https://github.com/apache/lucene-solr/pull/165
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/165.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #165

commit 676c13c0c70e3f344ad6fb430eb5868270be83aa
Author: Paul Elschot <paul.j.elschot@gmail.com>
Date:   2017-03-08T22:10:40Z
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
","You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/165.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #165

commit 676c13c0c70e3f344ad6fb430eb5868270be83aa
Author: Paul Elschot <paul.j.elschot@gmail.com>
Date:   2017-03-08T22:10:40Z
 LUCENE-7615 of 8 March 2017.","githubbot","NULL","0",NULL,"0","0","0","0","0"
"92","92","2143","273","GitHub user PaulElschot opened a pull request:
 https://github.com/apache/lucene-solr/pull/165
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/165.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #165

commit 676c13c0c70e3f344ad6fb430eb5868270be83aa
Author: Paul Elschot <paul.j.elschot@gmail.com>
Date:   2017-03-08T22:10:40Z
 LUCENE-7615 of 8 March 2017.
    Adds support for SpanSynonymQuery in xml queryparser.
","Adds support for SpanSynonymQuery in xml queryparser.","githubbot","NULL","0",NULL,"0","0","0","0","0"
"93","93","3572","432","GitHub user tballison opened a pull request:
 https://github.com/apache/lucene-solr/pull/75
 LUCENE-7434, first draft
 LUCENE-7434, first draft
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/tballison/lucene-solr master
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/75.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #75

commit c37f1e0d66f1f28a5c83033d9496cc33c55f265e
Author: tballison <tallison@mitre.org>
Date:   2016-09-01T19:33:55Z
 LUCENE-7434, first draft
","GitHub user tballison opened a pull request:
 https://github.com/apache/lucene-solr/pull/75
 LUCENE-7434, first draft
 LUCENE-7434, first draft
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/tballison/lucene-solr master
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/75.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #75

commit c37f1e0d66f1f28a5c83033d9496cc33c55f265e
Author: tballison <tallison@mitre.org>
Date:   2016-09-01T19:33:55Z
 LUCENE-7434, first draft","githubbot","NULL","0",NULL,"0","0","0","0","0"
"94","94","3573","432","But this allow to create Span Disjunction Query, which is considered as a black sheep in Lucene herd. I don't know why exactly, but have an idea.   ","But this allow to create Span Disjunction Query, which is considered as a black sheep in Lucene herd.","mkhludnev","NULL","1","con","0","0","0","1","0"
"95","95","3573","432","But this allow to create Span Disjunction Query, which is considered as a black sheep in Lucene herd. I don't know why exactly, but have an idea.   ","I don't know why exactly, but have an idea.","mkhludnev","NULL","0",NULL,"0","0","0","0","0"
"96","96","3574","432","Sorry, I've been away from Lucene for too long.  Can you explain a bit more?","Sorry, I've been away from Lucene for too long.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"97","97","3574","432","Sorry, I've been away from Lucene for too long.  Can you explain a bit more?","Can you explain a bit more?","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"98","98","3575","432","Thanks Tim Allison for creating this issue.
In order to merge threads, I want to clarify that my original question was about limiting the search window as well as the number of matches.
The slop  parameter sets the maximum distance allowed between each of the subspans and I was looking to add another parameter for the maximum window in which multiple the sub spans should appear together - between the beginning of the first, to the beginning/end of the last one.","Thanks Tim Allison for creating this issue.","saar32","NULL","0",NULL,"0","0","0","0","0"
"99","99","3575","432","Thanks Tim Allison for creating this issue.
In order to merge threads, I want to clarify that my original question was about limiting the search window as well as the number of matches.
The slop  parameter sets the maximum distance allowed between each of the subspans and I was looking to add another parameter for the maximum window in which multiple the sub spans should appear together - between the beginning of the first, to the beginning/end of the last one.","In order to merge threads, I want to clarify that my original question was about limiting the search window as well as the number of matches.","saar32","NULL","1","issue","1","0","0","0","0"
"100","100","3575","432","Thanks Tim Allison for creating this issue.
In order to merge threads, I want to clarify that my original question was about limiting the search window as well as the number of matches.
The slop  parameter sets the maximum distance allowed between each of the subspans and I was looking to add another parameter for the maximum window in which multiple the sub spans should appear together - between the beginning of the first, to the beginning/end of the last one.","The slop  parameter sets the maximum distance allowed between each of the subspans and I was looking to add another parameter for the maximum window in which multiple the sub spans should appear together - between the beginning of the first, to the beginning/end of the last one.","saar32","NULL","1","alternative","0","1","0","0","0"
"101","101","3576","432","As it happens I am in slow progress making something for the case minNumberShouldMatch=2, all pairs.
In case there is interest in an early version of that, please let me know.
For the maximum window there will be similar restrictions as for LUCENE-7398.","As it happens I am in slow progress making something for the case minNumberShouldMatch=2, all pairs.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"102","102","3576","432","As it happens I am in slow progress making something for the case minNumberShouldMatch=2, all pairs.
In case there is interest in an early version of that, please let me know.
For the maximum window there will be similar restrictions as for LUCENE-7398.","In case there is interest in an early version of that, please let me know.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"103","103","3576","432","As it happens I am in slow progress making something for the case minNumberShouldMatch=2, all pairs.
In case there is interest in an early version of that, please let me know.
For the maximum window there will be similar restrictions as for LUCENE-7398.","For the maximum window there will be similar restrictions as for LUCENE-7398.","paul.elschot@xs4all.nl","NULL","1","alternative, con","0","1","0","1","0"
"104","104","3577","432","This all pairs thing is useful here anyway, so here it is. 
There is a nocommit for an unfinished corner.
It splits off DisjunctionSpans from SpanOr and uses that to determine the matching pairs.","This all pairs thing is useful here anyway, so here it is.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"105","105","3577","432","This all pairs thing is useful here anyway, so here it is. 
There is a nocommit for an unfinished corner.
It splits off DisjunctionSpans from SpanOr and uses that to determine the matching pairs.","There is a nocommit for an unfinished corner.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"106","106","3577","432","This all pairs thing is useful here anyway, so here it is. 
There is a nocommit for an unfinished corner.
It splits off DisjunctionSpans from SpanOr and uses that to determine the matching pairs.","It splits off DisjunctionSpans from SpanOr and uses that to determine the matching pairs.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"107","107","3579","432","Interesting.  Whoa on LUCENE-7398!  What's the use case for minNumberShouldMatch=2, all pairs?  Apologies for my daftness...","Interesting.","tallison@mitre.org","NULL","1","pro","0","0","1","0","0"
"108","108","3579","432","Interesting.  Whoa on LUCENE-7398!  What's the use case for minNumberShouldMatch=2, all pairs?  Apologies for my daftness...","Whoa on LUCENE-7398!","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"109","109","3579","432","Interesting.  Whoa on LUCENE-7398!  What's the use case for minNumberShouldMatch=2, all pairs?  Apologies for my daftness...","What's the use case for minNumberShouldMatch=2, all pairs?","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"110","110","3579","432","Interesting.  Whoa on LUCENE-7398!  What's the use case for minNumberShouldMatch=2, all pairs?  Apologies for my daftness...","Apologies for my daftness...","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"111","111","3580","432","What's the use case for minNumberShouldMatch=2, all pairs?
This might generalize to higher values of minNumberShouldMatch, and replacing the slop by a window is easy.
For higher values of minNumberShouldMatch it would probably be good to reuse the implementation from boolean queries.","What's the use case for minNumberShouldMatch=2, all pairs?","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"112","112","3580","432","What's the use case for minNumberShouldMatch=2, all pairs?
This might generalize to higher values of minNumberShouldMatch, and replacing the slop by a window is easy.
For higher values of minNumberShouldMatch it would probably be good to reuse the implementation from boolean queries.","This might generalize to higher values of minNumberShouldMatch, and replacing the slop by a window is easy.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"113","113","3580","432","What's the use case for minNumberShouldMatch=2, all pairs?
This might generalize to higher values of minNumberShouldMatch, and replacing the slop by a window is easy.
For higher values of minNumberShouldMatch it would probably be good to reuse the implementation from boolean queries.","For higher values of minNumberShouldMatch it would probably be good to reuse the implementation from boolean queries.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"114","114","3581","432","Ah, ok.  Thank you.  Is my proposed approach flawed for the minNumberShouldMatch component of Saar Carmi's request?","Ah, ok.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"115","115","3581","432","Ah, ok.  Thank you.  Is my proposed approach flawed for the minNumberShouldMatch component of Saar Carmi's request?","Thank you.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"116","116","3581","432","Ah, ok.  Thank you.  Is my proposed approach flawed for the minNumberShouldMatch component of Saar Carmi's request?","Is my proposed approach flawed for the minNumberShouldMatch component of Saar Carmi's request?","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"117","117","3582","432","Is my proposed approach flawed for the minNumberShouldMatch component ... ?
Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.
I think that is too strict when more than the required number of subSpans are actually present in the segment.
The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.
The (untested) all pairs code above tries to do that, but only for pairs of subSpans.","Is my proposed approach flawed for the minNumberShouldMatch component ... ?","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"118","118","3582","432","Is my proposed approach flawed for the minNumberShouldMatch component ... ?
Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.
I think that is too strict when more than the required number of subSpans are actually present in the segment.
The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.
The (untested) all pairs code above tries to do that, but only for pairs of subSpans.","Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"119","119","3582","432","Is my proposed approach flawed for the minNumberShouldMatch component ... ?
Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.
I think that is too strict when more than the required number of subSpans are actually present in the segment.
The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.
The (untested) all pairs code above tries to do that, but only for pairs of subSpans.","I think that is too strict when more than the required number of subSpans are actually present in the segment.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"120","120","3582","432","Is my proposed approach flawed for the minNumberShouldMatch component ... ?
Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.
I think that is too strict when more than the required number of subSpans are actually present in the segment.
The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.
The (untested) all pairs code above tries to do that, but only for pairs of subSpans.","The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.","paul.elschot@xs4all.nl","NULL","1","alternative, con","0","1","0","1","0"
"121","121","3582","432","Is my proposed approach flawed for the minNumberShouldMatch component ... ?
Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.
I think that is too strict when more than the required number of subSpans are actually present in the segment.
The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.
The (untested) all pairs code above tries to do that, but only for pairs of subSpans.","The (untested) all pairs code above tries to do that, but only for pairs of subSpans.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"122","122","3584","432","If I get it right, this is a minor difference. For my case it should be fine.","If I get it right, this is a minor difference.","saar32","NULL","1","pro","0","0","1","0","0"
"123","123","3584","432","If I get it right, this is a minor difference. For my case it should be fine.","For my case it should be fine.","saar32","NULL","1","pro","0","0","1","0","0"
"124","124","3586","432","Dupe of LUCENE-3369?","Dupe of LUCENE-3369?","trejkaz","NULL","0",NULL,"0","0","0","0","0"
"125","125","3588","432","Y. Thank you.","Y.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"126","126","3588","432","Y. Thank you.","Thank you.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"127","127","3589","432","Thanks to Trejkaz for pointing out that LUCENE-7434 is a duplicate of LUCENE-3369.","Thanks to Trejkaz for pointing out that LUCENE-7434 is a duplicate of LUCENE-3369.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"128","128","3590","432","Very cool.  I've been meaning to look into TermAutomatonQueries for a while.  My two concerns: I'm not sure how this could play nicely with the other SpanQueries, and we'd want to integrate multiterm handling.","Very cool.","tallison@mitre.org","NULL","1","pro","0","0","1","0","0"
"129","129","3590","432","Very cool.  I've been meaning to look into TermAutomatonQueries for a while.  My two concerns: I'm not sure how this could play nicely with the other SpanQueries, and we'd want to integrate multiterm handling.","I've been meaning to look into TermAutomatonQueries for a while.","tallison@mitre.org","NULL","1","issue","1","0","0","0","0"
"130","130","3590","432","Very cool.  I've been meaning to look into TermAutomatonQueries for a while.  My two concerns: I'm not sure how this could play nicely with the other SpanQueries, and we'd want to integrate multiterm handling.","My two concerns: I'm not sure how this could play nicely with the other SpanQueries, and we'd want to integrate multiterm handling.","tallison@mitre.org","NULL","1","issue","1","0","0","0","0"
"131","131","3592","432","I think so, if we blow up the seed sequence to a a a B B B c c c d d d.","I think so, if we blow up the seed sequence to a a a B B B c c c d d d.","mkhludnev","NULL","1","issue","1","0","0","0","0"
"132","132","3593","432","attaching a proof for x a a a and a terrific pic for it","attaching a proof for x a a a and a terrific pic for it","mkhludnev","NULL","0",NULL,"0","0","0","0","0"
"133","133","3594","432","The most intriguing question about TermAutomatonQuery is its' efficiency. Can it load term positions only for those docs which passed term level minShouldMatch condition? eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries. Will the later speedups TAQ by faster dragging?   ","The most intriguing question about TermAutomatonQuery is its' efficiency.","mkhludnev","NULL","1","issue","1","0","0","0","0"
"134","134","3594","432","The most intriguing question about TermAutomatonQuery is its' efficiency. Can it load term positions only for those docs which passed term level minShouldMatch condition? eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries. Will the later speedups TAQ by faster dragging?   ","Can it load term positions only for those docs which passed term level minShouldMatch condition?","mkhludnev","NULL","1","issue","1","0","0","0","0"
"135","135","3594","432","The most intriguing question about TermAutomatonQuery is its' efficiency. Can it load term positions only for those docs which passed term level minShouldMatch condition? eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries. Will the later speedups TAQ by faster dragging?   ","eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries.","mkhludnev","NULL","1","pro","0","0","1","0","0"
"136","136","3594","432","The most intriguing question about TermAutomatonQuery is its' efficiency. Can it load term positions only for those docs which passed term level minShouldMatch condition? eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries. Will the later speedups TAQ by faster dragging?   ","Will the later speedups TAQ by faster dragging?","mkhludnev","NULL","1","issue","1","0","0","0","0"
"137","137","3595","432","The most intriguing question about TermAutomatonQuery is its' efficiency. 
I think there's plenty of work to be done here.  E.g. LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.","The most intriguing question about TermAutomatonQuery is its' efficiency.","mikemccand","NULL","1","issue","1","0","0","0","0"
"138","138","3595","432","The most intriguing question about TermAutomatonQuery is its' efficiency. 
I think there's plenty of work to be done here.  E.g. LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.","I think there's plenty of work to be done here.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"139","139","3595","432","The most intriguing question about TermAutomatonQuery is its' efficiency. 
I think there's plenty of work to be done here.  E.g. LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.","E.g.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"140","140","3595","432","The most intriguing question about TermAutomatonQuery is its' efficiency. 
I think there's plenty of work to be done here.  E.g. LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.","LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.","mikemccand","NULL","1","pro","0","0","1","0","0"
"141","141","3902","463","Commit 0c1cab71920a540807555501f7198ca402e16740 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0c1cab7 ]
LUCENE-7401: Make sure BKD trees index all dimensions.","Commit 0c1cab71920a540807555501f7198ca402e16740 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0c1cab7 ]
LUCENE-7401: Make sure BKD trees index all dimensions.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"142","142","3903","463","Commit ba47f530d1165d4518569422472bc9e4f1c04b26 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ba47f53 ]
LUCENE-7401: Make sure BKD trees index all dimensions.","Commit ba47f530d1165d4518569422472bc9e4f1c04b26 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ba47f53 ]
LUCENE-7401: Make sure BKD trees index all dimensions.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"143","143","3904","463","Thanks Mike for having a look.","Thanks Mike for having a look.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"144","144","4260","508","I am looking into it. It does not seem to be related to BS1 this time since the test still fails when I disable BS1.","I am looking into it.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"145","145","4260","508","I am looking into it. It does not seem to be related to BS1 this time since the test still fails when I disable BS1.","It does not seem to be related to BS1 this time since the test still fails when I disable BS1.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"146","146","4261","508","This is a test bug. CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation. This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.","This is a test bug.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"147","147","4261","508","This is a test bug. CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation. This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.","CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation.","jpountz","NULL","1","issue","1","0","0","0","0"
"148","148","4261","508","This is a test bug. CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation. This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.","This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.","jpountz","NULL","1","con","0","0","0","1","0"
"149","149","4262","508","+1
Thank you for digging Adrien Grand!","+1
Thank you for digging Adrien Grand!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"150","150","4263","508","Commit c5defadd70a9f91bc31012b7c31c39f16d883849 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c5defad ]
LUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.","Commit c5defadd70a9f91bc31012b7c31c39f16d883849 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c5defad ]
LUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"151","151","4264","508","Commit 1e4d51f4085664ef073ecac18dd572b0a9a02757 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1e4d51f ]
LUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.","Commit 1e4d51f4085664ef073ecac18dd572b0a9a02757 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1e4d51f ]
LUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"152","152","4265","508","Bulk close resolved issues after 6.2.0 release.","Bulk close resolved issues after 6.2.0 release.","mikemccand","NULL","1","decision","0","0","0","0","1"
"153","153","4266","509","I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same. The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\% (635MB -> 577MB). The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value). For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\%).","I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same.","jpountz","NULL","1","alternative","0","1","0","0","0"
"154","154","4266","509","I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same. The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\% (635MB -> 577MB). The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value). For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\%).","The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\% (635MB -> 577MB).","jpountz","NULL","1","pro, con","0","0","1","1","0"
"155","155","4266","509","I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same. The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\% (635MB -> 577MB). The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value). For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\%).","The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value).","jpountz","NULL","1","pro","0","0","1","0","0"
"156","156","4266","509","I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same. The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\% (635MB -> 577MB). The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value). For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\%).","For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\%).","jpountz","NULL","1","pro","0","0","1","0","0"
"157","157","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","Updated patch.","jpountz","NULL","1","alternative","0","1","0","0","0"
"158","158","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).","jpountz","NULL","1","pro","0","0","1","0","0"
"159","159","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value.","jpountz","NULL","1","alternative","0","1","0","0","0"
"160","160","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","In that case, we delta-encode using vints.","jpountz","NULL","1","alternative","0","1","0","0","0"
"161","161","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","doc ids requiring less than 24 bits, which are encoded on 3 bytes.","jpountz","NULL","1","alternative","0","1","0","0","0"
"162","162","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.","jpountz","NULL","1","alternative","0","1","0","0","0"
"163","163","4267","509","Updated patch. It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).
It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value. In that case, we delta-encode using vints.
doc ids requiring less than 24 bits, which are encoded on 3 bytes.
doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.

I think it's ready to go?","I think it's ready to go?","jpountz","NULL","0",NULL,"0","0","0","0","0"
"164","164","4268","509","I like this better than the last patch, I think the optimization is more general. 
I think in the base test class, tesMostEqual() is a mistake?","I like this better than the last patch, I think the optimization is more general.","rcmuir","NULL","1","pro","0","0","1","0","0"
"165","165","4268","509","I like this better than the last patch, I think the optimization is more general. 
I think in the base test class, tesMostEqual() is a mistake?","I think in the base test class, tesMostEqual() is a mistake?","rcmuir","NULL","1","con","0","0","0","1","0"
"166","166","4269","509","Hmm I can remove both actually, they do not bring value now that the detection of whether doc ids are sorted is based on the doc ids themselves rather than the fact that there is a single value in a block.","Hmm I can remove both actually, they do not bring value now that the detection of whether doc ids are sorted is based on the doc ids themselves rather than the fact that there is a single value in a block.","jpountz","NULL","1","alternative, con","0","1","0","1","0"
"167","167","4270","509","Commit 01de73bc0a1b315bbbe4df046b5c0661cec8de2e in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=01de73b ]
LUCENE-7351: Doc id compression for points.","Commit 01de73bc0a1b315bbbe4df046b5c0661cec8de2e in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=01de73b ]
LUCENE-7351: Doc id compression for points.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"168","168","4271","509","Commit d66e9935c39ed859659de46d3d5cfb66f2279bd4 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d66e993 ]
LUCENE-7351: Doc id compression for points.","Commit d66e9935c39ed859659de46d3d5cfb66f2279bd4 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d66e993 ]
LUCENE-7351: Doc id compression for points.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"169","169","4272","509","Bulk close resolved issues after 6.2.0 release.","Bulk close resolved issues after 6.2.0 release.","mikemccand","NULL","1","decision","0","0","0","0","1"
"170","170","4393","527","Simple patch:

removes GeoPointTestUtil from TestGeoPointQuery
fixes a range corner case in GeoPointPrefixTermsEnum
adds an explicit test for the corner case

","Simple patch:

removes GeoPointTestUtil from TestGeoPointQuery
fixes a range corner case in GeoPointPrefixTermsEnum
adds an explicit test for the corner case","nknize","NULL","1","alternative","0","1","0","0","0"
"171","171","4394","527","+1","+1","jpountz","NULL","1","pro","0","0","1","0","0"
"172","172","4395","527","Commit 1bbac6bbd896c110d08656f79fef3ce6d7828d6b in lucene-solr's branch refs/heads/branch_6_1 from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1bbac6b ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.","Commit 1bbac6bbd896c110d08656f79fef3ce6d7828d6b in lucene-solr's branch refs/heads/branch_6_1 from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1bbac6b ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"173","173","4396","527","Commit f767855da30e8d8b070b7566cb6eebb29af63334 in lucene-solr's branch refs/heads/master from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f767855 ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.","Commit f767855da30e8d8b070b7566cb6eebb29af63334 in lucene-solr's branch refs/heads/master from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f767855 ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"174","174","4397","527","Nicholas Knize Did you forget to cherry-pick to branch_6x?","Nicholas Knize Did you forget to cherry-pick to branch_6x?","jpountz","NULL","0",NULL,"0","0","0","0","0"
"175","175","4398","527","Commit 7448abb3bca7b8204e56a52fc115f7a2d813884d in lucene-solr's branch refs/heads/branch_6x from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7448abb ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.","Commit 7448abb3bca7b8204e56a52fc115f7a2d813884d in lucene-solr's branch refs/heads/branch_6x from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7448abb ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"176","176","4756","563","David Smiley Pinging you in case you want to have a chance to look into it before we release 6.1. FYI the seed still reproduces for me on master.","David Smiley Pinging you in case you want to have a chance to look into it before we release 6.1.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"177","177","4756","563","David Smiley Pinging you in case you want to have a chance to look into it before we release 6.1. FYI the seed still reproduces for me on master.","FYI the seed still reproduces for me on master.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"178","178","4759","563","No I didn't; I already caught that and will commit that unchanged.","No I didn't; I already caught that and will commit that unchanged.","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"179","179","4760","563","Commit b33d7176aa3624df2de1708b17919f20d034872f in lucene-solr's branch refs/heads/master from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b33d717 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes","Commit b33d7176aa3624df2de1708b17919f20d034872f in lucene-solr's branch refs/heads/master from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b33d717 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"180","180","4761","563","Commit 7520d79e040c16c5ab666f1ad28c8665fb0ceb40 in lucene-solr's branch refs/heads/branch_6x from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7520d79 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit b33d717)","Commit 7520d79e040c16c5ab666f1ad28c8665fb0ceb40 in lucene-solr's branch refs/heads/branch_6x from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7520d79 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit b33d717)","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"181","181","4762","563","Commit 6372c0b4042ec2c8d94e50e5c2b9c1df469414e2 in lucene-solr's branch refs/heads/branch_6_1 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6372c0b ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","Commit 6372c0b4042ec2c8d94e50e5c2b9c1df469414e2 in lucene-solr's branch refs/heads/branch_6_1 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6372c0b ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"182","182","4763","563","Reopening to backport to 6.0.2, 5.6 and 5.5.2","Reopening to backport to 6.0.2, 5.6 and 5.5.2","steve_rowe","NULL","1","decision","0","0","0","0","1"
"183","183","4764","563","Commit 5c546537d7b8130c05263832baff4946260f6a31 in lucene-solr's branch refs/heads/branch_5_5 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5c54653 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","Commit 5c546537d7b8130c05263832baff4946260f6a31 in lucene-solr's branch refs/heads/branch_5_5 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5c54653 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"184","184","4765","563","Commit 1d7ad90947699e103de39fded5b78f76a30e449b in lucene-solr's branch refs/heads/branch_5_5 from Steve Rowe
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1d7ad90 ]
LUCENE-7291: Add 5.5.2 CHANGES entry","Commit 1d7ad90947699e103de39fded5b78f76a30e449b in lucene-solr's branch refs/heads/branch_5_5 from Steve Rowe
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1d7ad90 ]
LUCENE-7291: Add 5.5.2 CHANGES entry","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"185","185","4766","563","Commit f6b0fb95dea43f9f508b613cf32f489aaa263c4e in lucene-solr's branch refs/heads/branch_5x from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6b0fb9 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","Commit f6b0fb95dea43f9f508b613cf32f489aaa263c4e in lucene-solr's branch refs/heads/branch_5x from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6b0fb9 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"186","186","4767","563","Commit a7f2876ec5ce9ca5ef271cad97027a5cb5e43619 in lucene-solr's branch refs/heads/branch_6_0 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a7f2876 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","Commit a7f2876ec5ce9ca5ef271cad97027a5cb5e43619 in lucene-solr's branch refs/heads/branch_6_0 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a7f2876 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"187","187","5039","583","This patch changes the DocIdSetBuilder API. add() is gone. Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method. This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.","This patch changes the DocIdSetBuilder API.","jpountz","NULL","1","alternative","0","1","0","0","0"
"188","188","5039","583","This patch changes the DocIdSetBuilder API. add() is gone. Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method. This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.","add() is gone.","jpountz","NULL","1","alternative","0","1","0","0","0"
"189","189","5039","583","This patch changes the DocIdSetBuilder API. add() is gone. Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method. This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.","Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method.","jpountz","NULL","1","alternative","0","1","0","0","0"
"190","190","5039","583","This patch changes the DocIdSetBuilder API. add() is gone. Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method. This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.","This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.","jpountz","NULL","1","pro","0","0","1","0","0"
"191","191","5040","583","One possible downside to this change is that it changes a predictable branch (that is handled at the CPU level) into a method call... which if it's not monomorphic can be un-inlined at the point of the call and thus end up slower (method call vs predictable branch).  Will be interesting to see the benchmark results.","One possible downside to this change is that it changes a predictable branch (that is handled at the CPU level) into a method call... which if it's not monomorphic can be un-inlined at the point of the call and thus end up slower (method call vs predictable branch).","yseeley@gmail.com","NULL","1","con","0","0","0","1","0"
"192","192","5040","583","One possible downside to this change is that it changes a predictable branch (that is handled at the CPU level) into a method call... which if it's not monomorphic can be un-inlined at the point of the call and thus end up slower (method call vs predictable branch).  Will be interesting to see the benchmark results.","Will be interesting to see the benchmark results.","yseeley@gmail.com","NULL","0",NULL,"0","0","0","0","0"
"193","193","5041","583","I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs). The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.
In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize. (For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)","I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs).","jpountz","NULL","1","alternative, con","0","1","0","1","0"
"194","194","5041","583","I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs). The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.
In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize. (For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)","The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.","jpountz","NULL","1","pro","0","0","1","0","0"
"195","195","5041","583","I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs). The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.
In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize. (For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)","In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize.","jpountz","NULL","1","pro","0","0","1","0","0"
"196","196","5041","583","I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs). The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.
In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize. (For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)","(For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)","jpountz","NULL","1","pro","0","0","1","0","0"
"197","197","5042","583","Ah, thanks for that reference... need to update my mental models ","Ah, thanks for that reference... need to update my mental models","yseeley@gmail.com","NULL","0",NULL,"0","0","0","0","0"
"198","198","5043","583","Commit 95c360d053a35486aa12498c6fd319aef0377bb8 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=95c360d ]
LUCENE-7264: Fewer conditionals in DocIdSetBuilder.","Commit 95c360d053a35486aa12498c6fd319aef0377bb8 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=95c360d ]
LUCENE-7264: Fewer conditionals in DocIdSetBuilder.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"199","199","5044","583","Commit aa81ba8642a57181a4eaa017b52d0d3c3462544b in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aa81ba8 ]
LUCENE-7264: Fewer conditionals in DocIdSetBuilder.","Commit aa81ba8642a57181a4eaa017b52d0d3c3462544b in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aa81ba8 ]
LUCENE-7264: Fewer conditionals in DocIdSetBuilder.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"200","200","5045","583","Commit 14b42fe10ba64bb4468ea8ef298e54a751f16dd3 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=14b42fe ]
LUCENE-7264: Fix test bug in TestReqExclBulkScorer.","Commit 14b42fe10ba64bb4468ea8ef298e54a751f16dd3 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=14b42fe ]
LUCENE-7264: Fix test bug in TestReqExclBulkScorer.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"201","201","5046","583","Commit f7b333f10583639ee3d0f2631fee41c577c60452 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f7b333f ]
LUCENE-7264: Fix test bug in TestReqExclBulkScorer.","Commit f7b333f10583639ee3d0f2631fee41c577c60452 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f7b333f ]
LUCENE-7264: Fix test bug in TestReqExclBulkScorer.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"202","202","5047","583","Manually correcting fixVersion per Step #S5 of LUCENE-7271","Manually correcting fixVersion per Step #S5 of LUCENE-7271","hossman","NULL","1","decision","0","0","0","0","1"
"206","203","5625","645","Another failure, on 6.x:

Suite: org.apache.lucene.spatial3d.TestGeo3DPoint
   [smoker]    [junit4] IGNOR/A 0.01s J2 | TestGeo3DPoint.testRandomBig
   [smoker]    [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [smoker]    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGeo3DPoint -Dtests.method=testRandomMedium -Dtests.seed=AB1C87AA82F2EF89 -Dtests.multiplier=2 -Dtests.locale=es-VE -Dtests.timezone=Africa/Niamey -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [smoker]    [junit4] FAILURE 2.55s J2 | TestGeo3DPoint.testRandomMedium <<<
   [smoker]    [junit4]    > Throwable #1: java.lang.AssertionError: FAIL: id=8110 should have matched but did not
   [smoker]    [junit4]    >   shape=GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}
   [smoker]    [junit4]    >   point=[X=-0.20200947969927532, Y=-0.4709330291599749, Z=0.8571474154523655]
   [smoker]    [junit4]    >   docID=7962 deleted?=false
   [smoker]    [junit4]    >   query=PointInGeo3DShapeQuery: field=point: Shape: GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}
   [smoker]    [junit4]    >   explanation:
   [smoker]    [junit4]    >     target is in leaf _1(6.1.0):C20016 of full reader StandardDirectoryReader(segments:5:nrt _1(6.1.0):C20016)
   [smoker]    [junit4]    >     full BKD path to target doc:
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.01197272304385482 TO 0.35595394953475784); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.3559539490685756 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.6956355809209895); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.6956355813871717 TO 0.10900276736191666); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.10900276689573439 TO 0.7189540192817846); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.7189540188156024 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.68605594687831 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-0.6860559473444922 TO -0.32059454789968617 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=-0.006916229704337817 TO 0.6718162758370178); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=0.6718162753708355 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true
   [smoker]    [junit4]    >     on cell Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_OUTSIDE_QUERY  on cell Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_CROSSES_QUERY
   [smoker]    [junit4]    > 	at __randomizedtesting.SeedInfo.seed([AB1C87AA82F2EF89:16C2B002C3978CEF]:0)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.verify(TestGeo3DPoint.java:796)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.doTestRandom(TestGeo3DPoint.java:518)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.testRandomMedium(TestGeo3DPoint.java:445)
   [smoker]    [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [smoker]    [junit4]   2> NOTE: test params are: codec=Asserting(Lucene60): {id=Lucene50(blocksize=128)}, docValues:{id=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=421, maxMBSortInHeap=6.865543172299073, sim=ClassicSimilarity, locale=es-VE, timezone=Africa/Niamey
   [smoker]    [junit4]   2> NOTE: Linux 3.13.0-52-generic amd64/Oracle Corporation 1.8.0_74 (64-bit)/cpus=4,threads=1,free=292169440,total=351797248
   [smoker]    [junit4]   2> NOTE: All tests run in this JVM: [TestGeo3DPoint]

","Another failure, on 6.x:

Suite: org.apache.lucene.spatial3d.TestGeo3DPoint
   [smoker]    [junit4] IGNOR/A 0.01s J2 | TestGeo3DPoint.testRandomBig
   [smoker]    [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [smoker]    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGeo3DPoint -Dtests.method=testRandomMedium -Dtests.seed=AB1C87AA82F2EF89 -Dtests.multiplier=2 -Dtests.locale=es-VE -Dtests.timezone=Africa/Niamey -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [smoker]    [junit4] FAILURE 2.55s J2 | TestGeo3DPoint.testRandomMedium <<<
   [smoker]    [junit4]    > Throwable #1: java.lang.AssertionError: FAIL: id=8110 should have matched but did not
   [smoker]    [junit4]    >   shape=GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}
   [smoker]    [junit4]    >   point=[X=-0.20200947969927532, Y=-0.4709330291599749, Z=0.8571474154523655]
   [smoker]    [junit4]    >   docID=7962 deleted?=false
   [smoker]    [junit4]    >   query=PointInGeo3DShapeQuery: field=point: Shape: GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}
   [smoker]    [junit4]    >   explanation:
   [smoker]    [junit4]    >     target is in leaf _1(6.1.0):C20016 of full reader StandardDirectoryReader(segments:5:nrt _1(6.1.0):C20016)
   [smoker]    [junit4]    >     full BKD path to target doc:
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.01197272304385482 TO 0.35595394953475784); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.3559539490685756 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.6956355809209895); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.6956355813871717 TO 0.10900276736191666); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.10900276689573439 TO 0.7189540192817846); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.7189540188156024 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.68605594687831 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-0.6860559473444922 TO -0.32059454789968617 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=-0.006916229704337817 TO 0.6718162758370178); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=0.6718162753708355 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true
   [smoker]    [junit4]    >     on cell Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_OUTSIDE_QUERY  on cell Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_CROSSES_QUERY
   [smoker]    [junit4]    > 	at __randomizedtesting.SeedInfo.seed([AB1C87AA82F2EF89:16C2B002C3978CEF]:0)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.verify(TestGeo3DPoint.java:796)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.doTestRandom(TestGeo3DPoint.java:518)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.testRandomMedium(TestGeo3DPoint.java:445)
   [smoker]    [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [smoker]    [junit4]   2> NOTE: test params are: codec=Asserting(Lucene60): {id=Lucene50(blocksize=128)}, docValues:{id=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=421, maxMBSortInHeap=6.865543172299073, sim=ClassicSimilarity, locale=es-VE, timezone=Africa/Niamey
   [smoker]    [junit4]   2> NOTE: Linux 3.13.0-52-generic amd64/Oracle Corporation 1.8.0_74 (64-bit)/cpus=4,threads=1,free=292169440,total=351797248
   [smoker]    [junit4]   2> NOTE: All tests run in this JVM: [TestGeo3DPoint]","mikemccand","NULL","1","issue","1","0","0","0","0"
"207","204","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()?","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"208","205","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"209","206","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"210","207","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"211","208","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"212","209","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"213","210","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"214","211","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"215","212","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","I'm wondering if this is a result of granularity?","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"216","213","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"217","214","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","It does seem odd that all failures we've seen have been for very complex polygons, though.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"218","215","5626","645","Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()? false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()? false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()? false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else  and yet there are no edge intersections anywhere either.  I'm wondering if this is a result of granularity?  Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.
It does seem odd that all failures we've seen have been for very complex polygons, though.  I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"219","216","5627","645","I verified that the shape appears to be properly built, with the right internal edges, and that when looking for edge intersections it examines the correct number and type of edges.  The last thing to verify is whether the edge points for the XYZSolid are correct.","I verified that the shape appears to be properly built, with the right internal edges, and that when looking for edge intersections it examines the correct number and type of edges.","kwright@metacarta.com","NULL","1","pro","0","0","1","0","0"
"220","217","5627","645","I verified that the shape appears to be properly built, with the right internal edges, and that when looking for edge intersections it examines the correct number and type of edges.  The last thing to verify is whether the edge points for the XYZSolid are correct.","The last thing to verify is whether the edge points for the XYZSolid are correct.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"221","218","5629","645","Added more diagnostics to figure out if scaling the point to the surface would result in it leaving the cell.  Short answer: no:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true; Scaled point within cell = true; Scaled point within shape = true


So something is definitely wrong with the intersection logic, but I'm not yet clear what that is.","Added more diagnostics to figure out if scaling the point to the surface would result in it leaving the cell.","kwright@metacarta.com","NULL","1","alternative","0","1","0","0","0"
"222","219","5629","645","Added more diagnostics to figure out if scaling the point to the surface would result in it leaving the cell.  Short answer: no:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true; Scaled point within cell = true; Scaled point within shape = true


So something is definitely wrong with the intersection logic, but I'm not yet clear what that is.","Short answer: no:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true; Scaled point within cell = true; Scaled point within shape = true


So something is definitely wrong with the intersection logic, but I'm not yet clear what that is.","kwright@metacarta.com","NULL","1","alternative, con","0","1","0","1","0"
"223","220","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section?","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"224","221","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","false
   [junit4]   2> Is point within p2 section?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"225","222","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","true
   [junit4]   2> Is point within p3 section?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"226","223","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","false
   [junit4]   2> Is point within concave section?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"227","224","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","false
   [junit4]   2> Is point within p5 section?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"228","225","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","true


The point is within TWO parts of the composite polygon.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"229","226","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","This is, of course, nonsense unless the point happens to be on a shared edge between the two.","kwright@metacarta.com","NULL","1","con","0","0","0","1","0"
"230","227","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","But these two don't even share an edge.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"231","228","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"232","229","5630","645","When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section? false
   [junit4]   2> Is point within p2 section? true
   [junit4]   2> Is point within p3 section? false
   [junit4]   2> Is point within concave section? false
   [junit4]   2> Is point within p5 section? true


The point is within TWO parts of the composite polygon.  This is, of course, nonsense unless the point happens to be on a shared edge between the two.  But these two don't even share an edge.
I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.  I'll look at that next.","I'll look at that next.","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"233","230","5631","645","Ok, found the problem.  Cut and paste error in complex polygon building.  Resolves the first problem, confirming the second.","Ok, found the problem.","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"234","231","5631","645","Ok, found the problem.  Cut and paste error in complex polygon building.  Resolves the first problem, confirming the second.","Cut and paste error in complex polygon building.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"235","232","5631","645","Ok, found the problem.  Cut and paste error in complex polygon building.  Resolves the first problem, confirming the second.","Resolves the first problem, confirming the second.","kwright@metacarta.com","NULL","1","decision","0","0","0","0","1"
"236","233","5632","645","Commit d377e7fd34b4ace829ee6d4ba0486500aaef506b in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d377e7f ]
LUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.","Commit d377e7fd34b4ace829ee6d4ba0486500aaef506b in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d377e7f ]
LUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"237","234","5633","645","Commit 65a69b9757a6cd0a8e8be0ed08797643054634b1 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=65a69b9 ]
LUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.","Commit 65a69b9757a6cd0a8e8be0ed08797643054634b1 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=65a69b9 ]
LUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"203","235","5623","645","I augmented Michael McCandless's explain output to include relationship information, which then leads to the following line in the forensics dump from this failure:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true


From this it looked plausible that the cell is off the top of the world in z, and thus did not intersect with it for that reason.  But:


   [junit4]    >   world bounds=( minX=-1.0011188539924791 maxX=1.0011188539924791 minY=-1.0011188539924791 maxY=1.0011188539924791 minZ=-0.9977622920221051 maxZ=0.9977622920221051


So I'll have to work a bit harder to see why no intersection is detected.","I augmented Michael McCandless's explain output to include relationship information, which then leads to the following line in the forensics dump from this failure:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true


From this it looked plausible that the cell is off the top of the world in z, and thus did not intersect with it for that reason.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"204","236","5623","645","I augmented Michael McCandless's explain output to include relationship information, which then leads to the following line in the forensics dump from this failure:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true


From this it looked plausible that the cell is off the top of the world in z, and thus did not intersect with it for that reason.  But:


   [junit4]    >   world bounds=( minX=-1.0011188539924791 maxX=1.0011188539924791 minY=-1.0011188539924791 maxY=1.0011188539924791 minZ=-0.9977622920221051 maxZ=0.9977622920221051


So I'll have to work a bit harder to see why no intersection is detected.","But:


   [junit4]    >   world bounds=( minX=-1.0011188539924791 maxX=1.0011188539924791 minY=-1.0011188539924791 maxY=1.0011188539924791 minZ=-0.9977622920221051 maxZ=0.9977622920221051


So I'll have to work a bit harder to see why no intersection is detected.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"205","237","5624","645","Yay forensics!","Yay forensics!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"238","238","5645","648","+1, we should roll our own, hopefully correctly this time ","+1, we should roll our own, hopefully correctly this time","mikemccand","NULL","1","decision, pro","0","0","1","0","1"
"239","239","5646","648","It does seem buggy that even StrictMath shows this difference between versions. WTF is the point of StrictMath then?","It does seem buggy that even StrictMath shows this difference between versions.","rcmuir","NULL","1","issue","1","0","0","0","0"
"240","240","5646","648","It does seem buggy that even StrictMath shows this difference between versions. WTF is the point of StrictMath then?","WTF is the point of StrictMath then?","rcmuir","NULL","1","issue","1","0","0","0","0"
"241","241","5648","648","
This is a simple multiplication, computed using two well-known constants: pi and 180.0. How can this possibly be inexact??
See the linked openjdk issue. Looks like they changed it from division to inverse multiplication.
IMO at least StrictMath should not have this optimization...","This is a simple multiplication, computed using two well-known constants: pi and 180.0.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"242","242","5648","648","
This is a simple multiplication, computed using two well-known constants: pi and 180.0. How can this possibly be inexact??
See the linked openjdk issue. Looks like they changed it from division to inverse multiplication.
IMO at least StrictMath should not have this optimization...","How can this possibly be inexact??","rcmuir","NULL","1","issue","1","0","0","0","0"
"243","243","5648","648","
This is a simple multiplication, computed using two well-known constants: pi and 180.0. How can this possibly be inexact??
See the linked openjdk issue. Looks like they changed it from division to inverse multiplication.
IMO at least StrictMath should not have this optimization...","See the linked openjdk issue.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"244","244","5648","648","
This is a simple multiplication, computed using two well-known constants: pi and 180.0. How can this possibly be inexact??
See the linked openjdk issue. Looks like they changed it from division to inverse multiplication.
IMO at least StrictMath should not have this optimization...","Looks like they changed it from division to inverse multiplication.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"245","245","5648","648","
This is a simple multiplication, computed using two well-known constants: pi and 180.0. How can this possibly be inexact??
See the linked openjdk issue. Looks like they changed it from division to inverse multiplication.
IMO at least StrictMath should not have this optimization...","IMO at least StrictMath should not have this optimization...","rcmuir","NULL","1","con","0","0","0","1","0"
"246","246","5649","648","IMO at least StrictMath should not have this optimization...
+1 for that.","IMO at least StrictMath should not have this optimization...
+1 for that.","kwright@metacarta.com","NULL","1","pro","0","0","1","0","0"
"247","247","5650","648","StrictMath doesn't say anything about being constant in time, all it says is, basically:
To help ensure portability of Java programs, the definitions of some of the numeric functions in this package require that they produce the same results as certain published algorithms. [...] The Java math library is defined with respect to fdlibm version 5.3.
And fdlibm doesn't have these conversion methods, so it's not violating its spec? 
","StrictMath doesn't say anything about being constant in time, all it says is, basically:
To help ensure portability of Java programs, the definitions of some of the numeric functions in this package require that they produce the same results as certain published algorithms.","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"248","248","5650","648","StrictMath doesn't say anything about being constant in time, all it says is, basically:
To help ensure portability of Java programs, the definitions of some of the numeric functions in this package require that they produce the same results as certain published algorithms. [...] The Java math library is defined with respect to fdlibm version 5.3.
And fdlibm doesn't have these conversion methods, so it's not violating its spec? 
","[...] The Java math library is defined with respect to fdlibm version 5.3.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"249","249","5650","648","StrictMath doesn't say anything about being constant in time, all it says is, basically:
To help ensure portability of Java programs, the definitions of some of the numeric functions in this package require that they produce the same results as certain published algorithms. [...] The Java math library is defined with respect to fdlibm version 5.3.
And fdlibm doesn't have these conversion methods, so it's not violating its spec? 
","And fdlibm doesn't have these conversion methods, so it's not violating its spec?","dweiss","NULL","0",NULL,"0","0","0","0","0"
"250","250","5651","648","Then why does StrictMath.java have a separate toRadians method at all with strictfp?","Then why does StrictMath.java have a separate toRadians method at all with strictfp?","rcmuir","NULL","1","issue","1","0","0","0","0"
"251","251","5653","648","That's what I'm saying  I don't know! And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation. I would file a bug. 
strictfp is another issue; in reality you won't be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings? I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.","That's what I'm saying  I don't know!","dweiss","NULL","0",NULL,"0","0","0","0","0"
"252","252","5653","648","That's what I'm saying  I don't know! And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation. I would file a bug. 
strictfp is another issue; in reality you won't be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings? I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.","And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation.","dweiss","NULL","1","alternative","0","1","0","0","0"
"253","253","5653","648","That's what I'm saying  I don't know! And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation. I would file a bug. 
strictfp is another issue; in reality you won't be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings? I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.","I would file a bug.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"254","254","5653","648","That's what I'm saying  I don't know! And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation. I would file a bug. 
strictfp is another issue; in reality you won't be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings? I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.","strictfp is another issue; in reality you won't be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings?","dweiss","NULL","1","issue, con","1","0","0","1","0"
"255","255","5653","648","That's what I'm saying  I don't know! And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation. I would file a bug. 
strictfp is another issue; in reality you won't be able to keep floating point computations exact (unless you declare everything as strictfp) because of processor-dependent truncations and roundings? I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.","I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel  we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.","dweiss","NULL","1","issue","1","0","0","0","0"
"256","256","5654","648","I know what it does, i am saying it makes no sense to have the method there at all if its result may change.","I know what it does, i am saying it makes no sense to have the method there at all if its result may change.","rcmuir","NULL","1","con","0","0","0","1","0"
"257","257","5655","648","Commit a4bf526a62dbf5e2c3fed6d98112c71ed33e15d6 in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a4bf526 ]
LUCENE-7194: Roll our own toRadians() method, and also make it less likely we'll need to restaple the toString() tests.","Commit a4bf526a62dbf5e2c3fed6d98112c71ed33e15d6 in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a4bf526 ]
LUCENE-7194: Roll our own toRadians() method, and also make it less likely we'll need to restaple the toString() tests.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"258","258","5656","648","Commit f6be813308133de06e08717309750e1f47dd73d1 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6be813 ]
LUCENE-7194: Roll our own toRadians() method, and also make it less likely we'll need to restaple the toString() tests.","Commit f6be813308133de06e08717309750e1f47dd73d1 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6be813 ]
LUCENE-7194: Roll our own toRadians() method, and also make it less likely we'll need to restaple the toString() tests.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"259","259","5657","648","+1 to rolling our own in any case.
Committed changes to the spatial3d module for this purpose.  Not sure where else it's used?","+1 to rolling our own in any case.","kwright@metacarta.com","NULL","1","pro","0","0","1","0","0"
"260","260","5657","648","+1 to rolling our own in any case.
Committed changes to the spatial3d module for this purpose.  Not sure where else it's used?","Committed changes to the spatial3d module for this purpose.","kwright@metacarta.com","NULL","1","decision","0","0","0","0","1"
"261","261","5657","648","+1 to rolling our own in any case.
Committed changes to the spatial3d module for this purpose.  Not sure where else it's used?","Not sure where else it's used?","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"262","262","5658","648","Thanks Karl Wright.
I think we should just add this to our forbidden API list, then see what fails (because it's using these APIs), and correct them...","Thanks Karl Wright.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"263","263","5658","648","Thanks Karl Wright.
I think we should just add this to our forbidden API list, then see what fails (because it's using these APIs), and correct them...","I think we should just add this to our forbidden API list, then see what fails (because it's using these APIs), and correct them...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"264","264","5659","648","Michael McCandless How do I add these to the forbidden API list?","Michael McCandless How do I add these to the forbidden API list?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"265","265","5660","648","Karl Wright Oh, I think you just add it to lucene/tools/forbiddenApis/lucene.txt?  And then run ant precommit and you should see failures from places using these APIs...","Karl Wright Oh, I think you just add it to lucene/tools/forbiddenApis/lucene.txt?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"266","266","5660","648","Karl Wright Oh, I think you just add it to lucene/tools/forbiddenApis/lucene.txt?  And then run ant precommit and you should see failures from places using these APIs...","And then run ant precommit and you should see failures from places using these APIs...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"267","267","5661","648","Here's what it spits out:


[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:94)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:95)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:151)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:169)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.util.SloppyMath (SloppyMath.java:212)
[forbidden-apis] Scanned 2733 (and 585 related) class file(s) for forbidden API
invocations (in 2.98s), 9 error(s).

","Here's what it spits out:


[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:94)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:95)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:151)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:169)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.util.SloppyMath (SloppyMath.java:212)
[forbidden-apis] Scanned 2733 (and 585 related) class file(s) for forbidden API
invocations (in 2.98s), 9 error(s).","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"268","268","5662","648","Robert Muir: Is this still needed?  In SloppyMath.java I see the following:


  // haversin
  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.
  public static final double TO_RADIANS = Math.PI / 180D;
  public static final double TO_DEGREES = 180D / Math.PI;


... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?  Maybe Uwe Schindler knows?","Robert Muir: Is this still needed?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"269","269","5662","648","Robert Muir: Is this still needed?  In SloppyMath.java I see the following:


  // haversin
  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.
  public static final double TO_RADIANS = Math.PI / 180D;
  public static final double TO_DEGREES = 180D / Math.PI;


... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?  Maybe Uwe Schindler knows?","In SloppyMath.java I see the following:


  // haversin
  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.","kwright@metacarta.com","NULL","1","issue","1","0","0","0","0"
"270","270","5662","648","Robert Muir: Is this still needed?  In SloppyMath.java I see the following:


  // haversin
  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.
  public static final double TO_RADIANS = Math.PI / 180D;
  public static final double TO_DEGREES = 180D / Math.PI;


... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?  Maybe Uwe Schindler knows?","public static final double TO_RADIANS = Math.PI / 180D;
  public static final double TO_DEGREES = 180D / Math.PI;


... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?","kwright@metacarta.com","NULL","1","alternative","0","1","0","0","0"
"271","271","5662","648","Robert Muir: Is this still needed?  In SloppyMath.java I see the following:


  // haversin
  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.
  public static final double TO_RADIANS = Math.PI / 180D;
  public static final double TO_DEGREES = 180D / Math.PI;


... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?  Maybe Uwe Schindler knows?","Maybe Uwe Schindler knows?","kwright@metacarta.com","NULL","0",NULL,"0","0","0","0","0"
"272","272","5664","648","Here's the patch","Here's the patch","kwright@metacarta.com","NULL","1","alternative","0","1","0","0","0"
"273","273","5665","648","Commit b11e48c7553daed127b1b231641d7367a09aed1b in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b11e48c ]
LUCENE-7194: Ban Math.toRadians and Math.toDegrees","Commit b11e48c7553daed127b1b231641d7367a09aed1b in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b11e48c ]
LUCENE-7194: Ban Math.toRadians and Math.toDegrees","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"274","274","5666","648","Commit 40ca6f1d64ab8ec2e873c2a6c6815ca449b046a4 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=40ca6f1 ]
LUCENE-7194: Ban Math.toRadians and Math.toDegrees","Commit 40ca6f1d64ab8ec2e873c2a6c6815ca449b046a4 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=40ca6f1 ]
LUCENE-7194: Ban Math.toRadians and Math.toDegrees","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"275","275","5667","648","Commit 388d388c990c8d9e05ec9ba9bc881fdc921e734b in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=388d388 ]
LUCENE-7194: Ban Math.toRadians, Math.toDegrees","Commit 388d388c990c8d9e05ec9ba9bc881fdc921e734b in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=388d388 ]
LUCENE-7194: Ban Math.toRadians, Math.toDegrees","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"276","276","5668","648","Thank you Karl Wright!","Thank you Karl Wright!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"277","277","6675","749","Yikes, I'll take it.","Yikes, I'll take it.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"278","278","6676","749","Patch.
I also hit and fixed a separate bug in InetAddressPoint.newSetQuery where it hit an exception if you tried to pass more than one InetAddress in the set   Sheesh.","Patch.","mikemccand","NULL","0","alternative","0","1","0","0","0"
"279","279","6676","749","Patch.
I also hit and fixed a separate bug in InetAddressPoint.newSetQuery where it hit an exception if you tried to pass more than one InetAddress in the set   Sheesh.","I also hit and fixed a separate bug in InetAddressPoint.newSetQuery where it hit an exception if you tried to pass more than one InetAddress in the set   Sheesh.","mikemccand","NULL","1","decision","0","0","0","0","1"
"280","280","6677","749","s/comparsion/comparison/ but otherwise +1 to the patch","s/comparsion/comparison/ but otherwise +1 to the patch","jpountz","NULL","1","pro, con","0","0","1","1","0"
"281","281","6678","749","nice find!","nice find!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"282","282","6679","749","Commit 770e508fd3d908e9bf7997361299af96aa437b75 in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=770e508 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal","Commit 770e508fd3d908e9bf7997361299af96aa437b75 in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=770e508 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"283","283","6680","749","Commit 9f8fe1239afb7089b9f85432d076bdd778d3cd50 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9f8fe12 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal","Commit 9f8fe1239afb7089b9f85432d076bdd778d3cd50 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9f8fe12 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"284","284","6681","749","Commit f0ed113bb6ebed008bc9aa5954e12de98d62c951 in lucene-solr's branch refs/heads/branch_6_0 from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f0ed113 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal","Commit f0ed113bb6ebed008bc9aa5954e12de98d62c951 in lucene-solr's branch refs/heads/branch_6_0 from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f0ed113 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"285","285","6682","749","What a nice catch, thanks Adrien Grand!","What a nice catch, thanks Adrien Grand!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"318","286","7370","827","I have no problem committing and pushing this soon. I just want to be sure that maybe others also tested it with his/her Git installation.","I just want to be sure that maybe others also tested it with his/her Git installation.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"319","287","7372","827","Commit 424a647af4d093915108221bcd4390989303b426 in lucene-solr's branch refs/heads/master from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=424a647 ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change","Commit 424a647af4d093915108221bcd4390989303b426 in lucene-solr's branch refs/heads/master from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=424a647 ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"320","288","7373","827","Commit 0ef36fcdd107084a2ac3156943f01eb5f7dd9c74 in lucene-solr's branch refs/heads/branch_5x from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0ef36fc ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change","Commit 0ef36fcdd107084a2ac3156943f01eb5f7dd9c74 in lucene-solr's branch refs/heads/branch_5x from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0ef36fc ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"321","289","7374","827","I committed this. I think we should open another issues about the multiple build dir change.","I committed this.","thetaphi","NULL","1","decision","0","0","0","0","1"
"322","290","7374","827","I committed this. I think we should open another issues about the multiple build dir change.","I think we should open another issues about the multiple build dir change.","thetaphi","NULL","1","issue","1","0","0","0","0"
"323","291","7375","827","Commit b0e769c3ec598dd7398cc8df123bc4c41069e2c3 in lucene-solr's branch refs/heads/branch_5_4 from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b0e769c ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change","Commit b0e769c3ec598dd7398cc8df123bc4c41069e2c3 in lucene-solr's branch refs/heads/branch_5_4 from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b0e769c ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"286","292","7357","827","Patch.","Patch.","thetaphi","NULL","0","alternative","0","1","0","0","0"
"287","293","7358","827","this is a really good idea. otherwise we are going to get bug reports that look like data corruption:

   [junit4]    > Caused by: java.lang.IllegalArgumentException: Version is too old, should be at least 2 (got 0)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.checkVersion(PackedInts.java:77)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.getDecoder(PackedInts.java:742)
   [junit4]    >     at org.apache.lucene.codecs.lucene50.ForUtil.<clinit>(ForUtil.java:64)
   [junit4]    >     ... 50 more

","this is a really good idea.","rcmuir","NULL","1","pro","0","0","1","0","0"
"288","294","7358","827","this is a really good idea. otherwise we are going to get bug reports that look like data corruption:

   [junit4]    > Caused by: java.lang.IllegalArgumentException: Version is too old, should be at least 2 (got 0)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.checkVersion(PackedInts.java:77)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.getDecoder(PackedInts.java:742)
   [junit4]    >     at org.apache.lucene.codecs.lucene50.ForUtil.<clinit>(ForUtil.java:64)
   [junit4]    >     ... 50 more

","otherwise we are going to get bug reports that look like data corruption:

   [junit4]    > Caused by: java.lang.IllegalArgumentException: Version is too old, should be at least 2 (got 0)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.checkVersion(PackedInts.java:77)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.getDecoder(PackedInts.java:742)
   [junit4]    >     at org.apache.lucene.codecs.lucene50.ForUtil.<clinit>(ForUtil.java:64)
   [junit4]    >     ... 50 more","rcmuir","NULL","1","con","0","0","0","1","0"
"289","295","7359","827","This is interesting because I don't see this behavior. When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?


$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 35579 Jan 26 14:55 build.xml

# historical hash from Sep. 5:
$ git checkout 8f5259b4ff2d5f0
$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 25633 Jan 26 14:56 build.xml


","This is interesting because I don't see this behavior.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"290","296","7359","827","This is interesting because I don't see this behavior. When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?


$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 35579 Jan 26 14:55 build.xml

# historical hash from Sep. 5:
$ git checkout 8f5259b4ff2d5f0
$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 25633 Jan 26 14:56 build.xml


","When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?","dweiss","NULL","0",NULL,"0","0","0","0","0"
"291","297","7359","827","This is interesting because I don't see this behavior. When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?


$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 35579 Jan 26 14:55 build.xml

# historical hash from Sep. 5:
$ git checkout 8f5259b4ff2d5f0
$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 25633 Jan 26 14:56 build.xml


","$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 35579 Jan 26 14:55 build.xml

# historical hash from Sep. 5:
$ git checkout 8f5259b4ff2d5f0
$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 25633 Jan 26 14:56 build.xml","dweiss","NULL","0",NULL,"0","0","0","0","0"
"292","298","7361","827","I think it's your build folders that are in an insane state, not the checked out files? I'm not against this patch, I just find it odd you're experiencing the issue because I've never had any problem with it (yes, git will not wipe out any ignored files automatically  these are ignored after all  but it'll switch any versioned files and update timestamps properly). 
Please make this an opt-out feature. I prefer to git clean the build folder myself, actually (faster than ant).","I think it's your build folders that are in an insane state, not the checked out files?","dweiss","NULL","0",NULL,"0","0","0","0","0"
"293","299","7361","827","I think it's your build folders that are in an insane state, not the checked out files? I'm not against this patch, I just find it odd you're experiencing the issue because I've never had any problem with it (yes, git will not wipe out any ignored files automatically  these are ignored after all  but it'll switch any versioned files and update timestamps properly). 
Please make this an opt-out feature. I prefer to git clean the build folder myself, actually (faster than ant).","I'm not against this patch, I just find it odd you're experiencing the issue because I've never had any problem with it (yes, git will not wipe out any ignored files automatically  these are ignored after all  but it'll switch any versioned files and update timestamps properly).","dweiss","NULL","1","con","0","0","0","1","0"
"294","300","7361","827","I think it's your build folders that are in an insane state, not the checked out files? I'm not against this patch, I just find it odd you're experiencing the issue because I've never had any problem with it (yes, git will not wipe out any ignored files automatically  these are ignored after all  but it'll switch any versioned files and update timestamps properly). 
Please make this an opt-out feature. I prefer to git clean the build folder myself, actually (faster than ant).","Please make this an opt-out feature.","dweiss","NULL","1","alternative","0","1","0","0","0"
"295","301","7361","827","I think it's your build folders that are in an insane state, not the checked out files? I'm not against this patch, I just find it odd you're experiencing the issue because I've never had any problem with it (yes, git will not wipe out any ignored files automatically  these are ignored after all  but it'll switch any versioned files and update timestamps properly). 
Please make this an opt-out feature. I prefer to git clean the build folder myself, actually (faster than ant).","I prefer to git clean the build folder myself, actually (faster than ant).","dweiss","NULL","1","pro","0","0","1","0","0"
"296","302","7362","827","It happened to me twice this morning already. Just doing things like running tests and forgetting to clean when switching branches.","It happened to me twice this morning already.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"297","303","7362","827","It happened to me twice this morning already. Just doing things like running tests and forgetting to clean when switching branches.","Just doing things like running tests and forgetting to clean when switching branches.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"298","304","7363","827","Ok, but this isn't related to timestamps on version-controlled files, only on the fact that build artefacts coexist? If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.","Ok, but this isn't related to timestamps on version-controlled files, only on the fact that build artefacts coexist?","dweiss","NULL","0",NULL,"0","0","0","0","0"
"299","305","7363","827","Ok, but this isn't related to timestamps on version-controlled files, only on the fact that build artefacts coexist? If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.","If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder.","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"300","306","7363","827","Ok, but this isn't related to timestamps on version-controlled files, only on the fact that build artefacts coexist? If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.","This way you could actually switch a branch and then continue working as usual because it should work flawlessly.","dweiss","NULL","1","pro","0","0","1","0","0"
"301","307","7364","827","...have branch-dedicated build folder
I like that idea!   I guess it's manually possible now with -Dbuild.dir=my_branch_dir?","...have branch-dedicated build folder
I like that idea!","ehatcher","NULL","1","pro","0","0","1","0","0"
"302","308","7364","827","...have branch-dedicated build folder
I like that idea!   I guess it's manually possible now with -Dbuild.dir=my_branch_dir?","I guess it's manually possible now with -Dbuild.dir=my_branch_dir?","ehatcher","NULL","1","alternative","0","1","0","0","0"
"303","309","7365","827","
If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.
This is an interesting idea. But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans). I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on. ","If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"304","310","7365","827","
If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.
This is an interesting idea. But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans). I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on. ","This way you could actually switch a branch and then continue working as usual because it should work flawlessly.","rcmuir","NULL","1","pro","0","0","1","0","0"
"305","311","7365","827","
If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.
This is an interesting idea. But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans). I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on. ","This is an interesting idea.","rcmuir","NULL","1","pro","0","0","1","0","0"
"306","312","7365","827","
If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.
This is an interesting idea. But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans). I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on. ","But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"307","313","7365","827","
If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder. This way you could actually switch a branch and then continue working as usual because it should work flawlessly.
This is an interesting idea. But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans). I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on. ","I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"308","314","7367","827","Yes, absolutely. Ant clean should really clean everything. As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.) and restores a pristine checkout state. Just in case somebody finds it useful.","Yes, absolutely.","dweiss","NULL","1","pro","0","0","1","0","0"
"309","315","7367","827","Yes, absolutely. Ant clean should really clean everything. As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.) and restores a pristine checkout state. Just in case somebody finds it useful.","Ant clean should really clean everything.","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"310","316","7367","827","Yes, absolutely. Ant clean should really clean everything. As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.) and restores a pristine checkout state. Just in case somebody finds it useful.","As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.)","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"311","317","7367","827","Yes, absolutely. Ant clean should really clean everything. As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.) and restores a pristine checkout state. Just in case somebody finds it useful.","and restores a pristine checkout state.","dweiss","NULL","1","pro","0","0","1","0","0"
"312","318","7367","827","Yes, absolutely. Ant clean should really clean everything. As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.) and restores a pristine checkout state. Just in case somebody finds it useful.","Just in case somebody finds it useful.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"313","319","7368","827","+1 to close this trap ASAP.  The errors you hit when ant doesn't realize you've switched git branches are awful.","+1 to close this trap ASAP.","mikemccand","NULL","1","pro","0","0","1","0","0"
"314","320","7368","827","+1 to close this trap ASAP.  The errors you hit when ant doesn't realize you've switched git branches are awful.","The errors you hit when ant doesn't realize you've switched git branches are awful.","mikemccand","NULL","1","issue","1","0","0","0","0"
"315","321","7369","827","Can we iterate by committing this solution first, then working on the separate build directory as a followup.
Again, the current situation is a real problem, because the errors you see look like corruption.","Can we iterate by committing this solution first, then working on the separate build directory as a followup.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"316","322","7369","827","Can we iterate by committing this solution first, then working on the separate build directory as a followup.
Again, the current situation is a real problem, because the errors you see look like corruption.","Again, the current situation is a real problem, because the errors you see look like corruption.","rcmuir","NULL","1","issue","1","0","0","0","0"
"317","323","7370","827","I have no problem committing and pushing this soon. I just want to be sure that maybe others also tested it with his/her Git installation.","I have no problem committing and pushing this soon.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"324","324","8480","920","Patch of 11 Nov 2015.
Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.
This is incomplete, there no tests.
MinShouldMatchSumScorer only has the disjunctions done.
For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.
SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","Patch of 11 Nov 2015.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"325","325","8480","920","Patch of 11 Nov 2015.
Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.
This is incomplete, there no tests.
MinShouldMatchSumScorer only has the disjunctions done.
For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.
SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"326","326","8480","920","Patch of 11 Nov 2015.
Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.
This is incomplete, there no tests.
MinShouldMatchSumScorer only has the disjunctions done.
For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.
SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","This is incomplete, there no tests.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"327","327","8480","920","Patch of 11 Nov 2015.
Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.
This is incomplete, there no tests.
MinShouldMatchSumScorer only has the disjunctions done.
For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.
SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","MinShouldMatchSumScorer only has the disjunctions done.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"328","328","8480","920","Patch of 11 Nov 2015.
Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.
This is incomplete, there no tests.
MinShouldMatchSumScorer only has the disjunctions done.
For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.
SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"329","329","8480","920","Patch of 11 Nov 2015.
Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.
This is incomplete, there no tests.
MinShouldMatchSumScorer only has the disjunctions done.
For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.
SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"330","330","8481","920","The independence that is assumed is normally not there. However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.
But so would the current worst-case approach?","The independence that is assumed is normally not there.","jpountz","NULL","1","con","0","0","0","1","0"
"331","331","8481","920","The independence that is assumed is normally not there. However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.
But so would the current worst-case approach?","However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.","jpountz","NULL","1","pro","0","0","1","0","0"
"332","332","8481","920","The independence that is assumed is normally not there. However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.
But so would the current worst-case approach?","But so would the current worst-case approach?","jpountz","NULL","1","issue","1","0","0","0","0"
"333","333","8483","920","That one is actually solved nowadays by the two phase approach.
I'll think of a better example later.","That one is actually solved nowadays by the two phase approach.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"334","334","8483","920","That one is actually solved nowadays by the two phase approach.
I'll think of a better example later.","I'll think of a better example later.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"335","335","8484","920","There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates. However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.
It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries. However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries.","There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates.","spo","NULL","1","alternative, pro","0","1","1","0","0"
"336","336","8484","920","There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates. However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.
It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries. However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries.","However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.","spo","NULL","1","con","0","0","0","1","0"
"337","337","8484","920","There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates. However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.
It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries. However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries.","It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries.","spo","NULL","1","pro, con","0","0","1","1","0"
"338","338","8484","920","There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates. However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.
It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries. However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries.","However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries.","spo","NULL","1","alternative, pro","0","1","1","0","0"
"339","339","8485","920","Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.
I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.
For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.
Would it be worthwhile to open a separate issue for that?","Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"340","340","8485","920","Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.
I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.
For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.
Would it be worthwhile to open a separate issue for that?","I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"341","341","8485","920","Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.
I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.
For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.
Would it be worthwhile to open a separate issue for that?","For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"342","342","8485","920","Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.
I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.
For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.
Would it be worthwhile to open a separate issue for that?","Would it be worthwhile to open a separate issue for that?","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"343","343","8486","920","Patch of 15 Nov 2015.
Resolve conflicts after LUCENE-6276.
I tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.
This could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later.","Patch of 15 Nov 2015.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"344","344","8486","920","Patch of 15 Nov 2015.
Resolve conflicts after LUCENE-6276.
I tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.
This could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later.","Resolve conflicts after LUCENE-6276.","paul.elschot@xs4all.nl","NULL","1","decision","0","0","0","0","1"
"345","345","8486","920","Patch of 15 Nov 2015.
Resolve conflicts after LUCENE-6276.
I tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.
This could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later.","I tried this patch with the wikimedium5m benchmark.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"346","346","8486","920","Patch of 15 Nov 2015.
Resolve conflicts after LUCENE-6276.
I tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.
This could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later.","This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"347","347","8486","920","Patch of 15 Nov 2015.
Resolve conflicts after LUCENE-6276.
I tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.
This could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later.","This could be because the patch here should have influence for more complex queries than the one in the benchmark.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"348","348","8486","920","Patch of 15 Nov 2015.
Resolve conflicts after LUCENE-6276.
I tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.
This could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later.","I might try to add more complex queries to the benchmark later.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"349","349","8487","920","Here is the benchmark output, it might be good for future reference:


                    TaskQPS baseline      StdDevQPS my_modified_version      StdDev                Pct diff
                HighTerm      178.20      (1.8\%)      174.50      (5.6\%)   -2.1\% (  -9\% -    5\%)
                 MedTerm      641.36      (1.6\%)      630.32      (4.9\%)   -1.7\% (  -8\% -    4\%)
              OrHighHigh       57.02      (5.5\%)       56.32      (6.6\%)   -1.2\% ( -12\% -   11\%)
               OrHighMed      107.80      (5.2\%)      106.89      (6.2\%)   -0.8\% ( -11\% -   11\%)
             AndHighHigh      100.02      (2.3\%)       99.34      (0.7\%)   -0.7\% (  -3\% -    2\%)
                 LowTerm     2477.28      (3.0\%)     2463.27      (5.5\%)   -0.6\% (  -8\% -    8\%)
              AndHighMed      627.58      (1.5\%)      625.22      (1.2\%)   -0.4\% (  -3\% -    2\%)
              HighPhrase       81.21      (4.2\%)       80.98      (4.3\%)   -0.3\% (  -8\% -    8\%)
               OrHighLow      136.70      (3.1\%)      136.35      (2.1\%)   -0.3\% (  -5\% -    5\%)
               LowPhrase      181.55      (2.2\%)      181.09      (2.0\%)   -0.3\% (  -4\% -    4\%)
         MedSloppyPhrase       56.03      (2.9\%)       55.93      (3.1\%)   -0.2\% (  -5\% -    5\%)
             MedSpanNear       52.77      (1.7\%)       52.68      (2.6\%)   -0.2\% (  -4\% -    4\%)
         LowSloppyPhrase      106.15      (2.9\%)      106.01      (3.1\%)   -0.1\% (  -6\% -    6\%)
               MedPhrase       39.38      (3.8\%)       39.36      (3.3\%)   -0.1\% (  -6\% -    7\%)
                  Fuzzy1      137.14      (2.1\%)      137.06      (1.5\%)   -0.1\% (  -3\% -    3\%)
                  Fuzzy2       79.28      (1.9\%)       79.25      (1.5\%)   -0.0\% (  -3\% -    3\%)
             LowSpanNear       94.38      (1.7\%)       94.35      (2.8\%)   -0.0\% (  -4\% -    4\%)
            OrNotHighMed      444.12      (1.7\%)      444.36      (1.2\%)    0.1\% (  -2\% -    2\%)
              AndHighLow     1878.59      (2.0\%)     1880.20      (1.9\%)    0.1\% (  -3\% -    4\%)
                 Respell      106.47      (1.9\%)      106.62      (1.7\%)    0.1\% (  -3\% -    3\%)
            OrNotHighLow     1831.85      (1.7\%)     1834.68      (1.3\%)    0.2\% (  -2\% -    3\%)
           OrNotHighHigh       69.75      (1.6\%)       69.91      (1.4\%)    0.2\% (  -2\% -    3\%)
            HighSpanNear       36.38      (2.8\%)       36.47      (3.8\%)    0.3\% (  -6\% -    7\%)
        HighSloppyPhrase       45.58      (3.6\%)       45.70      (3.5\%)    0.3\% (  -6\% -    7\%)
            OrHighNotLow       65.78      (7.0\%)       66.03      (8.4\%)    0.4\% ( -14\% -   16\%)
                 Prefix3      448.85      (3.5\%)      450.67      (3.8\%)    0.4\% (  -6\% -    8\%)
                Wildcard      114.35      (4.8\%)      115.02      (4.6\%)    0.6\% (  -8\% -   10\%)
                  IntNRQ       23.48      (7.4\%)       23.71      (7.7\%)    1.0\% ( -13\% -   17\%)
                PKLookup      360.70      (1.7\%)      364.91      (3.1\%)    1.2\% (  -3\% -    6\%)
            OrHighNotMed      178.99      (7.2\%)      181.91      (8.2\%)    1.6\% ( -12\% -   18\%)
           OrHighNotHigh       39.78      (7.1\%)       40.63      (7.5\%)    2.1\% ( -11\% -   18\%)


","Here is the benchmark output, it might be good for future reference:


                    TaskQPS baseline      StdDevQPS my_modified_version      StdDev                Pct diff
                HighTerm      178.20      (1.8\%)      174.50      (5.6\%)   -2.1\% (  -9\% -    5\%)
                 MedTerm      641.36      (1.6\%)      630.32      (4.9\%)   -1.7\% (  -8\% -    4\%)
              OrHighHigh       57.02      (5.5\%)       56.32      (6.6\%)   -1.2\% ( -12\% -   11\%)
               OrHighMed      107.80      (5.2\%)      106.89      (6.2\%)   -0.8\% ( -11\% -   11\%)
             AndHighHigh      100.02      (2.3\%)       99.34      (0.7\%)   -0.7\% (  -3\% -    2\%)
                 LowTerm     2477.28      (3.0\%)     2463.27      (5.5\%)   -0.6\% (  -8\% -    8\%)
              AndHighMed      627.58      (1.5\%)      625.22      (1.2\%)   -0.4\% (  -3\% -    2\%)
              HighPhrase       81.21      (4.2\%)       80.98      (4.3\%)   -0.3\% (  -8\% -    8\%)
               OrHighLow      136.70      (3.1\%)      136.35      (2.1\%)   -0.3\% (  -5\% -    5\%)
               LowPhrase      181.55      (2.2\%)      181.09      (2.0\%)   -0.3\% (  -4\% -    4\%)
         MedSloppyPhrase       56.03      (2.9\%)       55.93      (3.1\%)   -0.2\% (  -5\% -    5\%)
             MedSpanNear       52.77      (1.7\%)       52.68      (2.6\%)   -0.2\% (  -4\% -    4\%)
         LowSloppyPhrase      106.15      (2.9\%)      106.01      (3.1\%)   -0.1\% (  -6\% -    6\%)
               MedPhrase       39.38      (3.8\%)       39.36      (3.3\%)   -0.1\% (  -6\% -    7\%)
                  Fuzzy1      137.14      (2.1\%)      137.06      (1.5\%)   -0.1\% (  -3\% -    3\%)
                  Fuzzy2       79.28      (1.9\%)       79.25      (1.5\%)   -0.0\% (  -3\% -    3\%)
             LowSpanNear       94.38      (1.7\%)       94.35      (2.8\%)   -0.0\% (  -4\% -    4\%)
            OrNotHighMed      444.12      (1.7\%)      444.36      (1.2\%)    0.1\% (  -2\% -    2\%)
              AndHighLow     1878.59      (2.0\%)     1880.20      (1.9\%)    0.1\% (  -3\% -    4\%)
                 Respell      106.47      (1.9\%)      106.62      (1.7\%)    0.1\% (  -3\% -    3\%)
            OrNotHighLow     1831.85      (1.7\%)     1834.68      (1.3\%)    0.2\% (  -2\% -    3\%)
           OrNotHighHigh       69.75      (1.6\%)       69.91      (1.4\%)    0.2\% (  -2\% -    3\%)
            HighSpanNear       36.38      (2.8\%)       36.47      (3.8\%)    0.3\% (  -6\% -    7\%)
        HighSloppyPhrase       45.58      (3.6\%)       45.70      (3.5\%)    0.3\% (  -6\% -    7\%)
            OrHighNotLow       65.78      (7.0\%)       66.03      (8.4\%)    0.4\% ( -14\% -   16\%)
                 Prefix3      448.85      (3.5\%)      450.67      (3.8\%)    0.4\% (  -6\% -    8\%)
                Wildcard      114.35      (4.8\%)      115.02      (4.6\%)    0.6\% (  -8\% -   10\%)
                  IntNRQ       23.48      (7.4\%)       23.71      (7.7\%)    1.0\% ( -13\% -   17\%)
                PKLookup      360.70      (1.7\%)      364.91      (3.1\%)    1.2\% (  -3\% -    6\%)
            OrHighNotMed      178.99      (7.2\%)      181.91      (8.2\%)    1.6\% ( -12\% -   18\%)
           OrHighNotHigh       39.78      (7.1\%)       40.63      (7.5\%)    2.1\% ( -11\% -   18\%)","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"363","350","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","Won't this change have the prospect of increasing the amount of GC due to all these extra objects?","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"364","351","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"365","352","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"366","353","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","The Term(String field, String text) constructor makes a new BytesRef anyway.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"367","354","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","so that users like Solr can exploit the fact that their code won't be making any further use of the input term?","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"368","355","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"369","356","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","There this call could be removed also.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"370","357","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","I'm not familiar with Solr code, so in case there is another impact there, please holler.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"371","358","9005","982","2nd patch of 3 October 2015.
In addition to the previous patch, this also

deletes the Term cloning in PhraseQuery,
adds a Term constructor from a BytesRefBuilder, and
removes BytesRef copying at Term construction from Solr's FieldType, SolrIndexSearcher, FacetField and SimpleMLTQParser.

","2nd patch of 3 October 2015.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"372","359","9005","982","2nd patch of 3 October 2015.
In addition to the previous patch, this also

deletes the Term cloning in PhraseQuery,
adds a Term constructor from a BytesRefBuilder, and
removes BytesRef copying at Term construction from Solr's FieldType, SolrIndexSearcher, FacetField and SimpleMLTQParser.

","In addition to the previous patch, this also

deletes the Term cloning in PhraseQuery,
adds a Term constructor from a BytesRefBuilder, and
removes BytesRef copying at Term construction from Solr's FieldType, SolrIndexSearcher, FacetField and SimpleMLTQParser.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"373","360","9006","982","I like how the patch makes things simpler. I'll wait a bit before committing to give other people a chance to comment. Can you also remove the explicit cloning that we added in LUCENE-6435?","I like how the patch makes things simpler.","jpountz","NULL","1","pro","0","0","1","0","0"
"374","361","9006","982","I like how the patch makes things simpler. I'll wait a bit before committing to give other people a chance to comment. Can you also remove the explicit cloning that we added in LUCENE-6435?","I'll wait a bit before committing to give other people a chance to comment.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"375","362","9006","982","I like how the patch makes things simpler. I'll wait a bit before committing to give other people a chance to comment. Can you also remove the explicit cloning that we added in LUCENE-6435?","Can you also remove the explicit cloning that we added in LUCENE-6435?","jpountz","NULL","1","alternative","0","1","0","0","0"
"376","363","9007","982","Can you also remove the explicit cloning that we added in LUCENE-6435?
I tried, but then a test fails.
Perhaps this is because a BytesRef is passed ClassificationResult there.","Can you also remove the explicit cloning that we added in LUCENE-6435?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"377","364","9007","982","Can you also remove the explicit cloning that we added in LUCENE-6435?
I tried, but then a test fails.
Perhaps this is because a BytesRef is passed ClassificationResult there.","I tried, but then a test fails.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"378","365","9007","982","Can you also remove the explicit cloning that we added in LUCENE-6435?
I tried, but then a test fails.
Perhaps this is because a BytesRef is passed ClassificationResult there.","Perhaps this is because a BytesRef is passed ClassificationResult there.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"379","366","9008","982","do you mean the BytesRef.deepCopyOfat https://github.com/apache/lucene-solr/blob/trunk/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java#L154 ?
If yes, that's because the reference is updated and used in the ClassificationResult. I'll see if I can simplify that.","do you mean the BytesRef.deepCopyOfat https://github.com/apache/lucene-solr/blob/trunk/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java#L154 ?","teofili","NULL","0",NULL,"0","0","0","0","0"
"380","367","9008","982","do you mean the BytesRef.deepCopyOfat https://github.com/apache/lucene-solr/blob/trunk/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java#L154 ?
If yes, that's because the reference is updated and used in the ClassificationResult. I'll see if I can simplify that.","If yes, that's because the reference is updated and used in the ClassificationResult.","teofili","NULL","1","con","0","0","0","1","0"
"381","368","9008","982","do you mean the BytesRef.deepCopyOfat https://github.com/apache/lucene-solr/blob/trunk/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java#L154 ?
If yes, that's because the reference is updated and used in the ClassificationResult. I'll see if I can simplify that.","I'll see if I can simplify that.","teofili","NULL","1","issue","1","0","0","0","0"
"382","369","9009","982","after a quick look it doesn't seem removing the deep copy in favour of creating new BytesRef would improve anything, actually it'd be slightly worse. I would say let's keep that.","after a quick look it doesn't seem removing the deep copy in favour of creating new BytesRef would improve anything, actually it'd be slightly worse.","teofili","NULL","1","alternative, con","0","1","0","1","0"
"383","370","9009","982","after a quick look it doesn't seem removing the deep copy in favour of creating new BytesRef would improve anything, actually it'd be slightly worse. I would say let's keep that.","I would say let's keep that.","teofili","NULL","1","alternative","0","1","0","0","0"
"384","371","9010","982","One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.
Term.bytes() can also be passed to the ClassificationResult.
The patch here has this javadoc at Term.bytes():
/** Returns the bytes of this term, these should not be modified. */","One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"385","372","9010","982","One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.
Term.bytes() can also be passed to the ClassificationResult.
The patch here has this javadoc at Term.bytes():
/** Returns the bytes of this term, these should not be modified. */","Term.bytes() can also be passed to the ClassificationResult.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"386","373","9010","982","One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.
Term.bytes() can also be passed to the ClassificationResult.
The patch here has this javadoc at Term.bytes():
/** Returns the bytes of this term, these should not be modified. */","The patch here has this javadoc at Term.bytes():
/** Returns the bytes of this term, these should not be modified.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"387","374","9010","982","One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.
Term.bytes() can also be passed to the ClassificationResult.
The patch here has this javadoc at Term.bytes():
/** Returns the bytes of this term, these should not be modified. */","*/","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"388","375","9011","982","One could also create the Term in the loop and pass that
good point indeed, in the end the underlying methods all create a new Term from the same BytesRefand field name, so that should be better than the current solution, so we should pass the Termcreated from within the loop to the methods to calculate prior and likelihood in SimpleNaiveBayesClassifier.","One could also create the Term in the loop and pass that
good point indeed, in the end the underlying methods all create a new Term from the same BytesRefand field name, so that should be better than the current solution, so we should pass the Termcreated from within the loop to the methods to calculate prior and likelihood in SimpleNaiveBayesClassifier.","teofili","NULL","1","alternative, pro","0","1","1","0","0"
"389","376","9012","982","attached the Paul Elschot's patch modified to pass the Terminstead of the BytesRefin SimpleNaiveBayesClassifier.","attached the Paul Elschot's patch modified to pass the Terminstead of the BytesRefin SimpleNaiveBayesClassifier.","teofili","NULL","1","alternative","0","1","0","0","0"
"390","377","9013","982","from my perspective we can proceed committing this patch. Adrien, Paul what do you think?","from my perspective we can proceed committing this patch.","teofili","NULL","1","decision","0","0","0","0","1"
"391","378","9013","982","from my perspective we can proceed committing this patch. Adrien, Paul what do you think?","Adrien, Paul what do you think?","teofili","NULL","0",NULL,"0","0","0","0","0"
"392","379","9014","982","The patch of 14 October LGTM, and tests pass here.","The patch of 14 October LGTM, and tests pass here.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"393","380","9015","982","+1 on my end as well","+1 on my end as well","jpountz","NULL","1","pro","0","0","1","0","0"
"394","381","9016","982","I will run another round of testing and inspections and commit the latest patch if no issues come up.","I will run another round of testing and inspections and commit the latest patch if no issues come up.","teofili","NULL","1","decision","0","0","0","0","1"
"395","382","9017","982","Commit 1709576 from Tommaso Teofili in branch 'dev/trunk'
[ https://svn.apache.org/r1709576 ]
LUCENE-6821 - TermQuery's constructors should clone the incoming term","Commit 1709576 from Tommaso Teofili in branch 'dev/trunk'
[ https://svn.apache.org/r1709576 ]
LUCENE-6821 - TermQuery's constructors should clone the incoming term","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"396","383","9018","982","committed and resolved, thanks Paul Elschot for your patch and Adrien Grand for your help.","committed and resolved, thanks Paul Elschot for your patch and Adrien Grand for your help.","teofili","NULL","1","decision","0","0","0","0","1"
"397","384","9019","982","Do we want to backport it to 5.x? I don't have a strong opinion but I like to get changes into the hands of our users as soon as possible, and I don't see how this one could break existing applications, it might just perform a few extra copies?","Do we want to backport it to 5.x?","jpountz","NULL","1","alternative","0","1","0","0","0"
"398","385","9019","982","Do we want to backport it to 5.x? I don't have a strong opinion but I like to get changes into the hands of our users as soon as possible, and I don't see how this one could break existing applications, it might just perform a few extra copies?","I don't have a strong opinion but I like to get changes into the hands of our users as soon as possible, and I don't see how this one could break existing applications, it might just perform a few extra copies?","jpountz","NULL","1","pro","0","0","1","0","0"
"399","386","9020","982","Do we want to backport it to 5.x?
+1, I consider this a bug fix, which should certainly go back to 5.x.","Do we want to backport it to 5.x?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"400","387","9020","982","Do we want to backport it to 5.x?
+1, I consider this a bug fix, which should certainly go back to 5.x.","+1, I consider this a bug fix, which should certainly go back to 5.x.","mikemccand","NULL","1","pro","0","0","1","0","0"
"401","388","9021","982","agreed, I've reopened and will backport it.","agreed, I've reopened and will backport it.","teofili","NULL","1","pro, decision","0","0","1","0","1"
"402","389","9022","982","Commit 1709683 from Yonik Seeley in branch 'dev/trunk'
[ https://svn.apache.org/r1709683 ]
LUCENE-6821: remove unnecessary term clones","Commit 1709683 from Yonik Seeley in branch 'dev/trunk'
[ https://svn.apache.org/r1709683 ]
LUCENE-6821: remove unnecessary term clones","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"403","390","9023","982","Commit 1709780 from Tommaso Teofili in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1709780 ]
LUCENE-6821 - TermQuery's constructors should clone the incoming term (backport branch 5.x)","Commit 1709780 from Tommaso Teofili in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1709780 ]
LUCENE-6821 - TermQuery's constructors should clone the incoming term (backport branch 5.x)","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"404","391","9024","982","backported to branch 5.x","backported to branch 5.x","teofili","NULL","1","decision","0","0","0","0","1"
"350","392","8998","982","See (also?) LUCENE-4483.","See (also?)","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"351","393","8998","982","See (also?) LUCENE-4483.","LUCENE-4483.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"352","394","8999","982","I had a look at the core code for the use of the TermQuery constructors, and I agree that it would be better to do the clone in the constructor.
Shall I give this a try?","I had a look at the core code for the use of the TermQuery constructors, and I agree that it would be better to do the clone in the constructor.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"353","395","8999","982","I had a look at the core code for the use of the TermQuery constructors, and I agree that it would be better to do the clone in the constructor.
Shall I give this a try?","Shall I give this a try?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"354","396","9000","982","Patch of 3 Oct 2015. This

adds a call to BytesRef.deepCopyOf in the Term constructor,
removes such calls where the Term constructor is used, and
documents that the result of Term.bytes() should not be modified.

","Patch of 3 Oct 2015.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"355","397","9000","982","Patch of 3 Oct 2015. This

adds a call to BytesRef.deepCopyOf in the Term constructor,
removes such calls where the Term constructor is used, and
documents that the result of Term.bytes() should not be modified.

","This

adds a call to BytesRef.deepCopyOf in the Term constructor,
removes such calls where the Term constructor is used, and
documents that the result of Term.bytes() should not be modified.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"356","398","9001","982","The patch also removes a call to the term constructor in BlendedTermQuery, which was actually making a clone.
This might have gone too far, but I think it should work because the boost is in BoostQuery now.","The patch also removes a call to the term constructor in BlendedTermQuery, which was actually making a clone.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"357","399","9001","982","The patch also removes a call to the term constructor in BlendedTermQuery, which was actually making a clone.
This might have gone too far, but I think it should work because the boost is in BoostQuery now.","This might have gone too far, but I think it should work because the boost is in BoostQuery now.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"358","400","9002","982","Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
Maybe might it be advisable to have an alternative constructor that doesn't clone so that users like Solr can exploit the fact that their code won't be making any further use of the input term?","Won't this change have the prospect of increasing the amount of GC due to all these extra objects?","jkrupan","NULL","1","alternative, pro","0","1","1","0","0"
"359","401","9002","982","Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
Maybe might it be advisable to have an alternative constructor that doesn't clone so that users like Solr can exploit the fact that their code won't be making any further use of the input term?","Maybe might it be advisable to have an alternative constructor that doesn't clone so that users like Solr can exploit the fact that their code won't be making any further use of the input term?","jkrupan","NULL","1","alternative, pro","0","1","1","0","0"
"360","402","9003","982","I don't think we should bother at all: executing a term query already performs I/O operations and allocates several objects per segment to create terms enums, scorers, iterators, leaf collectors, etc. so adding two extra object allocations to clone the incoming term is very unlikely to have noticeable impact on gc activity.","I don't think we should bother at all: executing a term query already performs I/O operations and allocates several objects per segment to create terms enums, scorers, iterators, leaf collectors, etc.","jpountz","NULL","1","alternative, con","0","1","0","1","0"
"361","403","9003","982","I don't think we should bother at all: executing a term query already performs I/O operations and allocates several objects per segment to create terms enums, scorers, iterators, leaf collectors, etc. so adding two extra object allocations to clone the incoming term is very unlikely to have noticeable impact on gc activity.","so adding two extra object allocations to clone the incoming term is very unlikely to have noticeable impact on gc activity.","jpountz","NULL","1","alternative, con","0","1","0","1","0"
"362","404","9004","982","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.
Won't this change have the prospect of increasing the amount of GC due to all these extra objects?
The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.
have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.
The Term(String field, String text) constructor makes a new BytesRef anyway.
so that users like Solr can exploit the fact that their code won't be making any further use of the input term?
There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher. There this call could be removed also.
I'm not familiar with Solr code, so in case there is another impact there, please holler.
","There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"405","405","10483","1102","Initial patch fixing the comparator to take into account the payload when everything else is same.","Initial patch fixing the comparator to take into account the payload when everything else is same.","arcadius","NULL","1","alternative","0","1","0","0","0"
"406","406","10484","1102","Commit 1691282 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1691282 ]
LUCENE-6680: don't lose a suggestion that differs only in payload from another suggestion","Commit 1691282 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1691282 ]
LUCENE-6680: don't lose a suggestion that differs only in payload from another suggestion","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"407","407","10485","1102","Thanks Arcadius Ahouansou, I just committed your patch with a small tweak to the if statement logic in the comparator...","Thanks Arcadius Ahouansou, I just committed your patch with a small tweak to the if statement logic in the comparator...","mikemccand","NULL","1","alternative, decision","0","1","0","0","1"
"408","408","10486","1102","Commit 1691283 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1691283 ]
LUCENE-6680: don't lose a suggestion that differs only in payload from another suggestion","Commit 1691283 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1691283 ]
LUCENE-6680: don't lose a suggestion that differs only in payload from another suggestion","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"409","409","10487","1102","Thank you very much Michael McCandless for helping get this issue fixed.","Thank you very much Michael McCandless for helping get this issue fixed.","arcadius","NULL","0",NULL,"0","0","0","0","0"
"410","410","10488","1102","Bulk close for 5.3.0 release","Bulk close for 5.3.0 release","shalinmangar","NULL","1","decision","0","0","0","0","1"
"411","411","10798","1136","We should define a complete standard
Lucene uses Sun's java coding conventions, apparently moved here: http://www.oracle.com/technetwork/java/codeconvtoc-136057.html
With one exception: 2 space indent, not 4.","We should define a complete standard
Lucene uses Sun's java coding conventions, apparently moved here: http://www.oracle.com/technetwork/java/codeconvtoc-136057.html
With one exception: 2 space indent, not 4.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"412","412","10800","1136","Currently by default the Codestyle is consistent with spaces.
The problem was actually with un-consistent  classes already committed that were causing the confusion.
We can close this now.","Currently by default the Codestyle is consistent with spaces.","alessandro.benedetti","NULL","1","pro","0","0","1","0","0"
"413","413","10800","1136","Currently by default the Codestyle is consistent with spaces.
The problem was actually with un-consistent  classes already committed that were causing the confusion.
We can close this now.","The problem was actually with un-consistent  classes already committed that were causing the confusion.","alessandro.benedetti","NULL","1","issue","1","0","0","0","0"
"414","414","10800","1136","Currently by default the Codestyle is consistent with spaces.
The problem was actually with un-consistent  classes already committed that were causing the confusion.
We can close this now.","We can close this now.","alessandro.benedetti","NULL","1","decision","0","0","0","0","1"
"415","415","10801","1136","And thanks Mike for the explanation, actually I missed the comments ! ","And thanks Mike for the explanation, actually I missed the comments !","alessandro.benedetti","NULL","0",NULL,"0","0","0","0","0"
"416","416","10802","1136","You're welcome Alessandro Benedetti!  Thank you for raising this.","You're welcome Alessandro Benedetti!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"417","417","10802","1136","You're welcome Alessandro Benedetti!  Thank you for raising this.","Thank you for raising this.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"418","418","11697","1229","Patch guards the Files.createDirectories call with a Files.exists check. Files.exists only demands read access, so it succeeds if you only have read access.","Patch guards the Files.createDirectories call with a Files.exists check.","trejkaz","NULL","1","alternative","0","1","0","0","0"
"419","419","11697","1229","Patch guards the Files.createDirectories call with a Files.exists check. Files.exists only demands read access, so it succeeds if you only have read access.","Files.exists only demands read access, so it succeeds if you only have read access.","trejkaz","NULL","1","pro","0","0","1","0","0"
"420","420","11702","1229","And your second patch: Please don't do this. This is all not worth a test. It just compromises our security manager. A SecurityManager that allows to override/delete itsself just makes itsself broken. ","And your second patch: Please don't do this.","thetaphi","NULL","1","con","0","0","0","1","0"
"421","421","11702","1229","And your second patch: Please don't do this. This is all not worth a test. It just compromises our security manager. A SecurityManager that allows to override/delete itsself just makes itsself broken. ","This is all not worth a test.","thetaphi","NULL","1","con","0","0","0","1","0"
"422","422","11702","1229","And your second patch: Please don't do this. This is all not worth a test. It just compromises our security manager. A SecurityManager that allows to override/delete itsself just makes itsself broken. ","It just compromises our security manager.","thetaphi","NULL","1","con","0","0","0","1","0"
"423","423","11702","1229","And your second patch: Please don't do this. This is all not worth a test. It just compromises our security manager. A SecurityManager that allows to override/delete itsself just makes itsself broken. ","A SecurityManager that allows to override/delete itsself just makes itsself broken.","thetaphi","NULL","1","con","0","0","0","1","0"
"424","424","11703","1229","Yes, if you really want to do this in a test:
Guideline 9-4 / ACCESS-4: Know how to restrict privileges through doPrivileged
http://www.oracle.com/technetwork/java/seccodeguide-139067.html#9
","Yes, if you really want to do this in a test:
Guideline 9-4 / ACCESS-4: Know how to restrict privileges through doPrivileged
http://www.oracle.com/technetwork/java/seccodeguide-139067.html#9","rcmuir","NULL","1","alternative","0","1","0","0","0"
"425","425","11704","1229","Good tip! ","Good tip!","thetaphi","NULL","1","pro","0","0","1","0","0"
"426","426","11706","1229","I have the test working here. Will post updated patch.","I have the test working here.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"427","427","11706","1229","I have the test working here. Will post updated patch.","Will post updated patch.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"428","428","11708","1229","Here is the test. I had to f*ck with the last Permission, because to run the code with restricted permissions you need the additional permission, otherwise it runs with no permissions at all  (see javadocs)
I also added a helper method, we may move this to LTC later","Here is the test.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"429","429","11708","1229","Here is the test. I had to f*ck with the last Permission, because to run the code with restricted permissions you need the additional permission, otherwise it runs with no permissions at all  (see javadocs)
I also added a helper method, we may move this to LTC later","I had to f*ck with the last Permission, because to run the code with restricted permissions you need the additional permission, otherwise it runs with no permissions at all  (see javadocs)
I also added a helper method, we may move this to LTC later","thetaphi","NULL","1","alternative","0","1","0","0","0"
"430","430","11709","1229","Slightly improved generics in the patch to make the helper method more universal (to move to LTC).","Slightly improved generics in the patch to make the helper method more universal (to move to LTC).","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"431","431","11710","1229","I also added a helper method, we may move this to LTC later
+1
This patch looks great Uwe, generalizing that method into LTC sounds good to me.","I also added a helper method, we may move this to LTC later
+1
This patch looks great Uwe, generalizing that method into LTC sounds good to me.","hossman","NULL","1","pro","0","0","1","0","0"
"432","432","11711","1229","More neat helper method (Java 8). I will blow this up to ten times its size when backporting to 5.x ","More neat helper method (Java 8).","thetaphi","NULL","1","pro","0","0","1","0","0"
"433","433","11711","1229","More neat helper method (Java 8). I will blow this up to ten times its size when backporting to 5.x ","I will blow this up to ten times its size when backporting to 5.x","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"434","434","11712","1229","This patch looks great Uwe, generalizing that method into LTC sounds good to me.
Will look into this tomorrow. Have to sleep now, its already 4 am. The time when Mike McCandless starts coding ","This patch looks great Uwe, generalizing that method into LTC sounds good to me.","thetaphi","NULL","1","pro","0","0","1","0","0"
"435","435","11712","1229","This patch looks great Uwe, generalizing that method into LTC sounds good to me.
Will look into this tomorrow. Have to sleep now, its already 4 am. The time when Mike McCandless starts coding ","Will look into this tomorrow.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"436","436","11712","1229","This patch looks great Uwe, generalizing that method into LTC sounds good to me.
Will look into this tomorrow. Have to sleep now, its already 4 am. The time when Mike McCandless starts coding ","Have to sleep now, its already 4 am.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"437","437","11712","1229","This patch looks great Uwe, generalizing that method into LTC sounds good to me.
Will look into this tomorrow. Have to sleep now, its already 4 am. The time when Mike McCandless starts coding ","The time when Mike McCandless starts coding","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"438","438","11713","1229","Moved the runWithLowerPermissions to LTC.
I also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).
Could somebody with Linux or MacOSX test the patch or state any other complaints with it?","Moved the runWithLowerPermissions to LTC.","thetaphi","NULL","1","decision","0","0","0","0","1"
"439","439","11713","1229","Moved the runWithLowerPermissions to LTC.
I also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).
Could somebody with Linux or MacOSX test the patch or state any other complaints with it?","I also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"440","440","11713","1229","Moved the runWithLowerPermissions to LTC.
I also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).
Could somebody with Linux or MacOSX test the patch or state any other complaints with it?","Could somebody with Linux or MacOSX test the patch or state any other complaints with it?","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"441","441","11714","1229","More tests that we cannot escape our sandbox!","More tests that we cannot escape our sandbox!","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"442","442","11715","1229","After discussion with Hoss Man, I added an assume to the runWithRestrictedPermissions, so it cancels test execution if no security manager is available. Running those tests without a security manager makes no sense, because they would assert nothing (because they have all permissions).","After discussion with Hoss Man, I added an assume to the runWithRestrictedPermissions, so it cancels test execution if no security manager is available.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"443","443","11715","1229","After discussion with Hoss Man, I added an assume to the runWithRestrictedPermissions, so it cancels test execution if no security manager is available. Running those tests without a security manager makes no sense, because they would assert nothing (because they have all permissions).","Running those tests without a security manager makes no sense, because they would assert nothing (because they have all permissions).","thetaphi","NULL","1","con","0","0","0","1","0"
"444","444","11716","1229","Commit 1688537 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1688537 ]
LUCENE-6542: FSDirectory's ctor now works with security policies or file systems that restrict write access","Commit 1688537 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1688537 ]
LUCENE-6542: FSDirectory's ctor now works with security policies or file systems that restrict write access","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"445","445","11717","1229","Commit 1688541 from Uwe Schindler in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1688541 ]
Merged revision(s) 1688537 from lucene/dev/trunk:
LUCENE-6542: FSDirectory's ctor now works with security policies or file systems that restrict write access","Commit 1688541 from Uwe Schindler in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1688541 ]
Merged revision(s) 1688537 from lucene/dev/trunk:
LUCENE-6542: FSDirectory's ctor now works with security policies or file systems that restrict write access","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"446","446","11718","1229","Committed and backported the lambdas to Java 7 in branch_5x.","Committed and backported the lambdas to Java 7 in branch_5x.","thetaphi","NULL","1","decision","0","0","0","0","1"
"447","447","11719","1229","Bulk close for 5.3.0 release","Bulk close for 5.3.0 release","shalinmangar","NULL","1","decision","0","0","0","0","1"
"448","448","11870","1244","Here is a patch.","Here is a patch.","jpountz","NULL","0","alternative","0","1","0","0","0"
"449","449","11871","1244","+1","+1","rcmuir","NULL","1","pro","0","0","1","0","0"
"450","450","11872","1244","Commit 1683734 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1683734 ]
LUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.","Commit 1683734 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1683734 ]
LUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"451","451","11873","1244","Commit 1683744 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1683744 ]
LUCENE-6526: Revert some changes that were committed by mistake.","Commit 1683744 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1683744 ]
LUCENE-6526: Revert some changes that were committed by mistake.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"452","452","11874","1244","Commit 1683745 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1683745 ]
LUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.","Commit 1683745 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1683745 ]
LUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"453","453","11875","1244","Bulk close for 5.3.0 release","Bulk close for 5.3.0 release","shalinmangar","NULL","1","decision","0","0","0","0","1"
"454","454","13030","1347","Commit 1672281 from Dawid Weiss in branch 'dev/trunk'
[ https://svn.apache.org/r1672281 ]
LUCENE-6413: Test runner should report the number of suites completed/ remaining.","Commit 1672281 from Dawid Weiss in branch 'dev/trunk'
[ https://svn.apache.org/r1672281 ]
LUCENE-6413: Test runner should report the number of suites completed/ remaining.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"455","455","13031","1347","Commit 1672282 from Dawid Weiss in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1672282 ]
LUCENE-6413: Test runner should report the number of suites completed/ remaining.","Commit 1672282 from Dawid Weiss in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1672282 ]
LUCENE-6413: Test runner should report the number of suites completed/ remaining.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"456","456","13032","1347","Woohoo! Thanks Dawid!","Woohoo!","jpountz","NULL","1","pro","0","0","1","0","0"
"457","457","13032","1347","Woohoo! Thanks Dawid!","Thanks Dawid!","jpountz","NULL","0",NULL,"0","0","0","0","0"
"458","458","13033","1347","You're very welcome although I am so far behind with other changes I'd like to make to the RR that there's hardly any reason to celebrate  (yet .","You're very welcome although I am so far behind with other changes I'd like to make to the RR that there's hardly any reason to celebrate  (yet .","dweiss","NULL","0",NULL,"0","0","0","0","0"
"459","459","13034","1347","This is awesome.  It won't make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.","This is awesome.","elyograg","NULL","1","pro","0","0","1","0","0"
"460","460","13034","1347","This is awesome.  It won't make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.","It won't make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.","elyograg","NULL","1","pro","0","0","1","0","0"
"461","461","13035","1347","Bulk close for 5.3.0 release","Bulk close for 5.3.0 release","shalinmangar","NULL","1","decision","0","0","0","0","1"
"462","462","13078","1354","Here is an initial shotgun approach.
I verified it finds things like file leaks by introducing them to old codecs. But it currently sometimes trips assertions during IW.commit(). I'm not sure everything is ok here. I'm not trying to test IW so I'm gonna neuter it and keep working.","Here is an initial shotgun approach.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"463","463","13078","1354","Here is an initial shotgun approach.
I verified it finds things like file leaks by introducing them to old codecs. But it currently sometimes trips assertions during IW.commit(). I'm not sure everything is ok here. I'm not trying to test IW so I'm gonna neuter it and keep working.","I verified it finds things like file leaks by introducing them to old codecs.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"464","464","13078","1354","Here is an initial shotgun approach.
I verified it finds things like file leaks by introducing them to old codecs. But it currently sometimes trips assertions during IW.commit(). I'm not sure everything is ok here. I'm not trying to test IW so I'm gonna neuter it and keep working.","But it currently sometimes trips assertions during IW.commit().","rcmuir","NULL","1","issue","1","0","0","0","0"
"465","465","13078","1354","Here is an initial shotgun approach.
I verified it finds things like file leaks by introducing them to old codecs. But it currently sometimes trips assertions during IW.commit(). I'm not sure everything is ok here. I'm not trying to test IW so I'm gonna neuter it and keep working.","I'm not sure everything is ok here.","rcmuir","NULL","1","con","0","0","0","1","0"
"466","466","13078","1354","Here is an initial shotgun approach.
I verified it finds things like file leaks by introducing them to old codecs. But it currently sometimes trips assertions during IW.commit(). I'm not sure everything is ok here. I'm not trying to test IW so I'm gonna neuter it and keep working.","I'm not trying to test IW so I'm gonna neuter it and keep working.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"467","467","13079","1354","False alarm (fake IOE -> slowFileExists == false -> scary assertion). I disable it during commit (with a comment) as a workaround. But its still on during reopen which does not have this check. I think its good enough.","False alarm (fake IOE -> slowFileExists == false -> scary assertion).","rcmuir","NULL","1","issue","1","0","0","0","0"
"468","468","13079","1354","False alarm (fake IOE -> slowFileExists == false -> scary assertion). I disable it during commit (with a comment) as a workaround. But its still on during reopen which does not have this check. I think its good enough.","I disable it during commit (with a comment) as a workaround.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"469","469","13079","1354","False alarm (fake IOE -> slowFileExists == false -> scary assertion). I disable it during commit (with a comment) as a workaround. But its still on during reopen which does not have this check. I think its good enough.","But its still on during reopen which does not have this check.","rcmuir","NULL","1","issue","1","0","0","0","0"
"470","470","13079","1354","False alarm (fake IOE -> slowFileExists == false -> scary assertion). I disable it during commit (with a comment) as a workaround. But its still on during reopen which does not have this check. I think its good enough.","I think its good enough.","rcmuir","NULL","1","pro","0","0","1","0","0"
"471","471","13080","1354","False alarm 
Phew!","False alarm 
Phew!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"472","472","13081","1354","Commit 1671900 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671900 ]
LUCENE-6405: add shotgun test for exception handling","Commit 1671900 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671900 ]
LUCENE-6405: add shotgun test for exception handling","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"473","473","13082","1354","Commit 1671902 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671902 ]
LUCENE-6405: add shotgun test for exception handling","Commit 1671902 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671902 ]
LUCENE-6405: add shotgun test for exception handling","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"474","474","13083","1354","I'll close the issue, but we should improve it more later. Really each of these tests should have unit tests (using a MDW that failOn's openInput/createOutput/whereever). I think it would even better. But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/","I'll close the issue, but we should improve it more later.","rcmuir","NULL","1","decision","0","0","0","0","1"
"475","475","13083","1354","I'll close the issue, but we should improve it more later. Really each of these tests should have unit tests (using a MDW that failOn's openInput/createOutput/whereever). I think it would even better. But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/","Really each of these tests should have unit tests (using a MDW that failOn's openInput/createOutput/whereever).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"476","476","13083","1354","I'll close the issue, but we should improve it more later. Really each of these tests should have unit tests (using a MDW that failOn's openInput/createOutput/whereever). I think it would even better. But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/","I think it would even better.","rcmuir","NULL","1","pro","0","0","1","0","0"
"477","477","13083","1354","I'll close the issue, but we should improve it more later. Really each of these tests should have unit tests (using a MDW that failOn's openInput/createOutput/whereever). I think it would even better. But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/","But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/","rcmuir","NULL","1","pro, decision","0","0","1","0","1"
"478","478","13084","1354","Commit 1671995 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671995 ]
LUCENE-6405: add infos exc unit tests. fix TestTransactions to handle exceptions on openInput and let MDW do it","Commit 1671995 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671995 ]
LUCENE-6405: add infos exc unit tests.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"479","479","13084","1354","Commit 1671995 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671995 ]
LUCENE-6405: add infos exc unit tests. fix TestTransactions to handle exceptions on openInput and let MDW do it","fix TestTransactions to handle exceptions on openInput and let MDW do it","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"480","480","13085","1354","Commit 1671996 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671996 ]
LUCENE-6405: add infos exc unit tests. fix TestTransactions to handle exceptions on openInput and let MDW do it","Commit 1671996 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671996 ]
LUCENE-6405: add infos exc unit tests.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"481","481","13085","1354","Commit 1671996 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671996 ]
LUCENE-6405: add infos exc unit tests. fix TestTransactions to handle exceptions on openInput and let MDW do it","fix TestTransactions to handle exceptions on openInput and let MDW do it","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"482","482","13086","1354","Bulk close for 5.2.0.","Bulk close for 5.2.0.","anshumg","NULL","1","decision","0","0","0","0","1"
"483","483","13212","1370","
Report after iter 10:
Chart saved to out.png... (wd: /home/rmuir/workspace/util/src/python)
                    Task   QPS trunk      StdDev   QPS patch      StdDev                Pct diff
             MedSpanNear       75.69      (2.0\%)       80.58      (3.9\%)    6.5\% (   0\% -   12\%)
             LowSpanNear      233.30      (3.8\%)      259.44      (6.5\%)   11.2\% (   0\% -   22\%)
            HighSpanNear        9.43      (3.6\%)       10.76      (7.5\%)   14.0\% (   2\% -   25\%)

","
Report after iter 10:
Chart saved to out.png... (wd: /home/rmuir/workspace/util/src/python)
                    Task   QPS trunk      StdDev   QPS patch      StdDev                Pct diff
             MedSpanNear       75.69      (2.0\%)       80.58      (3.9\%)    6.5\% (   0\% -   12\%)
             LowSpanNear      233.30      (3.8\%)      259.44      (6.5\%)   11.2\% (   0\% -   22\%)
            HighSpanNear        9.43      (3.6\%)       10.76      (7.5\%)   14.0\% (   2\% -   25\%)","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"484","484","13213","1370","Oops, I thought by now ArrayList would be JIT-ed away, thanks.
Also the UOE's in the NearSpansOrdered payload methods have gone in this patch, I had put these in to check the tests.","Oops, I thought by now ArrayList would be JIT-ed away, thanks.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"485","485","13213","1370","Oops, I thought by now ArrayList would be JIT-ed away, thanks.
Also the UOE's in the NearSpansOrdered payload methods have gone in this patch, I had put these in to check the tests.","Also the UOE's in the NearSpansOrdered payload methods have gone in this patch, I had put these in to check the tests.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"486","486","13214","1370","I removed the UOE because now the no-payload impl is used if a segment doesn't happen to have any payloads. But this is valid, the documents might just not have any.","I removed the UOE because now the no-payload impl is used if a segment doesn't happen to have any payloads.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"487","487","13214","1370","I removed the UOE because now the no-payload impl is used if a segment doesn't happen to have any payloads. But this is valid, the documents might just not have any.","But this is valid, the documents might just not have any.","rcmuir","NULL","1","pro","0","0","1","0","0"
"488","488","13215","1370","+1 too bad we can't expect ArrayList to always perform like a plain array ","+1 too bad we can't expect ArrayList to always perform like a plain array","jpountz","NULL","1","pro","0","0","1","0","0"
"489","489","13216","1370","Commit 1671078 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671078 ]
LUCENE-6388: Optimize SpanNearQuery","Commit 1671078 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671078 ]
LUCENE-6388: Optimize SpanNearQuery","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"490","490","13217","1370","Commit 1671081 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671081 ]
LUCENE-6388: Optimize SpanNearQuery","Commit 1671081 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671081 ]
LUCENE-6388: Optimize SpanNearQuery","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"491","491","13218","1370","For now the check is implemented via Terms.getPayloads() until LUCENE-6390 is fixed.","For now the check is implemented via Terms.getPayloads() until LUCENE-6390 is fixed.","rcmuir","NULL","1","decision","0","0","0","0","1"
"492","492","13219","1370","Bulk close for 5.3.0 release","Bulk close for 5.3.0 release","shalinmangar","NULL","1","decision","0","0","0","0","1"
"493","493","13708","1422","I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false. Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won't fail. 
This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/. But we should probably think about better fixes for the future and avoid calling score() when it's not needed.","I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false.","jpountz","NULL","1","issue, alternative","1","1","0","0","0"
"494","494","13708","1422","I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false. Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won't fail. 
This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/. But we should probably think about better fixes for the future and avoid calling score() when it's not needed.","Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won't fail.","jpountz","NULL","1","alternative","0","1","0","0","0"
"495","495","13708","1422","I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false. Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won't fail. 
This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/. But we should probably think about better fixes for the future and avoid calling score() when it's not needed.","This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/.","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"496","496","13708","1422","I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false. Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won't fail. 
This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/. But we should probably think about better fixes for the future and avoid calling score() when it's not needed.","But we should probably think about better fixes for the future and avoid calling score() when it's not needed.","jpountz","NULL","1","alternative","0","1","0","0","0"
"497","497","13710","1422","I opened LUCENE-6330 to tackle BooleanScorer (and maybe others).","I opened LUCENE-6330 to tackle BooleanScorer (and maybe others).","jpountz","NULL","1","issue","1","0","0","0","0"
"498","498","13711","1422","Commit 1663756 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1663756 ]
LUCENE-6329: Calling score() should be ok even if needsScores is false.","Commit 1663756 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1663756 ]
LUCENE-6329: Calling score() should be ok even if needsScores is false.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"499","499","13712","1422","Commit 1663757 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1663757 ]
LUCENE-6329: Calling score() should be ok even if needsScores is false.","Commit 1663757 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1663757 ]
LUCENE-6329: Calling score() should be ok even if needsScores is false.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"500","500","13713","1422","Bulk close after 5.1 release","Bulk close after 5.1 release","thelabdude","NULL","1","decision","0","0","0","0","1"
"501","501","13731","1426","Here was my first patch (moved from LUCENE-6320).
Maybe we can add some fieldinfos tests to exercise this directly, and maybe there is a way to simplify the scary logic too.","Here was my first patch (moved from LUCENE-6320).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"502","502","13731","1426","Here was my first patch (moved from LUCENE-6320).
Maybe we can add some fieldinfos tests to exercise this directly, and maybe there is a way to simplify the scary logic too.","Maybe we can add some fieldinfos tests to exercise this directly, and maybe there is a way to simplify the scary logic too.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"503","503","13733","1426","+1, this looks like an easy, possibly high-impact win.
I think we can be more aggressive about using the array: TreeMap has much more than 50\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer.  I think we can safely do this opto when it's > 10\% of the space?","+1, this looks like an easy, possibly high-impact win.","mikemccand","NULL","1","pro","0","0","1","0","0"
"504","504","13733","1426","+1, this looks like an easy, possibly high-impact win.
I think we can be more aggressive about using the array: TreeMap has much more than 50\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer.  I think we can safely do this opto when it's > 10\% of the space?","I think we can be more aggressive about using the array: TreeMap has much more than 50\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"505","505","13733","1426","+1, this looks like an easy, possibly high-impact win.
I think we can be more aggressive about using the array: TreeMap has much more than 50\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer.  I think we can safely do this opto when it's > 10\% of the space?","I think we can safely do this opto when it's > 10\% of the space?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"506","506","13734","1426","Mike, if you have time, can you do calculations and adjust the patch? You can take the issue too, i had basically given up on this.","Mike, if you have time, can you do calculations and adjust the patch?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"507","507","13734","1426","Mike, if you have time, can you do calculations and adjust the patch? You can take the issue too, i had basically given up on this.","You can take the issue too, i had basically given up on this.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"508","508","13735","1426","OK I can try to update this & commit...","OK I can try to update this & commit...","mikemccand","NULL","1","decision","0","0","0","0","1"
"509","509","13736","1426","New patch, using dense array when > 1/16th of the numbers are used:
Each TreeMap$Entry has object header (8 or 16 bytes), 5 pointers (4 or
8 bytes), and a boolean (likely rounded up to 4 bytes), times 2 for
all the inner nodes of the tree, plus the overhead of Integer (object
header, int), so net/net each entry in the TreeMap costs 68 - 124 bytes.
The array is 4 or 8 bytes per int.","New patch, using dense array when > 1/16th of the numbers are used:
Each TreeMap$Entry has object header (8 or 16 bytes), 5 pointers (4 or
8 bytes), and a boolean (likely rounded up to 4 bytes), times 2 for
all the inner nodes of the tree, plus the overhead of Integer (object
header, int), so net/net each entry in the TreeMap costs 68 - 124 bytes.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"510","510","13736","1426","New patch, using dense array when > 1/16th of the numbers are used:
Each TreeMap$Entry has object header (8 or 16 bytes), 5 pointers (4 or
8 bytes), and a boolean (likely rounded up to 4 bytes), times 2 for
all the inner nodes of the tree, plus the overhead of Integer (object
header, int), so net/net each entry in the TreeMap costs 68 - 124 bytes.
The array is 4 or 8 bytes per int.","The array is 4 or 8 bytes per int.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"511","511","13737","1426","I think the patch is ready...","I think the patch is ready...","mikemccand","NULL","1","pro","0","0","1","0","0"
"512","512","13738","1426","Thanks, can you change 16 to 16L? I don't want to think about overflowing.","Thanks, can you change 16 to 16L?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"513","513","13738","1426","Thanks, can you change 16 to 16L? I don't want to think about overflowing.","I don't want to think about overflowing.","rcmuir","NULL","1","con","0","0","0","1","0"
"514","514","13739","1426","Thanks, can you change 16 to 16L?
Oh, good catch!  I'll fix ...","Thanks, can you change 16 to 16L?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"515","515","13739","1426","Thanks, can you change 16 to 16L?
Oh, good catch!  I'll fix ...","Oh, good catch!","mikemccand","NULL","1","pro","0","0","1","0","0"
"516","516","13739","1426","Thanks, can you change 16 to 16L?
Oh, good catch!  I'll fix ...","I'll fix ...","mikemccand","NULL","1","decision","0","0","0","0","1"
"517","517","13740","1426","+1 to commit with that modification, thanks for doing the calculations here.","+1 to commit with that modification, thanks for doing the calculations here.","rcmuir","NULL","1","pro","0","0","1","0","0"
"518","518","13741","1426","Commit 1687789 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1687789 ]
LUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases","Commit 1687789 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1687789 ]
LUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"519","519","13742","1426","Commit 1687792 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1687792 ]
LUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases","Commit 1687792 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1687792 ]
LUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"520","520","13743","1426","Thanks Robert Muir!","Thanks Robert Muir!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"521","521","13744","1426","Bulk close for 5.3.0 release","Bulk close for 5.3.0 release","shalinmangar","NULL","1","decision","0","0","0","0","1"
"522","522","13938","1444","Patch that adds the MatchNoDocsQuery and uses it for empty SimpleQueryParser queries as well as when a BooleanQuery is rewritten and has no clauses.","Patch that adds the MatchNoDocsQuery and uses it for empty SimpleQueryParser queries as well as when a BooleanQuery is rewritten and has no clauses.","dakrone","NULL","1","alternative","0","1","0","0","0"
"523","523","13939","1444","I think its confusing we have MatchAll but not MatchNone.
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
Then it wouldn't need a weight and scorer.","I think its confusing we have MatchAll but not MatchNone.","rcmuir","NULL","1","con","0","0","0","1","0"
"524","524","13939","1444","I think its confusing we have MatchAll but not MatchNone.
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
Then it wouldn't need a weight and scorer.","I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"525","525","13939","1444","I think its confusing we have MatchAll but not MatchNone.
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
Then it wouldn't need a weight and scorer.","Then it wouldn't need a weight and scorer.","rcmuir","NULL","1","pro","0","0","1","0","0"
"526","526","13940","1444","I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code. Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
+1 to rewrite to an empty BooleanQuery","I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code.","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"527","527","13940","1444","I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code. Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
+1 to rewrite to an empty BooleanQuery","Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"528","528","13940","1444","I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code. Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
+1 to rewrite to an empty BooleanQuery","I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses?","jpountz","NULL","1","alternative","0","1","0","0","0"
"529","529","13940","1444","I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code. Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().
I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses? 
+1 to rewrite to an empty BooleanQuery","+1 to rewrite to an empty BooleanQuery","jpountz","NULL","1","pro","0","0","1","0","0"
"530","530","13941","1444","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"531","531","13942","1444","New patch that changes MatchNoDocsQuery to rewrite to an empty BooleanQuery. Also removes the nocommit as per Adrien's suggestion","New patch that changes MatchNoDocsQuery to rewrite to an empty BooleanQuery.","dakrone","NULL","1","alternative","0","1","0","0","0"
"532","532","13942","1444","New patch that changes MatchNoDocsQuery to rewrite to an empty BooleanQuery. Also removes the nocommit as per Adrien's suggestion","Also removes the nocommit as per Adrien's suggestion","dakrone","NULL","1","alternative","0","1","0","0","0"
"533","533","13943","1444","is the hashcode/equals stuff needed here or can the superclass impls in Query be used? They seem to already have this logic.
In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries. This will do some tests on hashcode/equals.","is the hashcode/equals stuff needed here or can the superclass impls in Query be used?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"534","534","13943","1444","is the hashcode/equals stuff needed here or can the superclass impls in Query be used? They seem to already have this logic.
In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries. This will do some tests on hashcode/equals.","They seem to already have this logic.","rcmuir","NULL","1","pro","0","0","1","0","0"
"535","535","13943","1444","is the hashcode/equals stuff needed here or can the superclass impls in Query be used? They seem to already have this logic.
In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries. This will do some tests on hashcode/equals.","In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"536","536","13943","1444","is the hashcode/equals stuff needed here or can the superclass impls in Query be used? They seem to already have this logic.
In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries. This will do some tests on hashcode/equals.","This will do some tests on hashcode/equals.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"537","537","13945","1444","Feels wrong to me to override hashCode but not equals. I think we should move this class part of hashCode() to Query.hashCode()? (ie. return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()
If it is controversial then I'm happy with the previous patch that overrides both equals and hashCode().","Feels wrong to me to override hashCode but not equals.","jpountz","NULL","1","con","0","0","0","1","0"
"538","538","13945","1444","Feels wrong to me to override hashCode but not equals. I think we should move this class part of hashCode() to Query.hashCode()? (ie. return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()
If it is controversial then I'm happy with the previous patch that overrides both equals and hashCode().","I think we should move this class part of hashCode() to Query.hashCode()?","jpountz","NULL","1","alternative","0","1","0","0","0"
"539","539","13945","1444","Feels wrong to me to override hashCode but not equals. I think we should move this class part of hashCode() to Query.hashCode()? (ie. return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()
If it is controversial then I'm happy with the previous patch that overrides both equals and hashCode().","(ie.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"540","540","13945","1444","Feels wrong to me to override hashCode but not equals. I think we should move this class part of hashCode() to Query.hashCode()? (ie. return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()
If it is controversial then I'm happy with the previous patch that overrides both equals and hashCode().","return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()
If it is controversial then I'm happy with the previous patch that overrides both equals and hashCode().","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"541","541","13946","1444","+1 Adrien.","+1 Adrien.","dsmiley","NULL","1","pro","0","0","1","0","0"
"542","542","13947","1444","Adrien: I agree about having the hashCode.
Here is a new patch that doesn't override equals or hashCode and changes Query to use the class in the hashCode method as Adrien suggested.","Adrien: I agree about having the hashCode.","dakrone","NULL","1","pro","0","0","1","0","0"
"543","543","13947","1444","Adrien: I agree about having the hashCode.
Here is a new patch that doesn't override equals or hashCode and changes Query to use the class in the hashCode method as Adrien suggested.","Here is a new patch that doesn't override equals or hashCode and changes Query to use the class in the hashCode method as Adrien suggested.","dakrone","NULL","1","alternative","0","1","0","0","0"
"544","544","13948","1444","Thanks Lee, I like this.
When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign. 
We should commit this one, and remove duplicated logic and hardcoded constants in e.g. TermQuery and all other places in a followup. ","Thanks Lee, I like this.","rcmuir","NULL","1","pro","0","0","1","0","0"
"545","545","13948","1444","Thanks Lee, I like this.
When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign. 
We should commit this one, and remove duplicated logic and hardcoded constants in e.g. TermQuery and all other places in a followup. ","When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign.","rcmuir","NULL","1","con","0","0","0","1","0"
"546","546","13948","1444","Thanks Lee, I like this.
When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign. 
We should commit this one, and remove duplicated logic and hardcoded constants in e.g. TermQuery and all other places in a followup. ","We should commit this one, and remove duplicated logic and hardcoded constants in e.g.","rcmuir","NULL","1","decision","0","0","0","0","1"
"547","547","13948","1444","Thanks Lee, I like this.
When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign. 
We should commit this one, and remove duplicated logic and hardcoded constants in e.g. TermQuery and all other places in a followup. ","TermQuery and all other places in a followup.","rcmuir","NULL","1","decision","0","0","0","0","1"
"548","548","13949","1444","Robert: +1, I opened LUCENE-6333 for this, I'll work on a patch.","Robert: +1, I opened LUCENE-6333 for this, I'll work on a patch.","dakrone","NULL","1","pro","0","0","1","0","0"
"549","549","13950","1444","Commit 1663899 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1663899 ]
LUCENE-6304: Add MatchNoDocsQuery.","Commit 1663899 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1663899 ]
LUCENE-6304: Add MatchNoDocsQuery.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"550","550","13951","1444","Committed. Thanks Lee!","Committed.","jpountz","NULL","1","decision","0","0","0","0","1"
"551","551","13951","1444","Committed. Thanks Lee!","Thanks Lee!","jpountz","NULL","0",NULL,"0","0","0","0","0"
"552","552","13952","1444","Commit 1663901 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1663901 ]
LUCENE-6304: Add MatchNoDocsQuery.","Commit 1663901 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1663901 ]
LUCENE-6304: Add MatchNoDocsQuery.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"553","553","13953","1444","Bulk close after 5.1 release","Bulk close after 5.1 release","thelabdude","NULL","1","decision","0","0","0","0","1"
"556","554","14688","1508","
I think CheckIndex should not check Terms.getMin/Max for TVs?
+1","+1","rcmuir","NULL","1","pro","0","0","1","0","0"
"557","555","14690","1508","I added two more timings to the patch. here is the output on one of my wiki10m segments:

size (MB)=624.591
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_40-ea, lucene.version=6.0.0, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=3.13.0-43-generic, timestamp=1423097209630}
    has deletions [delGen=6]
    test: open reader.........OK [took 0.075 sec]
    test: check integrity.....OK [took 1.515 sec]
    test: check live docs.....OK [90031 deleted docs]
    test: field infos.........OK [8 fields] [took 0.000 sec]
    test: field norms.........OK [2 fields] [took 0.046 sec]
    test: terms, freq, prox...OK [6844227 terms; 170452948 terms/docs pairs; 240913350 tokens] [took 13.171 sec]
    test (ignoring deletes): terms, freq, prox...OK [7105194 terms; 179422787 terms/docs pairs; 253586353 tokens] [took 9.632 sec]
    test: stored fields.......OK [5135307 total field count; avg 3.0 fields per doc] [took 4.648 sec]
    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.036 sec]
    test: docvalues...........OK [2 docvalues fields; 0 BINARY; 1 NUMERIC; 1 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.206 sec]


Maybe check index should have a integrity-check only option as a followup. It would just be sugar to the user, but this would always be pretty fast.","I added two more timings to the patch.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"558","556","14690","1508","I added two more timings to the patch. here is the output on one of my wiki10m segments:

size (MB)=624.591
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_40-ea, lucene.version=6.0.0, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=3.13.0-43-generic, timestamp=1423097209630}
    has deletions [delGen=6]
    test: open reader.........OK [took 0.075 sec]
    test: check integrity.....OK [took 1.515 sec]
    test: check live docs.....OK [90031 deleted docs]
    test: field infos.........OK [8 fields] [took 0.000 sec]
    test: field norms.........OK [2 fields] [took 0.046 sec]
    test: terms, freq, prox...OK [6844227 terms; 170452948 terms/docs pairs; 240913350 tokens] [took 13.171 sec]
    test (ignoring deletes): terms, freq, prox...OK [7105194 terms; 179422787 terms/docs pairs; 253586353 tokens] [took 9.632 sec]
    test: stored fields.......OK [5135307 total field count; avg 3.0 fields per doc] [took 4.648 sec]
    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.036 sec]
    test: docvalues...........OK [2 docvalues fields; 0 BINARY; 1 NUMERIC; 1 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.206 sec]


Maybe check index should have a integrity-check only option as a followup. It would just be sugar to the user, but this would always be pretty fast.","here is the output on one of my wiki10m segments:

size (MB)=624.591
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_40-ea, lucene.version=6.0.0, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=3.13.0-43-generic, timestamp=1423097209630}
    has deletions [delGen=6]
    test: open reader.........OK [took 0.075 sec]
    test: check integrity.....OK [took 1.515 sec]
    test: check live docs.....OK [90031 deleted docs]
    test: field infos.........OK [8 fields] [took 0.000 sec]
    test: field norms.........OK [2 fields] [took 0.046 sec]
    test: terms, freq, prox...OK [6844227 terms; 170452948 terms/docs pairs; 240913350 tokens] [took 13.171 sec]
    test (ignoring deletes): terms, freq, prox...OK [7105194 terms; 179422787 terms/docs pairs; 253586353 tokens] [took 9.632 sec]
    test: stored fields.......OK [5135307 total field count; avg 3.0 fields per doc] [took 4.648 sec]
    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.036 sec]
    test: docvalues...........OK [2 docvalues fields; 0 BINARY; 1 NUMERIC; 1 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.206 sec]


Maybe check index should have a integrity-check only option as a followup.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"559","557","14690","1508","I added two more timings to the patch. here is the output on one of my wiki10m segments:

size (MB)=624.591
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_40-ea, lucene.version=6.0.0, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=3.13.0-43-generic, timestamp=1423097209630}
    has deletions [delGen=6]
    test: open reader.........OK [took 0.075 sec]
    test: check integrity.....OK [took 1.515 sec]
    test: check live docs.....OK [90031 deleted docs]
    test: field infos.........OK [8 fields] [took 0.000 sec]
    test: field norms.........OK [2 fields] [took 0.046 sec]
    test: terms, freq, prox...OK [6844227 terms; 170452948 terms/docs pairs; 240913350 tokens] [took 13.171 sec]
    test (ignoring deletes): terms, freq, prox...OK [7105194 terms; 179422787 terms/docs pairs; 253586353 tokens] [took 9.632 sec]
    test: stored fields.......OK [5135307 total field count; avg 3.0 fields per doc] [took 4.648 sec]
    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.036 sec]
    test: docvalues...........OK [2 docvalues fields; 0 BINARY; 1 NUMERIC; 1 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.206 sec]


Maybe check index should have a integrity-check only option as a followup. It would just be sugar to the user, but this would always be pretty fast.","It would just be sugar to the user, but this would always be pretty fast.","rcmuir","NULL","1","pro, con","0","0","1","1","0"
"560","558","14691","1508","sorry, here is the correct patch.","sorry, here is the correct patch.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"561","559","14692","1508","OK I noticed one case where live docs didn't confess how long it took 
I'll fix that and commit.","OK I noticed one case where live docs didn't confess how long it took 
I'll fix that and commit.","mikemccand","NULL","1","decision","0","0","0","0","1"
"562","560","14693","1508","Commit 1658831 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1658831 ]
LUCENE-6233: speed up CheckIndex when the index has term vectors","Commit 1658831 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1658831 ]
LUCENE-6233: speed up CheckIndex when the index has term vectors","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"563","561","14694","1508","Commit 1658832 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1658832 ]
LUCENE-6233: speed up CheckIndex when the index has term vectors","Commit 1658832 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1658832 ]
LUCENE-6233: speed up CheckIndex when the index has term vectors","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"564","562","14695","1508","Bulk close after 5.1 release","Bulk close after 5.1 release","thelabdude","NULL","1","decision","0","0","0","0","1"
"554","563","14687","1508","This was introduced with LUCENE-5610
I'll fix the nightly Lucene benchmark to plot CheckIndex time ... we could have spotted this performance regression.","This was introduced with LUCENE-5610
I'll fix the nightly Lucene benchmark to plot CheckIndex time ... we could have spotted this performance regression.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"555","564","14688","1508","
I think CheckIndex should not check Terms.getMin/Max for TVs?
+1","I think CheckIndex should not check Terms.getMin/Max for TVs?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"565","565","15629","1583","Patch attached + a couple of unit tests for allTermsRequired=false","Patch attached + a couple of unit tests for allTermsRequired=false","boonious","NULL","1","alternative","0","1","0","0","0"
"566","566","15630","1583","I didn't see this patch yet, but as I said in SOLR-6648, I think it makes sense to set those defaults in the constructor. ","I didn't see this patch yet, but as I said in SOLR-6648, I think it makes sense to set those defaults in the constructor.","tomasflobbe","NULL","1","alternative, pro","0","1","1","0","0"
"567","567","15631","1583","Boon Low, could you update the patch to a recent version of trunk? Also, could you add javadocs to the newly added constructors?","Boon Low, could you update the patch to a recent version of trunk?","tomasflobbe","NULL","1","decision","0","0","0","0","1"
"568","568","15631","1583","Boon Low, could you update the patch to a recent version of trunk? Also, could you add javadocs to the newly added constructors?","Also, could you add javadocs to the newly added constructors?","tomasflobbe","NULL","1","alternative","0","1","0","0","0"
"569","569","15632","1583","That patch was based upon and tested with the v4.10.3 release on Dec 20. But I can see that have been significant changes to AnalyzingInfixSuggester in the trunk.
Shall update and test the patch tomorrow.","That patch was based upon and tested with the v4.10.3 release on Dec 20.","boonious","NULL","0",NULL,"0","0","0","0","0"
"570","570","15632","1583","That patch was based upon and tested with the v4.10.3 release on Dec 20. But I can see that have been significant changes to AnalyzingInfixSuggester in the trunk.
Shall update and test the patch tomorrow.","But I can see that have been significant changes to AnalyzingInfixSuggester in the trunk.","boonious","NULL","0",NULL,"0","0","0","0","0"
"571","571","15632","1583","That patch was based upon and tested with the v4.10.3 release on Dec 20. But I can see that have been significant changes to AnalyzingInfixSuggester in the trunk.
Shall update and test the patch tomorrow.","Shall update and test the patch tomorrow.","boonious","NULL","0",NULL,"0","0","0","0","0"
"572","572","15633","1583","patch updated w.r.t. trunk 05/01/15","patch updated w.r.t.","boonious","NULL","1","alternative","0","1","0","0","0"
"573","573","15633","1583","patch updated w.r.t. trunk 05/01/15","trunk 05/01/15","boonious","NULL","0",NULL,"0","0","0","0","0"
"574","574","15634","1583","The patch looks good. I added some more tests to the new code and renamed the field highlighting->highlight","The patch looks good.","tomasflobbe","NULL","1","pro","0","0","1","0","0"
"575","575","15634","1583","The patch looks good. I added some more tests to the new code and renamed the field highlighting->highlight","I added some more tests to the new code and renamed the field highlighting->highlight","tomasflobbe","NULL","1","decision","0","0","0","0","1"
"576","576","15635","1583","Minor changes to the test and made the new fields private. I'll commit this soon","Minor changes to the test and made the new fields private.","tomasflobbe","NULL","1","alternative","0","1","0","0","0"
"577","577","15635","1583","Minor changes to the test and made the new fields private. I'll commit this soon","I'll commit this soon","tomasflobbe","NULL","1","decision","0","0","0","0","1"
"578","578","15636","1583","Commit 1649893 from Toms Fernndez Lbbe in branch 'dev/trunk'
[ https://svn.apache.org/r1649893 ]
LUCENE-6149: Infix suggesters' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup","Commit 1649893 from Toms Fernndez Lbbe in branch 'dev/trunk'
[ https://svn.apache.org/r1649893 ]
LUCENE-6149: Infix suggesters' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"579","579","15637","1583","Commit 1649955 from Toms Fernndez Lbbe in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1649955 ]
LUCENE-6149: Infix suggesters' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup","Commit 1649955 from Toms Fernndez Lbbe in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1649955 ]
LUCENE-6149: Infix suggesters' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"580","580","15638","1583","Thanks Toms, good to see the patch making into the trunk and branch_5x. I shall find some time soon to update and post the v.4.10.3 patch to include your changes.","Thanks Toms, good to see the patch making into the trunk and branch_5x.","boonious","NULL","1","decision","0","0","0","0","1"
"581","581","15638","1583","Thanks Toms, good to see the patch making into the trunk and branch_5x. I shall find some time soon to update and post the v.4.10.3 patch to include your changes.","I shall find some time soon to update and post the v.4.10.3 patch to include your changes.","boonious","NULL","0",NULL,"0","0","0","0","0"
"582","582","15639","1583","There's a typo the test class: testConstructorDefatuls","There's a typo the test class: testConstructorDefatuls","boonious","NULL","0",NULL,"0","0","0","0","0"
"583","583","15640","1583","patch for v4.10.3 release","patch for v4.10.3 release","boonious","NULL","1","alternative","0","1","0","0","0"
"584","584","15641","1583","Commit 1650132 from Toms Fernndez Lbbe in branch 'dev/trunk'
[ https://svn.apache.org/r1650132 ]
LUCENE-6149: Fixed typo","Commit 1650132 from Toms Fernndez Lbbe in branch 'dev/trunk'
[ https://svn.apache.org/r1650132 ]
LUCENE-6149: Fixed typo","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"585","585","15642","1583","Commit 1650134 from Toms Fernndez Lbbe in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650134 ]
LUCENE-6149: Fixed typo","Commit 1650134 from Toms Fernndez Lbbe in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650134 ]
LUCENE-6149: Fixed typo","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"586","586","15643","1583","Fixed. Thanks Boon!","Fixed.","tomasflobbe","NULL","1","decision","0","0","0","0","1"
"587","587","15643","1583","Fixed. Thanks Boon!","Thanks Boon!","tomasflobbe","NULL","0",NULL,"0","0","0","0","0"
"588","588","15644","1583","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"589","589","15813","1611","Simple patch + test.","Simple patch + test.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"590","590","15814","1611","Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.
E.g. when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.","Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"591","591","15814","1611","Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.
E.g. when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.","E.g.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"592","592","15814","1611","Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.
E.g. when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.","when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"593","593","15816","1611","I ran some tests with this approach and I think it's no good.
This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.
I think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ...","I ran some tests with this approach and I think it's no good.","mikemccand","NULL","1","con","0","0","0","1","0"
"594","594","15816","1611","I ran some tests with this approach and I think it's no good.
This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.
I think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ...","This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.","mikemccand","NULL","1","con","0","0","0","1","0"
"595","595","15816","1611","I ran some tests with this approach and I think it's no good.
This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.
I think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ...","When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.","mikemccand","NULL","1","con","0","0","0","1","0"
"596","596","15816","1611","I ran some tests with this approach and I think it's no good.
This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.
I think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ...","I think it's better if all throttling efforts happen in one place, e.g.","mikemccand","NULL","1","pro","0","0","1","0","0"
"597","597","15816","1611","I ran some tests with this approach and I think it's no good.
This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.
I think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ...","CMS.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"598","598","15816","1611","I ran some tests with this approach and I think it's no good.
This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.  When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.
I think it's better if all throttling efforts happen in one place, e.g. CMS.  I'l think about it ...","I'l think about it ...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"599","599","15818","1611","great to see progress nuking checkabort!","great to see progress nuking checkabort!","rcmuir","NULL","1","pro","0","0","1","0","0"
"600","600","15820","1611","

    // Defensive: sleep for at most 250 msec; the loop above will call us again if we should keep sleeping:
    if (curPauseNS > 250L*1000000000) {
      curPauseNS = 250L*1000000000;
    }


Did you mean 250 milliseconds or 250 seconds?","

    // Defensive: sleep for at most 250 msec; the loop above will call us again if we should keep sleeping:
    if (curPauseNS > 250L*1000000000) {
      curPauseNS = 250L*1000000000;
    }


Did you mean 250 milliseconds or 250 seconds?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"601","601","15821","1611","+1, I really like this approach.","+1, I really like this approach.","rcmuir","NULL","1","pro","0","0","1","0","0"
"602","602","15822","1611","Did you mean 250 milliseconds or 250 seconds?
Woops!  I'll fix, thanks.","Did you mean 250 milliseconds or 250 seconds?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"603","603","15822","1611","Did you mean 250 milliseconds or 250 seconds?
Woops!  I'll fix, thanks.","Woops!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"604","604","15822","1611","Did you mean 250 milliseconds or 250 seconds?
Woops!  I'll fix, thanks.","I'll fix, thanks.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"605","605","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.","jpountz","NULL","1","pro","0","0","1","0","0"
"606","606","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","+  /** true if we should rate-limit writes for each merge; false if not.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"607","607","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)?","jpountz","NULL","1","con","0","0","0","1","0"
"608","608","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge.","jpountz","NULL","1","con","0","0","0","1","0"
"609","609","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","*/
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?","jpountz","NULL","1","con","0","0","0","1","0"
"610","610","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"611","611","15824","1611","I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.  


+  /** true if we should rate-limit writes for each merge; false if not.  null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)? There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge. */
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?


/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit. */
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","*/
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"612","612","15825","1611","Thanks Adrien Grand, here's a new patch with those fixes.","Thanks Adrien Grand, here's a new patch with those fixes.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"613","613","15826","1611","Commit 1649532 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1649532 ]
LUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate","Commit 1649532 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1649532 ]
LUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"614","614","15827","1611","Commit 1649539 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1649539 ]
LUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate","Commit 1649539 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1649539 ]
LUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"615","615","15828","1611","Commit 1650025 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650025 ]
LUCENE-6119: fix just arrived merge to throttle correctly","Commit 1650025 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650025 ]
LUCENE-6119: fix just arrived merge to throttle correctly","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"616","616","15829","1611","Commit 1650026 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650026 ]
LUCENE-6119: fix just arrived merge to throttle correctly","Commit 1650026 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650026 ]
LUCENE-6119: fix just arrived merge to throttle correctly","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"617","617","15830","1611","Commit 1650027 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650027 ]
LUCENE-6119: fix just arrived merge to throttle correctly","Commit 1650027 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650027 ]
LUCENE-6119: fix just arrived merge to throttle correctly","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"618","618","15831","1611","Commit 1650463 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650463 ]
LUCENE-6119: set initial rate for forced merge correctly","Commit 1650463 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650463 ]
LUCENE-6119: set initial rate for forced merge correctly","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"619","619","15832","1611","Commit 1650464 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650464 ]
LUCENE-6119: set initial rate for forced merge correctly","Commit 1650464 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650464 ]
LUCENE-6119: set initial rate for forced merge correctly","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"620","620","15833","1611","Commit 1650594 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650594 ]
LUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping","Commit 1650594 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650594 ]
LUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"621","621","15834","1611","Commit 1650595 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650595 ]
LUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping","Commit 1650595 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650595 ]
LUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"622","622","15835","1611","Commit 1651305 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1651305 ]
LUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter","Commit 1651305 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1651305 ]
LUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"623","623","15836","1611","Commit 1651307 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1651307 ]
LUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter","Commit 1651307 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1651307 ]
LUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"624","624","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well.","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"625","625","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","Let me explain.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"626","626","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing).","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"627","627","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?).","dweiss","NULL","1","issue, alternative","1","1","0","0","0"
"628","628","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.","dweiss","NULL","1","issue","1","0","0","0","0"
"629","629","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).","dweiss","NULL","1","alternative","0","1","0","0","0"
"630","630","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","Now the big question is what this algorithm should look like, of course.","dweiss","NULL","1","issue","1","0","0","0","0"
"631","631","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms.","dweiss","NULL","1","alternative","0","1","0","0","0"
"632","632","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are).","dweiss","NULL","1","alternative","0","1","0","0","0"
"633","633","15837","1611","I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well. Let me explain.
When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing). Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?). The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.

What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f  the optimal number of merge threads would emerge by itself from looking at the data).

Now the big question is what this algorithm should look like, of course. The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms. 
I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are). I'll report my impressions once I have it done.","I'll report my impressions once I have it done.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"634","634","15838","1611","I think auto-tuning merge thread count would be a great addition!","I think auto-tuning merge thread count would be a great addition!","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"635","635","15839","1611","I know. It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.) off the map. And it could gracefully play with other components of the system without clogging everything (like ionice). We'll see.","I know.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"636","636","15839","1611","I know. It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.) off the map. And it could gracefully play with other components of the system without clogging everything (like ionice). We'll see.","It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.)","dweiss","NULL","1","con","0","0","0","1","0"
"637","637","15839","1611","I know. It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.) off the map. And it could gracefully play with other components of the system without clogging everything (like ionice). We'll see.","off the map.","dweiss","NULL","1","pro","0","0","1","0","0"
"638","638","15839","1611","I know. It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.) off the map. And it could gracefully play with other components of the system without clogging everything (like ionice). We'll see.","And it could gracefully play with other components of the system without clogging everything (like ionice).","dweiss","NULL","1","pro","0","0","1","0","0"
"639","639","15839","1611","I know. It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.) off the map. And it could gracefully play with other components of the system without clogging everything (like ionice). We'll see.","We'll see.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"640","640","15840","1611","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"641","641","16416","1683","Patch against branch_5x","Patch against branch_5x","rjernst","NULL","1","alternative","0","1","0","0","0"
"642","642","16417","1683","Oops.  Here is the real patch against branch_5x.","Oops.","rjernst","NULL","0",NULL,"0","0","0","0","0"
"643","643","16417","1683","Oops.  Here is the real patch against branch_5x.","Here is the real patch against branch_5x.","rjernst","NULL","1","alternative","0","1","0","0","0"
"644","644","16418","1683","+1, looks good.","+1, looks good.","rcmuir","NULL","1","pro","0","0","1","0","0"
"645","645","16419","1683","Commit 1636025 from Ryan Ernst in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1636025 ]
LUCENE-6043: Fix backcompat support for UAX29URLEmailTokenizer","Commit 1636025 from Ryan Ernst in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1636025 ]
LUCENE-6043: Fix backcompat support for UAX29URLEmailTokenizer","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"646","646","16420","1683","Commit 1636074 from Ryan Ernst in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1636074 ]
LUCENE-6043: Forgot to svn add the new file","Commit 1636074 from Ryan Ernst in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1636074 ]
LUCENE-6043: Forgot to svn add the new file","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"647","647","16421","1683","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"648","648","16677","1716","Committed to trunk and branch_5x.
Thanks again for reporting, Ilia Sretenskii!","Committed to trunk and branch_5x.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"649","649","16677","1716","Committed to trunk and branch_5x.
Thanks again for reporting, Ilia Sretenskii!","Thanks again for reporting, Ilia Sretenskii!","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"650","650","16679","1716","Unfortunately I can not help you with that, guys.
My experience is limited with Apache Maven and I honestly have no idea about how Apache Ant and Apache Ivy work at all.
Isn't that just too many of different dependency managers to mess with, that are causing their own conflicts troubles?","Unfortunately I can not help you with that, guys.","koichisenada","NULL","0",NULL,"0","0","0","0","0"
"651","651","16679","1716","Unfortunately I can not help you with that, guys.
My experience is limited with Apache Maven and I honestly have no idea about how Apache Ant and Apache Ivy work at all.
Isn't that just too many of different dependency managers to mess with, that are causing their own conflicts troubles?","My experience is limited with Apache Maven and I honestly have no idea about how Apache Ant and Apache Ivy work at all.","koichisenada","NULL","0",NULL,"0","0","0","0","0"
"652","652","16679","1716","Unfortunately I can not help you with that, guys.
My experience is limited with Apache Maven and I honestly have no idea about how Apache Ant and Apache Ivy work at all.
Isn't that just too many of different dependency managers to mess with, that are causing their own conflicts troubles?","Isn't that just too many of different dependency managers to mess with, that are causing their own conflicts troubles?","koichisenada","NULL","1","con","0","0","0","1","0"
"653","653","16684","1716","+1 for backport to 4.10.x","+1 for backport to 4.10.x","janhoy","NULL","1","pro","0","0","1","0","0"
"654","654","16685","1716","Reopening to backport to the 4.10 branch","Reopening to backport to the 4.10 branch","steve_rowe","NULL","1","decision","0","0","0","0","1"
"655","655","16688","1716","Committed to lucene_solr_4_10.","Committed to lucene_solr_4_10.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"656","656","16689","1716","Thanks Jan. - Steve
","Thanks Jan. - Steve","sarowe@syr.edu","NULL","0",NULL,"0","0","0","0","0"
"657","657","16690","1716","updated description with work around for older sources","updated description with work around for older sources","hossman","NULL","1","decision","0","0","0","0","1"
"658","658","16781","1722","Can you post a test case which fails with this exception?","Can you post a test case which fails with this exception?","shaie","NULL","0",NULL,"0","0","0","0","0"
"659","659","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.","djotanov","NULL","0",NULL,"0","0","0","0","0"
"660","660","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.","djotanov","NULL","1","alternative","0","1","0","0","0"
"661","661","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","Of 26 segments that my index have, only one is coming with NullPointerException.","djotanov","NULL","1","issue","1","0","0","0","0"
"662","662","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","If I send 4 instead of 5 terms, it works properly.","djotanov","NULL","1","issue","1","0","0","0","0"
"663","663","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","Also if I leave out the NOT term, it works with any number of terms.","djotanov","NULL","1","issue","1","0","0","0","0"
"664","664","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","I'm sorry I couldn't be of more help.","djotanov","NULL","0",NULL,"0","0","0","0","0"
"665","665","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","If you tell me where to look and how to debug I can do it and post the results.","djotanov","NULL","0",NULL,"0","0","0","0","0"
"666","666","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception .","djotanov","NULL","1","issue, alternative","1","1","0","0","0"
"667","667","16782","1722","I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.
Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.
Of 26 segments that my index have, only one is coming with NullPointerException. If I send 4 instead of 5 terms, it works properly. Also if I leave out the NOT term, it works with any number of terms.
I'm sorry I couldn't be of more help. If you tell me where to look and how to debug I can do it and post the results.
I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception . In any way, this unchecked exception should probably be caught somewhere.","In any way, this unchecked exception should probably be caught somewhere.","djotanov","NULL","1","issue","1","0","0","0","0"
"668","668","16783","1722","If ReqExclScorer has a null 'req' part, then something in the logic of BooleanWeight.","If ReqExclScorer has a null 'req' part, then something in the logic of BooleanWeight.","rcmuir","NULL","1","issue","1","0","0","0","0"
"669","669","16784","1722","ok I see the bug. ReqExclScorer can indeed be buggy here, because it marks 'req' as null for 'exhausted'. So if you call cost() after that, you will get NPE.","ok I see the bug.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"670","670","16784","1722","ok I see the bug. ReqExclScorer can indeed be buggy here, because it marks 'req' as null for 'exhausted'. So if you call cost() after that, you will get NPE.","ReqExclScorer can indeed be buggy here, because it marks 'req' as null for 'exhausted'.","rcmuir","NULL","1","issue","1","0","0","0","0"
"671","671","16784","1722","ok I see the bug. ReqExclScorer can indeed be buggy here, because it marks 'req' as null for 'exhausted'. So if you call cost() after that, you will get NPE.","So if you call cost() after that, you will get NPE.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"672","672","16787","1722","We've been seeing this issue semi-regularly in our app. We've been working around it so far by simplifying our queries to remove clauses (probably a good idea anyway) but we can never be sure our users won't find a way to break it!","We've been seeing this issue semi-regularly in our app.","spmiller","NULL","1","issue","1","0","0","0","0"
"673","673","16787","1722","We've been seeing this issue semi-regularly in our app. We've been working around it so far by simplifying our queries to remove clauses (probably a good idea anyway) but we can never be sure our users won't find a way to break it!","We've been working around it so far by simplifying our queries to remove clauses (probably a good idea anyway) but we can never be sure our users won't find a way to break it!","spmiller","NULL","1","alternative, pro, con","0","1","1","1","0"
"674","674","16788","1722","this bug was fixed on trunk when reqScorer was modified to no longer be set to null
this patch is against 5.0, where ReqScorer can hit NPE if cost is called after nextDoc
patch includes test and fix to call cost before nextDoc","this bug was fixed on trunk when reqScorer was modified to no longer be set to null
this patch is against 5.0, where ReqScorer can hit NPE if cost is called after nextDoc
patch includes test and fix to call cost before nextDoc","janechang","NULL","1","decision","0","0","0","0","1"
"675","675","16790","1722","Commit 1662673 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'
[ https://svn.apache.org/r1662673 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches","Commit 1662673 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'
[ https://svn.apache.org/r1662673 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"676","676","16791","1722","Commit 1662674 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662674 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches","Commit 1662674 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662674 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"677","677","16792","1722","Thank you Dragan and jane!","Thank you Dragan and jane!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"678","678","16793","1722","Commit 1662681 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1662681 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches","Commit 1662681 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1662681 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"679","679","16794","1722","Commit 1662728 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'
[ https://svn.apache.org/r1662728 ]
LUCENE-6001: null check was backwards","Commit 1662728 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'
[ https://svn.apache.org/r1662728 ]
LUCENE-6001: null check was backwards","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"680","680","16795","1722","Commit 1662732 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662732 ]
LUCENE-6001: null check was backwards","Commit 1662732 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662732 ]
LUCENE-6001: null check was backwards","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"681","681","16796","1722","Commit 1662733 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1662733 ]
LUCENE-6001: null check was backwards","Commit 1662733 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1662733 ]
LUCENE-6001: null check was backwards","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"682","682","16797","1722","Bulk close for 4.10.4 release","Bulk close for 4.10.4 release","mikemccand","NULL","1","decision","0","0","0","0","1"
"683","683","17116","1751","GitHub user andyetitmoves opened a pull request:
 https://github.com/apache/lucene-solr/pull/96
    Explicitly stop beast from running on top-level modules
    Patch for LUCENE-5968
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/bloomberg/lucene-solr trunk-beast-error
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/96.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #96

commit 79578f7cd4825d3d0d6700c4ec5581374216ac6f
Author: Ramkumar Aiyengar <andyetitmoves@gmail.com>
Date:   2014-09-21T07:33:59Z
    Explicitly stop beast from running on top-level modules
","GitHub user andyetitmoves opened a pull request:
 https://github.com/apache/lucene-solr/pull/96
    Explicitly stop beast from running on top-level modules
    Patch for LUCENE-5968
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/bloomberg/lucene-solr trunk-beast-error
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/96.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #96

commit 79578f7cd4825d3d0d6700c4ec5581374216ac6f
Author: Ramkumar Aiyengar <andyetitmoves@gmail.com>
Date:   2014-09-21T07:33:59Z
    Explicitly stop beast from running on top-level modules","githubbot","NULL","0",NULL,"0","0","0","0","0"
"684","684","17117","1751","Uwe Schindler, Hoss Man: The pull request above has changes suggested by both, could you check and commit?","Uwe Schindler, Hoss Man: The pull request above has changes suggested by both, could you check and commit?","andyetitmoves","NULL","0",NULL,"0","0","0","0","0"
"685","685","17119","1751","Github user uschindler commented on the pull request:
 https://github.com/apache/lucene-solr/pull/96#issuecomment-56595072
    Hi, see comments on https://issues.apache.org/jira/browse/LUCENE-5968 !","Github user uschindler commented on the pull request:
 https://github.com/apache/lucene-solr/pull/96#issuecomment-56595072
    Hi, see comments on https://issues.apache.org/jira/browse/LUCENE-5968 !","githubbot","NULL","0",NULL,"0","0","0","0","0"
"686","686","17120","1751","Done..","Done..","andyetitmoves","NULL","0",NULL,"0","0","0","0","0"
"687","687","17121","1751","Hey Uwe Schindler, I have addressed your comments, could this be merged in? Thanks!","Hey Uwe Schindler, I have addressed your comments, could this be merged in?","andyetitmoves","NULL","0",NULL,"0","0","0","0","0"
"688","688","17121","1751","Hey Uwe Schindler, I have addressed your comments, could this be merged in? Thanks!","Thanks!","andyetitmoves","NULL","0",NULL,"0","0","0","0","0"
"689","689","17122","1751","Sorry,
I missed this issue. I will commit in a moment. Sorry.
Uwe","Sorry,
I missed this issue.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"690","690","17122","1751","Sorry,
I missed this issue. I will commit in a moment. Sorry.
Uwe","I will commit in a moment.","thetaphi","NULL","1","decision","0","0","0","0","1"
"691","691","17122","1751","Sorry,
I missed this issue. I will commit in a moment. Sorry.
Uwe","Sorry.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"692","692","17122","1751","Sorry,
I missed this issue. I will commit in a moment. Sorry.
Uwe","Uwe","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"693","693","17123","1751","Commit 1642488 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1642488 ]
LUCENE-5968: Improve error message when 'ant beast' is run on top-level modules
This closes #96","Commit 1642488 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1642488 ]
LUCENE-5968: Improve error message when 'ant beast' is run on top-level modules
This closes #96","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"694","694","17124","1751","Commit 1642489 from Uwe Schindler in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1642489 ]
Merged revision(s) 1642488 from lucene/dev/trunk:
LUCENE-5968: Improve error message when 'ant beast' is run on top-level modules
This closes #96","Commit 1642489 from Uwe Schindler in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1642489 ]
Merged revision(s) 1642488 from lucene/dev/trunk:
LUCENE-5968: Improve error message when 'ant beast' is run on top-level modules
This closes #96","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"695","695","17125","1751","Github user asfgit closed the pull request at:
 https://github.com/apache/lucene-solr/pull/96","Github user asfgit closed the pull request at:
 https://github.com/apache/lucene-solr/pull/96","githubbot","NULL","0",NULL,"0","0","0","0","0"
"696","696","17126","1751","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"697","697","17128","1753","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"698","698","17129","1753","Patch requiring resourceDescription (either datainput, or string). We had quite a few places missing this.","Patch requiring resourceDescription (either datainput, or string).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"699","699","17129","1753","Patch requiring resourceDescription (either datainput, or string). We had quite a few places missing this.","We had quite a few places missing this.","rcmuir","NULL","1","issue","1","0","0","0","0"
"700","700","17130","1753","+1 for this!","+1 for this!","thetaphi","NULL","1","pro","0","0","1","0","0"
"701","701","17131","1753","+1, looks awesome.","+1, looks awesome.","mikemccand","NULL","1","pro","0","0","1","0","0"
"702","702","17132","1753","Commit 1626372 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1626372 ]
LUCENE-5965: CorruptIndexException requires a String or DataInput resource","Commit 1626372 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1626372 ]
LUCENE-5965: CorruptIndexException requires a String or DataInput resource","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"703","703","17133","1753","Commit 1626375 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626375 ]
LUCENE-5965: CorruptIndexException requires a String or DataInput resource","Commit 1626375 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626375 ]
LUCENE-5965: CorruptIndexException requires a String or DataInput resource","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"704","704","17134","1753","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"705","705","17140","1755","Patch with suggested changes.","Patch with suggested changes.","markus_heiden","NULL","1","alternative","0","1","0","0","0"
"706","706","17141","1755","Thanks Markus, looks great, I'll commit shortly.","Thanks Markus, looks great, I'll commit shortly.","mikemccand","NULL","1","pro","0","0","1","0","0"
"707","707","17142","1755","Commit 1626241 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1626241 ]
LUCENE-5963: more efficient AnalyzingSuggester.replaceSep","Commit 1626241 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1626241 ]
LUCENE-5963: more efficient AnalyzingSuggester.replaceSep","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"708","708","17143","1755","Commit 1626242 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626242 ]
LUCENE-5963: more efficient AnalyzingSuggester.replaceSep","Commit 1626242 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626242 ]
LUCENE-5963: more efficient AnalyzingSuggester.replaceSep","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"709","709","17144","1755","Thanks Markus!","Thanks Markus!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"710","710","17145","1755","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"711","711","17636","1785","Patch, starting from Vitaly's test case (thank you!) and folding into Lucene's tests ... it fails with this on trunk:

1) testReopenReaderToOlderCommit(org.apache.lucene.index.TestDirectoryReaderReopen)
java.lang.IllegalStateException: same segment _0 has invalid changes; likely you are re-opening a reader after illegally removing index files yourself and building a new index in their place.  Use IndexWriter.deleteAll or OpenMode.CREATE instead
	at __randomizedtesting.SeedInfo.seed([D3F22B13D5839643:931C8A9673D003F4]:0)
	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:190)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:323)
	at org.apache.lucene.index.StandardDirectoryReader$2.doBody(StandardDirectoryReader.java:317)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromCommit(StandardDirectoryReader.java:312)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenNoWriter(StandardDirectoryReader.java:308)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:259)
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
	at org.apache.lucene.index.TestDirectoryReaderReopen.testReopenReaderToOlderCommit(TestDirectoryReaderReopen.java:824)

","Patch, starting from Vitaly's test case (thank you!)","mikemccand","NULL","1","alternative","0","1","0","0","0"
"712","712","17636","1785","Patch, starting from Vitaly's test case (thank you!) and folding into Lucene's tests ... it fails with this on trunk:

1) testReopenReaderToOlderCommit(org.apache.lucene.index.TestDirectoryReaderReopen)
java.lang.IllegalStateException: same segment _0 has invalid changes; likely you are re-opening a reader after illegally removing index files yourself and building a new index in their place.  Use IndexWriter.deleteAll or OpenMode.CREATE instead
	at __randomizedtesting.SeedInfo.seed([D3F22B13D5839643:931C8A9673D003F4]:0)
	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:190)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:323)
	at org.apache.lucene.index.StandardDirectoryReader$2.doBody(StandardDirectoryReader.java:317)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromCommit(StandardDirectoryReader.java:312)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenNoWriter(StandardDirectoryReader.java:308)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:259)
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
	at org.apache.lucene.index.TestDirectoryReaderReopen.testReopenReaderToOlderCommit(TestDirectoryReaderReopen.java:824)

","and folding into Lucene's tests ... it fails with this on trunk:

1) testReopenReaderToOlderCommit(org.apache.lucene.index.TestDirectoryReaderReopen)
java.lang.IllegalStateException: same segment _0 has invalid changes; likely you are re-opening a reader after illegally removing index files yourself and building a new index in their place.","mikemccand","NULL","1","issue","1","0","0","0","0"
"713","713","17636","1785","Patch, starting from Vitaly's test case (thank you!) and folding into Lucene's tests ... it fails with this on trunk:

1) testReopenReaderToOlderCommit(org.apache.lucene.index.TestDirectoryReaderReopen)
java.lang.IllegalStateException: same segment _0 has invalid changes; likely you are re-opening a reader after illegally removing index files yourself and building a new index in their place.  Use IndexWriter.deleteAll or OpenMode.CREATE instead
	at __randomizedtesting.SeedInfo.seed([D3F22B13D5839643:931C8A9673D003F4]:0)
	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:190)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:323)
	at org.apache.lucene.index.StandardDirectoryReader$2.doBody(StandardDirectoryReader.java:317)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromCommit(StandardDirectoryReader.java:312)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenNoWriter(StandardDirectoryReader.java:308)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:259)
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
	at org.apache.lucene.index.TestDirectoryReaderReopen.testReopenReaderToOlderCommit(TestDirectoryReaderReopen.java:824)

","Use IndexWriter.deleteAll or OpenMode.CREATE instead
	at __randomizedtesting.SeedInfo.seed([D3F22B13D5839643:931C8A9673D003F4]:0)
	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:190)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:323)
	at org.apache.lucene.index.StandardDirectoryReader$2.doBody(StandardDirectoryReader.java:317)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromCommit(StandardDirectoryReader.java:312)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenNoWriter(StandardDirectoryReader.java:308)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:259)
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
	at org.apache.lucene.index.TestDirectoryReaderReopen.testReopenReaderToOlderCommit(TestDirectoryReaderReopen.java:824)","mikemccand","NULL","1","issue","1","0","0","0","0"
"714","714","17637","1785","Patch: I think the use-case is cool and it should be supported: its just adding an 'if' and removing the current exception (which is geared at protecting some user who manually rm -rf's files from their index.)
I improved the test a bit to ensure that cores are shared and also tested the dv updates case.","Patch: I think the use-case is cool and it should be supported: its just adding an 'if' and removing the current exception (which is geared at protecting some user who manually rm -rf's files from their index.)","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"715","715","17637","1785","Patch: I think the use-case is cool and it should be supported: its just adding an 'if' and removing the current exception (which is geared at protecting some user who manually rm -rf's files from their index.)
I improved the test a bit to ensure that cores are shared and also tested the dv updates case.","I improved the test a bit to ensure that cores are shared and also tested the dv updates case.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"716","716","17638","1785","Thanks Rob, patch looks great, except: I think we can keep the illegalDocCountChange safety?  I think I could make a test case to trip that ...","Thanks Rob, patch looks great, except: I think we can keep the illegalDocCountChange safety?","mikemccand","NULL","1","pro, con","0","0","1","1","0"
"717","717","17638","1785","Thanks Rob, patch looks great, except: I think we can keep the illegalDocCountChange safety?  I think I could make a test case to trip that ...","I think I could make a test case to trip that ...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"718","718","17639","1785","Can you make a test change to trip it without manually removing files from your index?","Can you make a test change to trip it without manually removing files from your index?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"719","719","17640","1785","Can you make a test change to trip it without manually removing files from your index?
I don' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.
We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it's nice not to have false scares even if the app is doing something it shouldn't...","Can you make a test change to trip it without manually removing files from your index?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"720","720","17640","1785","Can you make a test change to trip it without manually removing files from your index?
I don' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.
We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it's nice not to have false scares even if the app is doing something it shouldn't...","I don' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"721","721","17640","1785","Can you make a test change to trip it without manually removing files from your index?
I don' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.
We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it's nice not to have false scares even if the app is doing something it shouldn't...","We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it's nice not to have false scares even if the app is doing something it shouldn't...","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"722","722","17641","1785","Yes, I really want to. We can't let such abuse prevent real features, thats just wrong.
If you want safety against deleting files, maybe look at NIO.2 WatchService. This is general and would give you notification when such things happen rather than having a hack for one particular user's mistake.","Yes, I really want to.","rcmuir","NULL","1","pro","0","0","1","0","0"
"723","723","17641","1785","Yes, I really want to. We can't let such abuse prevent real features, thats just wrong.
If you want safety against deleting files, maybe look at NIO.2 WatchService. This is general and would give you notification when such things happen rather than having a hack for one particular user's mistake.","We can't let such abuse prevent real features, thats just wrong.","rcmuir","NULL","1","con","0","0","0","1","0"
"724","724","17641","1785","Yes, I really want to. We can't let such abuse prevent real features, thats just wrong.
If you want safety against deleting files, maybe look at NIO.2 WatchService. This is general and would give you notification when such things happen rather than having a hack for one particular user's mistake.","If you want safety against deleting files, maybe look at NIO.2 WatchService.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"725","725","17641","1785","Yes, I really want to. We can't let such abuse prevent real features, thats just wrong.
If you want safety against deleting files, maybe look at NIO.2 WatchService. This is general and would give you notification when such things happen rather than having a hack for one particular user's mistake.","This is general and would give you notification when such things happen rather than having a hack for one particular user's mistake.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"726","726","17642","1785","
We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption
It is index corruption though. The user went and manually corrupted their index. Why hide that Mike?","We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption
It is index corruption though.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"727","727","17642","1785","
We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption
It is index corruption though. The user went and manually corrupted their index. Why hide that Mike?","The user went and manually corrupted their index.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"728","728","17642","1785","
We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption
It is index corruption though. The user went and manually corrupted their index. Why hide that Mike?","Why hide that Mike?","rcmuir","NULL","1","con","0","0","0","1","0"
"729","729","17646","1785","Michael,
Your updated patch definitely fixes the issue. But I just wanted to understand why deletes are so special, in that - if I don't have any buffered deletes for the segment, but new documents only, the reused reader instance won't pick them up, even without the fix in place. This is because liveDocs won't capture unflushed doc ids?","Michael,
Your updated patch definitely fixes the issue.","vfunstein","NULL","1","pro","0","0","1","0","0"
"730","730","17646","1785","Michael,
Your updated patch definitely fixes the issue. But I just wanted to understand why deletes are so special, in that - if I don't have any buffered deletes for the segment, but new documents only, the reused reader instance won't pick them up, even without the fix in place. This is because liveDocs won't capture unflushed doc ids?","But I just wanted to understand why deletes are so special, in that - if I don't have any buffered deletes for the segment, but new documents only, the reused reader instance won't pick them up, even without the fix in place.","vfunstein","NULL","1","issue","1","0","0","0","0"
"731","731","17646","1785","Michael,
Your updated patch definitely fixes the issue. But I just wanted to understand why deletes are so special, in that - if I don't have any buffered deletes for the segment, but new documents only, the reused reader instance won't pick them up, even without the fix in place. This is because liveDocs won't capture unflushed doc ids?","This is because liveDocs won't capture unflushed doc ids?","vfunstein","NULL","1","issue","1","0","0","0","0"
"732","732","17648","1785","Woops, this almost dropped past the event horizon of my TODO list.
I modernized the patch, and was able to improve how effective its check is, by switching to comparing segment IDs (a very good check that the segments changed on disk) vs what the patch used to do, comparing maxDoc.","Woops, this almost dropped past the event horizon of my TODO list.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"733","733","17648","1785","Woops, this almost dropped past the event horizon of my TODO list.
I modernized the patch, and was able to improve how effective its check is, by switching to comparing segment IDs (a very good check that the segments changed on disk) vs what the patch used to do, comparing maxDoc.","I modernized the patch, and was able to improve how effective its check is, by switching to comparing segment IDs (a very good check that the segments changed on disk) vs what the patch used to do, comparing maxDoc.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"734","734","17649","1785","Commit 664e39292bd0a90ed6f20debc872ab74a1d7294f in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=664e392 ]
LUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader","Commit 664e39292bd0a90ed6f20debc872ab74a1d7294f in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=664e392 ]
LUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"735","735","17650","1785","Commit 6b0b119074f4cd32adc2388fbcc01f2aa70c7d5d in lucene-solr's branch refs/heads/branch_6x from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6b0b119 ]
LUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader","Commit 6b0b119074f4cd32adc2388fbcc01f2aa70c7d5d in lucene-solr's branch refs/heads/branch_6x from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6b0b119 ]
LUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"736","736","17651","1785","Thanks Adrien Grand for pinging me about this almost lost issue!","Thanks Adrien Grand for pinging me about this almost lost issue!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"737","737","17652","1785","Bulk close resolved issues after 6.2.0 release.","Bulk close resolved issues after 6.2.0 release.","mikemccand","NULL","1","decision","0","0","0","0","1"
"738","738","18084","1818","Here is a simple patch.","Here is a simple patch.","jpountz","NULL","1","alternative","0","1","0","0","0"
"739","739","18085","1818","Looks good. FWIW CharTermAttribute.java has some optimizations for this method. Maybe if they are really useful we should see if we can pull them out into static methods.","Looks good.","rcmuir","NULL","1","pro","0","0","1","0","0"
"740","740","18085","1818","Looks good. FWIW CharTermAttribute.java has some optimizations for this method. Maybe if they are really useful we should see if we can pull them out into static methods.","FWIW CharTermAttribute.java has some optimizations for this method.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"741","741","18085","1818","Looks good. FWIW CharTermAttribute.java has some optimizations for this method. Maybe if they are really useful we should see if we can pull them out into static methods.","Maybe if they are really useful we should see if we can pull them out into static methods.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"742","742","18088","1818","Thanks Uwe, I'll fix before committing, what a crazy requirement!
There are potential things to share with CharTermAttributeImpl but I'd rather leave it to another issue as it has some pitfalls as noted.","Thanks Uwe, I'll fix before committing, what a crazy requirement!","jpountz","NULL","0",NULL,"0","0","0","0","0"
"743","743","18088","1818","Thanks Uwe, I'll fix before committing, what a crazy requirement!
There are potential things to share with CharTermAttributeImpl but I'd rather leave it to another issue as it has some pitfalls as noted.","There are potential things to share with CharTermAttributeImpl but I'd rather leave it to another issue as it has some pitfalls as noted.","jpountz","NULL","1","con","0","0","0","1","0"
"744","744","18089","1818","Commit 1618625 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1618625 ]
LUCENE-5892: CharsRefBuilder implements Accountable.","Commit 1618625 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1618625 ]
LUCENE-5892: CharsRefBuilder implements Accountable.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"745","745","18090","1818","Commit 1618627 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1618627 ]
LUCENE-5892: CharsRefBuilder implements Accountable.","Commit 1618627 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1618627 ]
LUCENE-5892: CharsRefBuilder implements Accountable.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"746","746","18091","1818","Commit 1618628 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1618628 ]
LUCENE-5892: Missed the covariant return type.","Commit 1618628 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1618628 ]
LUCENE-5892: Missed the covariant return type.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"747","747","18092","1818","Commit 1618629 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1618629 ]
LUCENE-5892: Missed the covariant return type.","Commit 1618629 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1618629 ]
LUCENE-5892: Missed the covariant return type.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"748","748","18093","1818","Thanks!
BTW: Typical example is StringBuilder itsself: It implements Appendable, but also uses covariant return type: http://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html#append(java.lang.CharSequence)","Thanks!","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"749","749","18093","1818","Thanks!
BTW: Typical example is StringBuilder itsself: It implements Appendable, but also uses covariant return type: http://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html#append(java.lang.CharSequence)","BTW: Typical example is StringBuilder itsself: It implements Appendable, but also uses covariant return type: http://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html#append(java.lang.CharSequence)","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"750","750","18742","1868","Here is a patch.
Do we have a benchmark that could be used to validate this change? I just checked out luceneutil but it only seems to have tasks for queries, not sorting?","Here is a patch.","jpountz","NULL","1","alternative","0","1","0","0","0"
"751","751","18742","1868","Here is a patch.
Do we have a benchmark that could be used to validate this change? I just checked out luceneutil but it only seems to have tasks for queries, not sorting?","Do we have a benchmark that could be used to validate this change?","jpountz","NULL","1","issue","1","0","0","0","0"
"752","752","18742","1868","Here is a patch.
Do we have a benchmark that could be used to validate this change? I just checked out luceneutil but it only seems to have tasks for queries, not sorting?","I just checked out luceneutil but it only seems to have tasks for queries, not sorting?","jpountz","NULL","1","alternative, con","0","1","0","1","0"
"753","753","18743","1868","I dont understand how the MatchNoBits case is safe.","I dont understand how the MatchNoBits case is safe.","rcmuir","NULL","1","con","0","0","0","1","0"
"754","754","18744","1868","If no document has values, then they will all return the missing value?","If no document has values, then they will all return the missing value?","jpountz","NULL","1","issue","1","0","0","0","0"
"755","755","18745","1868","Oops, I understand your question now, I didn't upload the latest version of my patch. ","Oops, I understand your question now, I didn't upload the latest version of my patch.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"756","756","18746","1868","i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.
I think the current null check is effective? it has to be handled anyway. And personally i would be wary of overspecialization here...","i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"757","757","18746","1868","i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.
I think the current null check is effective? it has to be handled anyway. And personally i would be wary of overspecialization here...","I think the current null check is effective?","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"758","758","18746","1868","i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.
I think the current null check is effective? it has to be handled anyway. And personally i would be wary of overspecialization here...","it has to be handled anyway.","rcmuir","NULL","1","pro","0","0","1","0","0"
"759","759","18746","1868","i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.
I think the current null check is effective? it has to be handled anyway. And personally i would be wary of overspecialization here...","And personally i would be wary of overspecialization here...","rcmuir","NULL","1","con","0","0","0","1","0"
"760","760","18747","1868","I dont think there is specialization needed.  The null check, Robert mentions, was done like this to optimize missing values.
the null check has to be done anyways by the jvm, so removing it brings nothing.
see the original missing values issue for discussion.","I dont think there is specialization needed.","thetaphi","NULL","1","con","0","0","0","1","0"
"761","761","18747","1868","I dont think there is specialization needed.  The null check, Robert mentions, was done like this to optimize missing values.
the null check has to be done anyways by the jvm, so removing it brings nothing.
see the original missing values issue for discussion.","The null check, Robert mentions, was done like this to optimize missing values.","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"762","762","18747","1868","I dont think there is specialization needed.  The null check, Robert mentions, was done like this to optimize missing values.
the null check has to be done anyways by the jvm, so removing it brings nothing.
see the original missing values issue for discussion.","the null check has to be done anyways by the jvm, so removing it brings nothing.","thetaphi","NULL","1","alternative, con","0","1","0","1","0"
"763","763","18747","1868","I dont think there is specialization needed.  The null check, Robert mentions, was done like this to optimize missing values.
the null check has to be done anyways by the jvm, so removing it brings nothing.
see the original missing values issue for discussion.","see the original missing values issue for discussion.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"764","764","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.","jpountz","NULL","1","issue","1","0","0","0","0"
"765","765","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents.","jpountz","NULL","1","alternative","0","1","0","0","0"
"766","766","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value.","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"767","767","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","So there is no need to check the missing values in the comparator.","jpountz","NULL","1","pro","0","0","1","0","0"
"768","768","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","I'm surprised that you think of it as a specialization as this is actually making things simpler?","jpountz","NULL","1","pro","0","0","1","0","0"
"769","769","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance.","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"770","770","18748","1868","Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.
The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents. To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value. So there is no need to check the missing values in the comparator.
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.","And it makes it easier (and potentially more efficient) to write selectors.","jpountz","NULL","1","pro","0","0","1","0","0"
"771","771","18751","1868","
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.
It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","I'm surprised that you think of it as a specialization as this is actually making things simpler?","rcmuir","NULL","1","pro","0","0","1","0","0"
"772","772","18751","1868","
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.
It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"773","773","18751","1868","
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.
It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","And it makes it easier (and potentially more efficient) to write selectors.","rcmuir","NULL","1","pro","0","0","1","0","0"
"774","774","18751","1868","
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.
It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues.","rcmuir","NULL","1","pro","0","0","1","0","0"
"775","775","18751","1868","
I'm surprised that you think of it as a specialization as this is actually making things simpler? The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance. And it makes it easier (and potentially more efficient) to write selectors.
It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","rcmuir","NULL","1","con","0","0","0","1","0"
"776","776","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","the goal was not to remove the null check, but the check for missing values.","thetaphi","NULL","1","issue","1","0","0","0","0"
"777","777","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"778","778","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast.","thetaphi","NULL","1","pro","0","0","1","0","0"
"779","779","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","We checked this at the time I added this to Lucene 3.5 or like that.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"780","780","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed.","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"781","781","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","The same that Robert discovered here, too.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"782","782","18752","1868","the goal was not to remove the null check, but the check for missing values.
In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick). It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast. We checked this at the time I added this to Lucene 3.5 or like that. We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed. The same that Robert discovered here, too.
In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.","thetaphi","NULL","1","pro, con","0","0","1","1","0"
"783","783","18753","1868","It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.
Doesn't it happen already if you have two fields that have different compression?","It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues.","jpountz","NULL","1","pro","0","0","1","0","0"
"784","784","18753","1868","It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.
Doesn't it happen already if you have two fields that have different compression?","and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.","jpountz","NULL","1","con","0","0","0","1","0"
"785","785","18753","1868","It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues. and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.
Doesn't it happen already if you have two fields that have different compression?","Doesn't it happen already if you have two fields that have different compression?","jpountz","NULL","1","issue","1","0","0","0","0"
"786","786","18754","1868","I didn't see it happening with the currently assembly generated.
I'm ok with the issue if we see a performance increase, just not seeing it.","I didn't see it happening with the currently assembly generated.","rcmuir","NULL","1","pro","0","0","1","0","0"
"787","787","18754","1868","I didn't see it happening with the currently assembly generated.
I'm ok with the issue if we see a performance increase, just not seeing it.","I'm ok with the issue if we see a performance increase, just not seeing it.","rcmuir","NULL","1","con","0","0","0","1","0"
"788","788","18755","1868","Fair enough","Fair enough","jpountz","NULL","1","pro","0","0","1","0","0"
"789","789","19003","1904","I don't think we should remove the default implementation for FilterScorer, as the scorer is not really changed when using this abstract class, its just wrapped? For the same reason, i think the boostingscorer (since its just an implementation detail of how the current BS2 stuff solves this case) should be transparent.","I don't think we should remove the default implementation for FilterScorer, as the scorer is not really changed when using this abstract class, its just wrapped?","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"790","790","19003","1904","I don't think we should remove the default implementation for FilterScorer, as the scorer is not really changed when using this abstract class, its just wrapped? For the same reason, i think the boostingscorer (since its just an implementation detail of how the current BS2 stuff solves this case) should be transparent.","For the same reason, i think the boostingscorer (since its just an implementation detail of how the current BS2 stuff solves this case) should be transparent.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"791","791","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","Thanks for taking the time to review my patch and comment on the approach.","shebiki","NULL","0",NULL,"0","0","0","0","0"
"792","792","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.","shebiki","NULL","1","pro","0","0","1","0","0"
"793","793","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.","shebiki","NULL","1","issue","1","0","0","0","0"
"794","794","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play.","shebiki","NULL","1","issue","1","0","0","0","0"
"795","795","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","This would impact any classes that extend FilterScorer and don't override getChildren().","shebiki","NULL","1","con","0","0","0","1","0"
"796","796","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.","shebiki","NULL","1","pro, con","0","0","1","1","0"
"797","797","19004","1904","Thanks for taking the time to review my patch and comment on the approach.
The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.
This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.
In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play. This would impact any classes that extend FilterScorer and don't override getChildren(). In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.
If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.    ","If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.","shebiki","NULL","0",NULL,"0","0","0","0","0"
"798","798","19005","1904","I see: this makes sense. If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency... I'll look at the patch again and reply back if I have more questions.","I see: this makes sense.","rcmuir","NULL","1","pro","0","0","1","0","0"
"799","799","19005","1904","I see: this makes sense. If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency... I'll look at the patch again and reply back if I have more questions.","If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency...","rcmuir","NULL","1","pro","0","0","1","0","0"
"800","800","19005","1904","I see: this makes sense. If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency... I'll look at the patch again and reply back if I have more questions.","I'll look at the patch again and reply back if I have more questions.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"801","801","19006","1904","Commit 1608454 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1608454 ]
LUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery","Commit 1608454 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1608454 ]
LUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"802","802","19007","1904","Commit 1608457 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1608457 ]
LUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery","Commit 1608457 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1608457 ]
LUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"803","803","19008","1904","Thanks Terry!","Thanks Terry!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"804","804","19133","1924","Commit 1603521 from Michael McCandless in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1603521 ]
LUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow","Commit 1603521 from Michael McCandless in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1603521 ]
LUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"805","805","19134","1924","Commit 1603523 from Michael McCandless in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1603523 ]
LUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow","Commit 1603523 from Michael McCandless in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1603523 ]
LUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"806","806","19135","1924","Hi,
should we maybe also deprecate the Solr factories? Because then Solr prints a warning on startup if they are used.","Hi,
should we maybe also deprecate the Solr factories?","thetaphi","NULL","1","alternative","0","1","0","0","0"
"807","807","19135","1924","Hi,
should we maybe also deprecate the Solr factories? Because then Solr prints a warning on startup if they are used.","Because then Solr prints a warning on startup if they are used.","thetaphi","NULL","1","pro","0","0","1","0","0"
"808","808","19136","1924","OK will do; just add @deprecated / @Deprecated to JaspellLookupFactory right?","OK will do; just add @deprecated / @Deprecated to JaspellLookupFactory right?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"809","809","19137","1924","I am trying to check that out, too ","I am trying to check that out, too","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"810","810","19138","1924","With Eclipse it looks like this should be enough to deprecate the factory you mentioned. But the default lookup for file-based stuff in Solr is still Jaspell:

./core/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookupFactory.java:public class JaspellLookupFactory extends LookupFactory {
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:  public static String DEFAULT_FILE_BASED_DICT = JaspellLookupFactory.class.getName();
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:      lookupImpl = JaspellLookupFactory.class.getName();


So maybe we should change defaults. Should we open SOLR issue?","With Eclipse it looks like this should be enough to deprecate the factory you mentioned.","thetaphi","NULL","1","pro","0","0","1","0","0"
"811","811","19138","1924","With Eclipse it looks like this should be enough to deprecate the factory you mentioned. But the default lookup for file-based stuff in Solr is still Jaspell:

./core/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookupFactory.java:public class JaspellLookupFactory extends LookupFactory {
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:  public static String DEFAULT_FILE_BASED_DICT = JaspellLookupFactory.class.getName();
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:      lookupImpl = JaspellLookupFactory.class.getName();


So maybe we should change defaults. Should we open SOLR issue?","But the default lookup for file-based stuff in Solr is still Jaspell:

./core/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookupFactory.java:public class JaspellLookupFactory extends LookupFactory {
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:  public static String DEFAULT_FILE_BASED_DICT = JaspellLookupFactory.class.getName();
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:      lookupImpl = JaspellLookupFactory.class.getName();


So maybe we should change defaults.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"812","812","19138","1924","With Eclipse it looks like this should be enough to deprecate the factory you mentioned. But the default lookup for file-based stuff in Solr is still Jaspell:

./core/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookupFactory.java:public class JaspellLookupFactory extends LookupFactory {
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:  public static String DEFAULT_FILE_BASED_DICT = JaspellLookupFactory.class.getName();
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:      lookupImpl = JaspellLookupFactory.class.getName();


So maybe we should change defaults. Should we open SOLR issue?","Should we open SOLR issue?","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"813","813","19139","1924","So maybe we should change defaults. Should we open SOLR issue?
OK I'll open a Solr issue.","So maybe we should change defaults.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"814","814","19139","1924","So maybe we should change defaults. Should we open SOLR issue?
OK I'll open a Solr issue.","Should we open SOLR issue?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"815","815","19139","1924","So maybe we should change defaults. Should we open SOLR issue?
OK I'll open a Solr issue.","OK I'll open a Solr issue.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"816","816","19140","1924","I opened SOLR-6178","I opened SOLR-6178","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"817","817","19141","1924","Commit 1603542 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1603542 ]
LUCENE-5775: deprecate JaspellLookup","Commit 1603542 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1603542 ]
LUCENE-5775: deprecate JaspellLookup","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"818","818","19142","1924","OK I committed the deprecation to trunk; I'd really like to just remove it, but we can't do that until we address SOLR-6178 ... so I'll leave this issue open to remove Jaspell in trunk.","OK I committed the deprecation to trunk; I'd really like to just remove it, but we can't do that until we address SOLR-6178 ... so I'll leave this issue open to remove Jaspell in trunk.","mikemccand","NULL","1","con, decision","0","0","0","1","1"
"819","819","19143","1924","Commit 1604122 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1604122 ]
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","Commit 1604122 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1604122 ]
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"820","820","19144","1924","Commit 1604124 from Uwe Schindler in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1604124 ]
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","Commit 1604124 from Uwe Schindler in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1604124 ]
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"821","821","19145","1924","Commit 1604125 from Uwe Schindler in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1604125 ]
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","Commit 1604125 from Uwe Schindler in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1604125 ]
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"822","822","19146","1924","Commit 1604707 from Uwe Schindler in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1604707 ]
Revert:
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","Commit 1604707 from Uwe Schindler in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1604707 ]
Revert:
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"823","823","19147","1924","Commit 1604710 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1604710 ]
Move changes of:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","Commit 1604710 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1604710 ]
Move changes of:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"824","824","19148","1924","Commit 1604711 from Uwe Schindler in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1604711 ]
Merged revision(s) 1604710 from lucene/dev/trunk:
Move changes of:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","Commit 1604711 from Uwe Schindler in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1604711 ]
Merged revision(s) 1604710 from lucene/dev/trunk:
Move changes of:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"825","825","21213","2113","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"826","826","21214","2113","patch, I also cleaned up all remnants/conditionals about not supporting offsets.","patch, I also cleaned up all remnants/conditionals about not supporting offsets.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"827","827","21215","2113","+1, nice that all codecs support offsets now!","+1, nice that all codecs support offsets now!","mikemccand","NULL","1","pro","0","0","1","0","0"
"828","828","21216","2113","Commit 1584140 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1584140 ]
LUCENE-5563: remove sep layout","Commit 1584140 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1584140 ]
LUCENE-5563: remove sep layout","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"829","829","21217","2113","I can backport if we want to, but I'm not sure its worth the trouble here. We still have 3.x codec in 4.x, as well as the fact the blockterms readers/indexes have changed in trunk and have better testing: so backporting poses some risks.","I can backport if we want to, but I'm not sure its worth the trouble here.","rcmuir","NULL","1","con","0","0","0","1","0"
"830","830","21217","2113","I can backport if we want to, but I'm not sure its worth the trouble here. We still have 3.x codec in 4.x, as well as the fact the blockterms readers/indexes have changed in trunk and have better testing: so backporting poses some risks.","We still have 3.x codec in 4.x, as well as the fact the blockterms readers/indexes have changed in trunk and have better testing: so backporting poses some risks.","rcmuir","NULL","1","con","0","0","0","1","0"
"831","831","21218","2113","Bulk close after 5.0 release.","Bulk close after 5.0 release.","anshumg","NULL","1","decision","0","0","0","0","1"
"832","832","21429","2136","Here is a simple test. Seems to be related to the use of index-time synonyms.","Here is a simple test.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"833","833","21429","2136","Here is a simple test. Seems to be related to the use of index-time synonyms.","Seems to be related to the use of index-time synonyms.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"834","834","21430","2136","Here's a patch I'm testing (all tests seem to pass, but I will see if i can add some better ones).
FieldTermStack is ordered by position, but the current code doesn't really handle position increments of 0 (synonyms). In this case when we reach a dead-end (nextMap == null), we keep looking as long as the incoming position is the same until we find a match.","Here's a patch I'm testing (all tests seem to pass, but I will see if i can add some better ones).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"835","835","21430","2136","Here's a patch I'm testing (all tests seem to pass, but I will see if i can add some better ones).
FieldTermStack is ordered by position, but the current code doesn't really handle position increments of 0 (synonyms). In this case when we reach a dead-end (nextMap == null), we keep looking as long as the incoming position is the same until we find a match.","FieldTermStack is ordered by position, but the current code doesn't really handle position increments of 0 (synonyms).","rcmuir","NULL","1","con","0","0","0","1","0"
"836","836","21430","2136","Here's a patch I'm testing (all tests seem to pass, but I will see if i can add some better ones).
FieldTermStack is ordered by position, but the current code doesn't really handle position increments of 0 (synonyms). In this case when we reach a dead-end (nextMap == null), we keep looking as long as the incoming position is the same until we find a match.","In this case when we reach a dead-end (nextMap == null), we keep looking as long as the incoming position is the same until we find a match.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"837","837","21432","2136","+1!","+1!","jpountz","NULL","1","pro","0","0","1","0","0"
"838","838","21433","2136","Commit 1579255 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1579255 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases","Commit 1579255 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1579255 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"839","839","21434","2136","Commit 1579264 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1579264 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases","Commit 1579264 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1579264 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"840","840","21435","2136","Commit 1579269 from Robert Muir in branch 'dev/branches/lucene_solr_4_7'
[ https://svn.apache.org/r1579269 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases","Commit 1579269 from Robert Muir in branch 'dev/branches/lucene_solr_4_7'
[ https://svn.apache.org/r1579269 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"841","841","21436","2136","Bulk close 4.7.1 issues","Bulk close 4.7.1 issues","steve_rowe","NULL","1","decision","0","0","0","0","1"
"842","842","23232","2296","here is a patch","here is a patch","simonw","NULL","1","alternative","0","1","0","0","0"
"843","843","23233","2296","The first ensureOpen() in incRef() now seems redundant after this patch?","The first ensureOpen() in incRef() now seems redundant after this patch?","yseeley@gmail.com","NULL","1","con","0","0","0","1","0"
"844","844","23234","2296","The first ensureOpen() in incRef() now seems redundant after this patch?
agreed - uploaded a new patch","The first ensureOpen() in incRef() now seems redundant after this patch?","simonw","NULL","1","con","0","0","0","1","0"
"845","845","23234","2296","The first ensureOpen() in incRef() now seems redundant after this patch?
agreed - uploaded a new patch","agreed - uploaded a new patch","simonw","NULL","1","alternative, pro","0","1","1","0","0"
"846","846","23235","2296","+1, looks correct. SegemntCoreReaders atomic increment is also correct - @BrianGoetzSays ","+1, looks correct.","thetaphi","NULL","1","pro","0","0","1","0","0"
"847","847","23235","2296","+1, looks correct. SegemntCoreReaders atomic increment is also correct - @BrianGoetzSays ","SegemntCoreReaders atomic increment is also correct - @BrianGoetzSays","thetaphi","NULL","1","pro","0","0","1","0","0"
"848","848","23236","2296","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"849","849","23237","2296","Commit 1549012 from Simon Willnauer in branch 'dev/trunk'
[ https://svn.apache.org/r1549012 ]
LUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.","Commit 1549012 from Simon Willnauer in branch 'dev/trunk'
[ https://svn.apache.org/r1549012 ]
LUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"850","850","23238","2296","Commit 1549013 from Simon Willnauer in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1549013 ]
LUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.","Commit 1549013 from Simon Willnauer in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1549013 ]
LUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"851","851","23239","2297","Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened.  I clone the child queries before setting their boost so I don't break anything that expects them unchanged.  I'm not super happy that I have to clone the queries but it seemed like the simplest solution.","Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened.","nik9000@gmail.com","NULL","1","alternative","0","1","0","0","0"
"852","852","23239","2297","Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened.  I clone the child queries before setting their boost so I don't break anything that expects them unchanged.  I'm not super happy that I have to clone the queries but it seemed like the simplest solution.","I clone the child queries before setting their boost so I don't break anything that expects them unchanged.","nik9000@gmail.com","NULL","1","alternative, pro","0","1","1","0","0"
"853","853","23239","2297","Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened.  I clone the child queries before setting their boost so I don't break anything that expects them unchanged.  I'm not super happy that I have to clone the queries but it seemed like the simplest solution.","I'm not super happy that I have to clone the queries but it seemed like the simplest solution.","nik9000@gmail.com","NULL","1","pro, con","0","0","1","1","0"
"854","854","23240","2297","Thanks Nik, your fix looks good! I don't think cloning the queries is an issue, it happens all the time when doing rewrites, and it's definitely better than modifying those queries in-place.
I'll commit it tomorrow if there is no objection.","Thanks Nik, your fix looks good!","jpountz","NULL","1","pro","0","0","1","0","0"
"855","855","23240","2297","Thanks Nik, your fix looks good! I don't think cloning the queries is an issue, it happens all the time when doing rewrites, and it's definitely better than modifying those queries in-place.
I'll commit it tomorrow if there is no objection.","I don't think cloning the queries is an issue, it happens all the time when doing rewrites, and it's definitely better than modifying those queries in-place.","jpountz","NULL","1","pro","0","0","1","0","0"
"856","856","23240","2297","Thanks Nik, your fix looks good! I don't think cloning the queries is an issue, it happens all the time when doing rewrites, and it's definitely better than modifying those queries in-place.
I'll commit it tomorrow if there is no objection.","I'll commit it tomorrow if there is no objection.","jpountz","NULL","1","decision","0","0","0","0","1"
"857","857","23241","2297","Commit 1556483 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1556483 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.","Commit 1556483 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1556483 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"858","858","23242","2297","Commit 1556484 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1556484 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.","Commit 1556484 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1556484 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"859","859","23243","2297","Commit 1556485 from Adrien Grand in branch 'dev/branches/lucene_solr_4_6'
[ https://svn.apache.org/r1556485 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.","Commit 1556485 from Adrien Grand in branch 'dev/branches/lucene_solr_4_6'
[ https://svn.apache.org/r1556485 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"860","860","23244","2297","While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone. I took the liberty to fix it before committing but please let me know if this looks wrong to you.
Committed, thanks!","While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone.","jpountz","NULL","1","issue","1","0","0","0","0"
"861","861","23244","2297","While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone. I took the liberty to fix it before committing but please let me know if this looks wrong to you.
Committed, thanks!","I took the liberty to fix it before committing but please let me know if this looks wrong to you.","jpountz","NULL","1","decision","0","0","0","0","1"
"862","862","23244","2297","While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone. I took the liberty to fix it before committing but please let me know if this looks wrong to you.
Committed, thanks!","Committed, thanks!","jpountz","NULL","1","decision","0","0","0","0","1"
"863","863","23245","2297","Wonderful!  Thanks.","Wonderful!","nik9000@gmail.com","NULL","1","pro","0","0","1","0","0"
"864","864","23245","2297","Wonderful!  Thanks.","Thanks.","nik9000@gmail.com","NULL","0",NULL,"0","0","0","0","0"
"865","865","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","First patch, 17 Oct 2013, quite rough, one nocommit.","paul.elschot@xs4all.nl","NULL","1","alternative, con","0","1","0","1","0"
"866","866","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"867","867","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","Otherwise it uses WAH8DocIdSet, the current behaviour.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"868","868","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","Does this choice make good use of the benchmark results?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"869","869","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"870","870","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","(Perhaps a similar method should be added to EliasFanoDocIdSet.)","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"871","871","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","In other cases, the patch falls back to WAH8DocIdSet.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"872","872","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"873","873","23914","2357","First patch, 17 Oct 2013, quite rough, one nocommit.
The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.
Otherwise it uses WAH8DocIdSet, the current behaviour.
Does this choice make good use of the benchmark results?
To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality. (Perhaps a similar method should be added to EliasFanoDocIdSet.)
In other cases, the patch falls back to WAH8DocIdSet.
I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.
The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.
","The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"874","874","23915","2357","I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance. In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance. We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number. Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?","I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance.","jpountz","NULL","1","pro, con","0","0","1","1","0"
"875","875","23915","2357","I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance. In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance. We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number. Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?","In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance.","jpountz","NULL","1","alternative, con","0","1","0","1","0"
"876","876","23915","2357","I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance. In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance. We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number. Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?","We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number.","jpountz","NULL","1","alternative, pro, con","0","1","1","1","0"
"877","877","23915","2357","I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance. In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance. We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number. Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?","Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?","jpountz","NULL","1","alternative","0","1","0","0","0"
"878","878","23916","2357","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e. it might be higher than the actual number of documents.
The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.
DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"879","879","23916","2357","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e. it might be higher than the actual number of documents.
The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.
DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.","it might be higher than the actual number of documents.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"880","880","23916","2357","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e. it might be higher than the actual number of documents.
The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.
DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.","The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"881","881","23916","2357","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e. it might be higher than the actual number of documents.
The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.
DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"882","882","23916","2357","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e. it might be higher than the actual number of documents.
The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.
DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.","Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"883","883","23916","2357","relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e. it might be higher than the actual number of documents.
The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.
DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.","That is not immediately clear from the benchmark results, but it could be so.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"884","884","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.","jpountz","NULL","1","con","0","0","0","1","0"
"885","885","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.","jpountz","NULL","1","pro","0","0","1","0","0"
"886","886","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?","jpountz","NULL","0",NULL,"0","0","0","0","0"
"887","887","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","That is not immediately clear from the benchmark results, but it could be so.","jpountz","NULL","0",NULL,"0","0","0","0","0"
"888","888","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg.","jpountz","NULL","1","pro","0","0","1","0","0"
"889","889","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS.","jpountz","NULL","1","pro","0","0","1","0","0"
"890","890","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set.","jpountz","NULL","1","alternative","0","1","0","0","0"
"891","891","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve...","jpountz","NULL","1","issue","1","0","0","0","0"
"892","892","23917","2357","DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.
I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.
Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?
That is not immediately clear from the benchmark results, but it could be so.
My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg. QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS. Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve... I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.","jpountz","NULL","1","issue","1","0","0","0","0"
"893","893","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","... the closest thing we have to a cardinality() which is available for every DocIdSet.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"894","894","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"895","895","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"896","896","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"897","897","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","There is also a TermFilter in the queries module.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"898","898","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","I have not looked at the code yet.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"899","899","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","Is it necessary to move that to core so a check for that can be used here, too?","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"900","900","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"901","901","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"902","902","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"903","903","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"904","904","23918","2357","... the closest thing we have to a cardinality() which is available for every DocIdSet.
For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.
(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)
Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
There is also a TermFilter in the queries module. I have not looked at the code yet.
Is it necessary to move that to core so a check for that can be used here, too?
... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???
For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.
I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set. 
Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.
In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.
","In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"905","905","23919","2357","Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
I don't like much having support for specific filters based on instanceof calls. I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set. And I would go for option 2 since option 1 is likely going to be slower?","Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?","jpountz","NULL","1","alternative","0","1","0","0","0"
"906","906","23919","2357","Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
I don't like much having support for specific filters based on instanceof calls. I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set. And I would go for option 2 since option 1 is likely going to be slower?","I don't like much having support for specific filters based on instanceof calls.","jpountz","NULL","1","con","0","0","0","1","0"
"907","907","23919","2357","Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
I don't like much having support for specific filters based on instanceof calls. I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set. And I would go for option 2 since option 1 is likely going to be slower?","I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set.","jpountz","NULL","1","alternative","0","1","0","0","0"
"908","908","23919","2357","Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?
I don't like much having support for specific filters based on instanceof calls. I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set. And I would go for option 2 since option 1 is likely going to be slower?","And I would go for option 2 since option 1 is likely going to be slower?","jpountz","NULL","1","pro, con","0","0","1","1","0"
"909","909","23920","2357","I don't like much having support for specific filters based on instanceof calls.
Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.
This method should return the actual number of doc ids in the set when its return value >= 0.
The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
I'll try and make another patch for this.","I don't like much having support for specific filters based on instanceof calls.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"910","910","23920","2357","I don't like much having support for specific filters based on instanceof calls.
Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.
This method should return the actual number of doc ids in the set when its return value >= 0.
The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
I'll try and make another patch for this.","Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"911","911","23920","2357","I don't like much having support for specific filters based on instanceof calls.
Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.
This method should return the actual number of doc ids in the set when its return value >= 0.
The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
I'll try and make another patch for this.","This method should return the actual number of doc ids in the set when its return value >= 0.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"912","912","23920","2357","I don't like much having support for specific filters based on instanceof calls.
Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.
This method should return the actual number of doc ids in the set when its return value >= 0.
The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
I'll try and make another patch for this.","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"913","913","23920","2357","I don't like much having support for specific filters based on instanceof calls.
Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.
This method should return the actual number of doc ids in the set when its return value >= 0.
The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
I'll try and make another patch for this.","I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.","paul.elschot@xs4all.nl","NULL","1","pro, con","0","0","1","1","0"
"914","914","23920","2357","I don't like much having support for specific filters based on instanceof calls.
Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.
This method should return the actual number of doc ids in the set when its return value >= 0.
The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
I'll try and make another patch for this.","I'll try and make another patch for this.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"915","915","23921","2357","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
Good point, I fixed it.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
To me this feels like a big change compared to what it gives us. I would prefer having another copy rather than adding this method.","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"916","916","23921","2357","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
Good point, I fixed it.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
To me this feels like a big change compared to what it gives us. I would prefer having another copy rather than adding this method.","Good point, I fixed it.","jpountz","NULL","1","pro, decision","0","0","1","0","1"
"917","917","23921","2357","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
Good point, I fixed it.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
To me this feels like a big change compared to what it gives us. I would prefer having another copy rather than adding this method.","I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.","jpountz","NULL","1","pro, con","0","0","1","1","0"
"918","918","23921","2357","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
Good point, I fixed it.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
To me this feels like a big change compared to what it gives us. I would prefer having another copy rather than adding this method.","To me this feels like a big change compared to what it gives us.","jpountz","NULL","1","con","0","0","0","1","0"
"919","919","23921","2357","The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.
Good point, I fixed it.
I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.
To me this feels like a big change compared to what it gives us. I would prefer having another copy rather than adding this method.","I would prefer having another copy rather than adding this method.","jpountz","NULL","1","alternative, pro, con","0","1","1","1","0"
"920","920","23922","2357","It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.
There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.
This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.
For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?
The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.
","It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.","paul.elschot@xs4all.nl","NULL","1","pro, con","0","0","1","1","0"
"921","921","23922","2357","It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.
There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.
This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.
For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?
The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.
","There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"922","922","23922","2357","It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.
There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.
This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.
For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?
The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.
","This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.","paul.elschot@xs4all.nl","NULL","1","pro, con","0","0","1","1","0"
"923","923","23922","2357","It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.
There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.
This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.
For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?
The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.
","For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"924","924","23922","2357","It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.
There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.
This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.
For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?
The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.
","The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.","paul.elschot@xs4all.nl","NULL","1","alternative, pro, con","0","1","1","1","0"
"925","925","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"926","926","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","The next() will be used once through the set to build the final docidset to be cached.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"927","927","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index).","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"928","928","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","This can be done because the maximum doc id value is known.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"929","929","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"930","930","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","The existing PackedInts code should be a nice fit for this.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"931","931","23923","2357","Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance. The next() will be used once through the set to build the final docidset to be cached.
However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index). This can be done because the maximum doc id value is known. While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high. The existing PackedInts code should be a nice fit for this.
Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.","paul.elschot@xs4all.nl","NULL","1","alternative, pro, con","0","1","1","1","0"
"932","932","23924","2357","After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.
I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.
This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values. Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative. 
(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)
","After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"933","933","23924","2357","After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.
I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.
This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values. Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative. 
(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)
","I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"934","934","23924","2357","After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.
I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.
This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values. Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative. 
(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)
","This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"935","935","23924","2357","After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.
I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.
This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values. Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative. 
(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)
","Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"936","936","23924","2357","After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.
I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.
This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values. Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative. 
(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)
","(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"937","937","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"938","938","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.","paul.elschot@xs4all.nl","NULL","1","alternative, pro, con","0","1","1","1","0"
"939","939","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","It works by growing by a factor of 2 as necessary, and reencoding completely.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"940","940","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"941","941","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"942","942","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","I ran some tests on this, it appears to work fine here.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"943","943","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.","paul.elschot@xs4all.nl","NULL","1","decision","0","0","0","0","1"
"944","944","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"945","945","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","For the rest see my comments on the patch of 18 Oct.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"946","946","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"947","947","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","The advance/next performance is only slightly less, again as expected.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"948","948","23925","2357","About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.
This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.
It works by growing by a factor of 2 as necessary, and reencoding completely.
This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.
The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.
I ran some tests on this, it appears to work fine here.
A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.
CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.
For the rest see my comments on the patch of 18 Oct.
I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
The advance/next performance is only slightly less, again as expected.
I could not measure build times, I expect it to just about double for the upperbound only constructor.","I could not measure build times, I expect it to just about double for the upperbound only constructor.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"949","949","23926","2357","I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?","I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.","jpountz","NULL","1","con","0","0","0","1","0"
"950","950","23926","2357","I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding?","jpountz","NULL","1","alternative","0","1","0","0","0"
"951","951","23926","2357","I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?","This is what the other (wah8/pfor) sets do.","jpountz","NULL","1","pro","0","0","1","0","0"
"952","952","23926","2357","I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.
Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?","and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?","jpountz","NULL","1","alternative, pro","0","1","1","0","0"
"953","953","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"954","954","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","This is what the other (wah8/pfor) sets do.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"955","955","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"956","956","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","That would certainly be possible.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"957","957","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","I'll have a look at wah8/pfor for this.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"958","958","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"959","959","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"960","960","23927","2357","Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding? This is what the other (wah8/pfor) sets do.
I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?
That would certainly be possible. I'll have a look at wah8/pfor for this.
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?
The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.
I'm working on this now, another patch is slowly on its way.","I'm working on this now, another patch is slowly on its way.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"961","961","23928","2357","About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.
Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.
Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.
Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.
For the rest see my remarks about the patch of 20 Oct.","About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"962","962","23928","2357","About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.
Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.
Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.
Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.
For the rest see my remarks about the patch of 20 Oct.","Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"963","963","23928","2357","About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.
Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.
Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.
Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.
For the rest see my remarks about the patch of 20 Oct.","Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"964","964","23928","2357","About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.
Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.
Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.
Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.
For the rest see my remarks about the patch of 20 Oct.","Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"965","965","23928","2357","About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.
Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.
Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.
Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.
For the rest see my remarks about the patch of 20 Oct.","For the rest see my remarks about the patch of 20 Oct.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"966","966","23929","2357","On the patch of 22 Oct:
The previous patch contains a bug in the freeze() code, it allocates more than an FBS size to one of the encoding arrays.
This should fix it.","On the patch of 22 Oct:
The previous patch contains a bug in the freeze() code, it allocates more than an FBS size to one of the encoding arrays.","paul.elschot@xs4all.nl","NULL","1","issue","1","0","0","0","0"
"967","967","23929","2357","On the patch of 22 Oct:
The previous patch contains a bug in the freeze() code, it allocates more than an FBS size to one of the encoding arrays.
This should fix it.","This should fix it.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"968","968","23930","2357","I've done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor. No surprising results but I'd like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.
Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?
The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor? 
","I've done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"969","969","23930","2357","I've done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor. No surprising results but I'd like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.
Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?
The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor? 
","No surprising results but I'd like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"970","970","23930","2357","I've done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor. No surprising results but I'd like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.
Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?
The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor? 
","Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"971","971","23930","2357","I've done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor. No surprising results but I'd like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.
Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?
The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor? 
","The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor?","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"972","972","23931","2357","I opened a github pull request for the latest patch:
https://github.com/apache/lucene-solr/pull/19","I opened a github pull request for the latest patch:
https://github.com/apache/lucene-solr/pull/19","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"973","973","23932","2357","Since LUCENE-5983 CachingWrapperFilter uses RoaringDocIdSet.
Are there any advantages for EliasFanoDocIdSet left for use there?","Since LUCENE-5983 CachingWrapperFilter uses RoaringDocIdSet.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"974","974","23932","2357","Since LUCENE-5983 CachingWrapperFilter uses RoaringDocIdSet.
Are there any advantages for EliasFanoDocIdSet left for use there?","Are there any advantages for EliasFanoDocIdSet left for use there?","paul.elschot@xs4all.nl","NULL","1","issue","1","0","0","0","0"
"975","975","23933","2357","Although the elias-fano set is smaller in the sparse case, it's true that RoaringDocIdSet tends to be faster to build and to iterate on. So overall I think the RoaringDocIdSet has a better trade-off indeed.","Although the elias-fano set is smaller in the sparse case, it's true that RoaringDocIdSet tends to be faster to build and to iterate on.","jpountz","NULL","1","pro","0","0","1","0","0"
"976","976","23933","2357","Although the elias-fano set is smaller in the sparse case, it's true that RoaringDocIdSet tends to be faster to build and to iterate on. So overall I think the RoaringDocIdSet has a better trade-off indeed.","So overall I think the RoaringDocIdSet has a better trade-off indeed.","jpountz","NULL","1","pro","0","0","1","0","0"
"977","977","23934","2357","Github user PaulElschot closed the pull request at:
 https://github.com/apache/lucene-solr/pull/19","Github user PaulElschot closed the pull request at:
 https://github.com/apache/lucene-solr/pull/19","githubbot","NULL","0",NULL,"0","0","0","0","0"
"978","978","25239","2460","Patch; it turned out to be easier than I expected: I just tapped into the existing logic that ShingleFilter has for handling holes between tokens.","Patch; it turned out to be easier than I expected: I just tapped into the existing logic that ShingleFilter has for handling holes between tokens.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"979","979","25240","2460","+1, patch looks good.
+1 to your suggestion about ShingleFilterTest.TestTokenStream:
// TODO: merge w/ CannedTokenStream?","+1, patch looks good.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"980","980","25240","2460","+1, patch looks good.
+1 to your suggestion about ShingleFilterTest.TestTokenStream:
// TODO: merge w/ CannedTokenStream?","+1 to your suggestion about ShingleFilterTest.TestTokenStream:
// TODO: merge w/ CannedTokenStream?","steve_rowe","NULL","1","pro","0","0","1","0","0"
"981","981","25241","2460","Thanks Steve!
Here's a new patch w/ that TODO done ... I think it's ready.","Thanks Steve!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"982","982","25241","2460","Thanks Steve!
Here's a new patch w/ that TODO done ... I think it's ready.","Here's a new patch w/ that TODO done ...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"983","983","25241","2460","Thanks Steve!
Here's a new patch w/ that TODO done ... I think it's ready.","I think it's ready.","mikemccand","NULL","1","pro","0","0","1","0","0"
"984","984","25242","2460","Commit 1524117 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1524117 ]
LUCENE-5180: ShingleFilter creates shingles from trailing holes","Commit 1524117 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1524117 ]
LUCENE-5180: ShingleFilter creates shingles from trailing holes","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"985","985","25243","2460","Commit 1524120 from Michael McCandless in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1524120 ]
LUCENE-5180: ShingleFilter creates shingles from trailing holes","Commit 1524120 from Michael McCandless in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1524120 ]
LUCENE-5180: ShingleFilter creates shingles from trailing holes","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"986","986","25244","2460","Commit 1524122 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1524122 ]
LUCENE-5180: move CHANGES entry","Commit 1524122 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1524122 ]
LUCENE-5180: move CHANGES entry","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"987","987","25400","2471","java.lang.IndexOutOfBoundsException: start 9999, end 10004, s.length() 10000
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:453)
	at java.lang.StringBuilder.append(StringBuilder.java:179)
	at org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.append(DefaultPassageFormatter.java:135)
	at org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.format(DefaultPassageFormatter.java:79)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightField(PostingsHighlighter.java:435)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:353)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:268)","java.lang.IndexOutOfBoundsException: start 9999, end 10004, s.length() 10000
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:453)
	at java.lang.StringBuilder.append(StringBuilder.java:179)
	at org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.append(DefaultPassageFormatter.java:135)
	at org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.format(DefaultPassageFormatter.java:79)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightField(PostingsHighlighter.java:435)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:353)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:268)","mamoabeng","NULL","0",NULL,"0","0","0","0","0"
"988","988","25401","2471","Can you show a real usecase for a document matching beyond content.length()? Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g. maybe a combination of TokenFilters may cause that?
But if e.g. the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...","Can you show a real usecase for a document matching beyond content.length()?","shaie","NULL","1","alternative","0","1","0","0","0"
"989","989","25401","2471","Can you show a real usecase for a document matching beyond content.length()? Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g. maybe a combination of TokenFilters may cause that?
But if e.g. the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...","Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g.","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"990","990","25401","2471","Can you show a real usecase for a document matching beyond content.length()? Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g. maybe a combination of TokenFilters may cause that?
But if e.g. the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...","maybe a combination of TokenFilters may cause that?","shaie","NULL","1","issue","1","0","0","0","0"
"991","991","25401","2471","Can you show a real usecase for a document matching beyond content.length()? Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g. maybe a combination of TokenFilters may cause that?
But if e.g. the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...","But if e.g.","shaie","NULL","0",NULL,"0","0","0","0","0"
"992","992","25401","2471","Can you show a real usecase for a document matching beyond content.length()? Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g. maybe a combination of TokenFilters may cause that?
But if e.g. the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...","the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...","shaie","NULL","1","issue","1","0","0","0","0"
"993","993","25402","2471","Please find attached another test case. It is sort of bad luck to run into this in a real use case but it actually happened to me.  ","Please find attached another test case.","mamoabeng","NULL","1","alternative","0","1","0","0","0"
"994","994","25402","2471","Please find attached another test case. It is sort of bad luck to run into this in a real use case but it actually happened to me.  ","It is sort of bad luck to run into this in a real use case but it actually happened to me.","mamoabeng","NULL","1","issue","1","0","0","0","0"
"995","995","25403","2471","Interesting. FYI, I will not be available in the next 2 weeks, and haven't reproduced it yet. If no one assigns himself to the issue, I will when I'm back.","Interesting.","shaie","NULL","1","pro","0","0","1","0","0"
"996","996","25403","2471","Interesting. FYI, I will not be available in the next 2 weeks, and haven't reproduced it yet. If no one assigns himself to the issue, I will when I'm back.","FYI, I will not be available in the next 2 weeks, and haven't reproduced it yet.","shaie","NULL","0",NULL,"0","0","0","0","0"
"997","997","25403","2471","Interesting. FYI, I will not be available in the next 2 weeks, and haven't reproduced it yet. If no one assigns himself to the issue, I will when I'm back.","If no one assigns himself to the issue, I will when I'm back.","shaie","NULL","0",NULL,"0","0","0","0","0"
"998","998","25404","2471","I have reproduced it with Manuel's test","I have reproduced it with Manuel's test","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"999","999","25405","2471","Attached is just a combined patch of Manuel's 2 patches.
There is definitely bug(s) here.
As far as the fix, to me the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases.","Attached is just a combined patch of Manuel's 2 patches.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1000","1000","25405","2471","Attached is just a combined patch of Manuel's 2 patches.
There is definitely bug(s) here.
As far as the fix, to me the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases.","There is definitely bug(s) here.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1001","1001","25405","2471","Attached is just a combined patch of Manuel's 2 patches.
There is definitely bug(s) here.
As far as the fix, to me the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases.","As far as the fix, to me the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1002","1002","25407","2471","OK here's a patch. the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms). 
So we cannot safely terminate when end >= limit (only start >= limit), but we don't have to confuse the formatter with the cases of terms that 'span' the limit.","OK here's a patch.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1003","1003","25407","2471","OK here's a patch. the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms). 
So we cannot safely terminate when end >= limit (only start >= limit), but we don't have to confuse the formatter with the cases of terms that 'span' the limit.","the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms).","rcmuir","NULL","1","issue","1","0","0","0","0"
"1004","1004","25407","2471","OK here's a patch. the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms). 
So we cannot safely terminate when end >= limit (only start >= limit), but we don't have to confuse the formatter with the cases of terms that 'span' the limit.","So we cannot safely terminate when end >= limit (only start >= limit), but we don't have to confuse the formatter with the cases of terms that 'span' the limit.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1005","1005","25408","2471","Hmm so this means we may pick a truncated passage to present?  I suppose it's unlikely to score well ... just seems bad though.
Wait, couldn't we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?","Hmm so this means we may pick a truncated passage to present?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1006","1006","25408","2471","Hmm so this means we may pick a truncated passage to present?  I suppose it's unlikely to score well ... just seems bad though.
Wait, couldn't we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?","I suppose it's unlikely to score well ... just seems bad though.","mikemccand","NULL","1","con","0","0","0","1","0"
"1007","1007","25408","2471","Hmm so this means we may pick a truncated passage to present?  I suppose it's unlikely to score well ... just seems bad though.
Wait, couldn't we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?","Wait, couldn't we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1008","1008","25410","2471","Improved patch, thank you Mike ","Improved patch, thank you Mike","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1009","1009","25413","2471","OK let's not try to address that on this issue ... I'm not even sure it needs fixing.  It ought to be rare-ish that a truncated passage is selected.","OK let's not try to address that on this issue ...","mikemccand","NULL","1","decision","0","0","0","0","1"
"1010","1010","25413","2471","OK let's not try to address that on this issue ... I'm not even sure it needs fixing.  It ought to be rare-ish that a truncated passage is selected.","I'm not even sure it needs fixing.","mikemccand","NULL","1","con","0","0","0","1","0"
"1011","1011","25413","2471","OK let's not try to address that on this issue ... I'm not even sure it needs fixing.  It ought to be rare-ish that a truncated passage is selected.","It ought to be rare-ish that a truncated passage is selected.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1012","1012","25414","2471","There was a bug in my patch: I added another unit test for this!
I think its ready.","There was a bug in my patch: I added another unit test for this!","rcmuir","NULL","1","issue, alternative","1","1","0","0","0"
"1013","1013","25414","2471","There was a bug in my patch: I added another unit test for this!
I think its ready.","I think its ready.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1014","1014","25415","2471","+1
Tricky!","+1
Tricky!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1015","1015","25416","2471","Commit 1513207 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1513207 ]
LUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException","Commit 1513207 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1513207 ]
LUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"1016","1016","25417","2471","Commit 1513231 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1513231 ]
LUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException","Commit 1513231 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1513231 ]
LUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"1017","1017","25418","2471","Thank you Manuel!","Thank you Manuel!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1018","1018","25419","2471","Thank you for the quick help!","Thank you for the quick help!","mamoabeng","NULL","0",NULL,"0","0","0","0","0"
"1019","1019","25420","2471","I just found another problem here: If we have both, matches that do and matches that don't span the content boundary the formatter is asked to highlight the spanning match.
Please find attached additional tests and a possible fix for this. ","I just found another problem here: If we have both, matches that do and matches that don't span the content boundary the formatter is asked to highlight the spanning match.","mamoabeng","NULL","1","issue","1","0","0","0","0"
"1020","1020","25420","2471","I just found another problem here: If we have both, matches that do and matches that don't span the content boundary the formatter is asked to highlight the spanning match.
Please find attached additional tests and a possible fix for this. ","Please find attached additional tests and a possible fix for this.","mamoabeng","NULL","1","alternative","0","1","0","0","0"
"1021","1021","25421","2471","Hi Manuel: thank you! Another bug, or a bug in my fix to the other bug 
I'll investigate deeper in a bit.","Hi Manuel: thank you!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1022","1022","25421","2471","Hi Manuel: thank you! Another bug, or a bug in my fix to the other bug 
I'll investigate deeper in a bit.","Another bug, or a bug in my fix to the other bug 
I'll investigate deeper in a bit.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1023","1023","25422","2471","Manuel: your fix is correct, thank you.
To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn't committed).
We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).
But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).","Manuel: your fix is correct, thank you.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1024","1024","25422","2471","Manuel: your fix is correct, thank you.
To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn't committed).
We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).
But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).","To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn't committed).","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1025","1025","25422","2471","Manuel: your fix is correct, thank you.
To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn't committed).
We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).
But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).","We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1026","1026","25422","2471","Manuel: your fix is correct, thank you.
To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn't committed).
We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).
But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).","But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).","rcmuir","NULL","1","decision","0","0","0","0","1"
"1027","1027","25423","2471","Commit 1514367 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1514367 ]
LUCENE-5166: also fix and test this case where tf > 1 within the passage for a term","Commit 1514367 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1514367 ]
LUCENE-5166: also fix and test this case where tf > 1 within the passage for a term","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"1028","1028","25424","2471","Commit 1514379 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1514379 ]
LUCENE-5166: also fix and test this case where tf > 1 within the passage for a term","Commit 1514379 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1514379 ]
LUCENE-5166: also fix and test this case where tf > 1 within the passage for a term","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"1029","1029","25425","2471","Thank you again!","Thank you again!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1030","1030","25426","2471","4.5 release -> bulk close","4.5 release -> bulk close","jpountz","NULL","1","decision","0","0","0","0","1"
"1031","1031","25427","2471","Commit 1594464 from Robert Muir in branch 'dev/branches/lucene5666'
[ https://svn.apache.org/r1594464 ]
LUCENE-5166: clear most nocommits, move ord/rord to solr (and speed them up), nuke old purging stuff","Commit 1594464 from Robert Muir in branch 'dev/branches/lucene5666'
[ https://svn.apache.org/r1594464 ]
LUCENE-5166: clear most nocommits, move ord/rord to solr (and speed them up), nuke old purging stuff","jira-bot","NULL","0",NULL,"0","0","0","0","0"
"1032","1032","26485","2590","First draft of patch attached.  Let me know how this looks. Thank you.","First draft of patch attached.","tallison@mitre.org","NULL","1","alternative","0","1","0","0","0"
"1033","1033","26485","2590","First draft of patch attached.  Let me know how this looks. Thank you.","Let me know how this looks.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"1034","1034","26485","2590","First draft of patch attached.  Let me know how this looks. Thank you.","Thank you.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"1035","1035","26486","2590","Thanks Tim!  This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.
And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.  Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?
For my sanity ... where exactly was the bug in the original code?","Thanks Tim!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1036","1036","26486","2590","Thanks Tim!  This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.
And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.  Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?
For my sanity ... where exactly was the bug in the original code?","This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1037","1037","26486","2590","Thanks Tim!  This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.
And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.  Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?
For my sanity ... where exactly was the bug in the original code?","And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1038","1038","26486","2590","Thanks Tim!  This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.
And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.  Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?
For my sanity ... where exactly was the bug in the original code?","Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1039","1039","26486","2590","Thanks Tim!  This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.
And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.  Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?
For my sanity ... where exactly was the bug in the original code?","For my sanity ... where exactly was the bug in the original code?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1040","1040","26487","2590","Thank you for your quick response!
I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.  Let me know if I'm missing something.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.  So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.  In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","Thank you for your quick response!","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"1041","1041","26487","2590","Thank you for your quick response!
I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.  Let me know if I'm missing something.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.  So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.  In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.","tallison@mitre.org","NULL","1","alternative, pro","0","1","1","0","0"
"1042","1042","26487","2590","Thank you for your quick response!
I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.  Let me know if I'm missing something.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.  So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.  In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","Let me know if I'm missing something.","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"1043","1043","26487","2590","Thank you for your quick response!
I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.  Let me know if I'm missing something.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.  So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.  In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.","tallison@mitre.org","NULL","1","issue","1","0","0","0","0"
"1044","1044","26487","2590","Thank you for your quick response!
I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.  Let me know if I'm missing something.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.  So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.  In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.","tallison@mitre.org","NULL","1","issue","1","0","0","0","0"
"1045","1045","26487","2590","Thank you for your quick response!
I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.  Let me know if I'm missing something.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.  So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.  In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","tallison@mitre.org","NULL","1","issue","1","0","0","0","0"
"1046","1046","26488","2590","Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!","Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1047","1047","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1048","1048","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","Let me know if I'm missing something.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1049","1049","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","Ahh, you're right ...","mikemccand","NULL","1","pro","0","0","1","0","0"
"1050","1050","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","I missed that.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1051","1051","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","OK.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1052","1052","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.","mikemccand","NULL","1","issue","1","0","0","0","0"
"1053","1053","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.","mikemccand","NULL","1","issue","1","0","0","0","0"
"1054","1054","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.","mikemccand","NULL","1","issue","1","0","0","0","0"
"1055","1055","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1056","1056","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","Now I understand the bug ... thanks.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1057","1057","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"1058","1058","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?","mikemccand","NULL","1","con","0","0","0","1","0"
"1059","1059","26489","2590","I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost. Let me know if I'm missing something.
Ahh, you're right ... I missed that.  OK.
The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f. So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value. In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.
Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.  Now I understand the bug ... thanks.
Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!
But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?  (The fixes are to LinearFuzzyTermsEnum).","(The fixes are to LinearFuzzyTermsEnum).","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1060","1060","26490","2590","Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).  The classic QueryParser silently converts distances > 2 to 2.
If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.  
My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.  In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.","Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).","tallison@mitre.org","NULL","1","pro, con","0","0","1","1","0"
"1061","1061","26490","2590","Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).  The classic QueryParser silently converts distances > 2 to 2.
If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.  
My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.  In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.","The classic QueryParser silently converts distances > 2 to 2.","tallison@mitre.org","NULL","1","con","0","0","0","1","0"
"1062","1062","26490","2590","Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).  The classic QueryParser silently converts distances > 2 to 2.
If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.  
My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.  In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.","If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.","tallison@mitre.org","NULL","1","issue","1","0","0","0","0"
"1063","1063","26490","2590","Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).  The classic QueryParser silently converts distances > 2 to 2.
If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.  
My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.  In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.","My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.","tallison@mitre.org","NULL","1","alternative, pro","0","1","1","0","0"
"1064","1064","26490","2590","Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).  The classic QueryParser silently converts distances > 2 to 2.
If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.  
My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.  In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.","In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.","tallison@mitre.org","NULL","1","pro, con","0","0","1","1","0"
"1065","1065","26491","2590","Updated patch.  Added short circuits to avoid calculating similarity when not necessary.  Corrected term.length to text.length in calcSimilarity call.  Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.","Updated patch.","tallison@mitre.org","NULL","1","decision","0","0","0","0","1"
"1066","1066","26491","2590","Updated patch.  Added short circuits to avoid calculating similarity when not necessary.  Corrected term.length to text.length in calcSimilarity call.  Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.","Added short circuits to avoid calculating similarity when not necessary.","tallison@mitre.org","NULL","1","alternative, pro","0","1","1","0","0"
"1067","1067","26491","2590","Updated patch.  Added short circuits to avoid calculating similarity when not necessary.  Corrected term.length to text.length in calcSimilarity call.  Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.","Corrected term.length to text.length in calcSimilarity call.","tallison@mitre.org","NULL","1","alternative","0","1","0","0","0"
"1068","1068","26491","2590","Updated patch.  Added short circuits to avoid calculating similarity when not necessary.  Corrected term.length to text.length in calcSimilarity call.  Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.","Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.","tallison@mitre.org","NULL","1","alternative","0","1","0","0","0"
"1069","1069","26492","2590","Thanks Tim, new patch looks great!","Thanks Tim, new patch looks great!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1070","1070","26493","2590","Thanks Tim!","Thanks Tim!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1071","1071","26494","2590","Mike,
  Thank you for your feedback and quick response!
","Mike,
  Thank you for your feedback and quick response!","tallison@mitre.org","NULL","0",NULL,"0","0","0","0","0"
"1072","1072","26495","2590","Bulk close resolved 4.4 issues","Bulk close resolved 4.4 issues","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1073","1073","26968","2638","patch.","patch.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1074","1074","26969","2638","looks good!","looks good!","shaie","NULL","1","pro","0","0","1","0","0"
"1075","1075","26970","2638","New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it's not being used by an IW ... I think it's ready.","New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it's not being used by an IW ...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1076","1076","26970","2638","New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it's not being used by an IW ... I think it's ready.","I think it's ready.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1077","1077","26971","2638","+1!","+1!","shaie","NULL","1","pro","0","0","1","0","0"
"1078","1078","26972","2638","[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478726
LUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk","[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478726
LUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1079","1079","26973","2638","[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478730
LUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk","[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478730
LUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1080","1080","26974","2638","[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478854
LUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit","[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478854
LUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1081","1081","26975","2638","[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478855
LUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit","[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478855
LUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1082","1082","26976","2638","[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1479394
LUCENE-4976: add missing sync / delete old save files","[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1479394
LUCENE-4976: add missing sync / delete old save files","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1083","1083","26977","2638","[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1479395
LUCENE-4976: add missing sync / delete old save files","[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1479395
LUCENE-4976: add missing sync / delete old save files","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1084","1084","26978","2638","Bulk close resolved 4.4 issues","Bulk close resolved 4.4 issues","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1086","1085","27832","2710","I think that we should use a primitive iterator, e.g. facet collections have IntIterator interface. And so the method should be something like IntIterator getChildren(int ordinal)?","facet collections have IntIterator interface.","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"1087","1086","27832","2710","I think that we should use a primitive iterator, e.g. facet collections have IntIterator interface. And so the method should be something like IntIterator getChildren(int ordinal)?","And so the method should be something like IntIterator getChildren(int ordinal)?","shaie","NULL","1","alternative","0","1","0","0","0"
"1088","1087","27833","2710","I don't know much about IntIterator, but I surmise it'll do good enough.","I don't know much about IntIterator, but I surmise it'll do good enough.","crocket","NULL","1","alternative, pro","0","1","1","0","0"
"1089","1088","27834","2710","Added TaxoReader.getChildren(int ordinal) and corresponding test. I also migrated PrintTaxonomyStats to use getChildren, which removed all mentions of ParallelTaxonomyArrays from it.","Added TaxoReader.getChildren(int ordinal) and corresponding test.","shaie","NULL","1","alternative","0","1","0","0","0"
"1090","1089","27834","2710","Added TaxoReader.getChildren(int ordinal) and corresponding test. I also migrated PrintTaxonomyStats to use getChildren, which removed all mentions of ParallelTaxonomyArrays from it.","I also migrated PrintTaxonomyStats to use getChildren, which removed all mentions of ParallelTaxonomyArrays from it.","shaie","NULL","1","alternative","0","1","0","0","0"
"1091","1090","27835","2710","Shouldn't next() throw NoSuchElementException if child is already INVALID_ORDINAL?  It shouldn't ever return INVALID_ORDINAL, right?  Ie, caller screwed up and called next w/o calling hasNext first.","Shouldn't next() throw NoSuchElementException if child is already INVALID_ORDINAL?","mikemccand","NULL","1","issue","1","0","0","0","0"
"1092","1091","27835","2710","Shouldn't next() throw NoSuchElementException if child is already INVALID_ORDINAL?  It shouldn't ever return INVALID_ORDINAL, right?  Ie, caller screwed up and called next w/o calling hasNext first.","It shouldn't ever return INVALID_ORDINAL, right?","mikemccand","NULL","1","issue","1","0","0","0","0"
"1093","1092","27835","2710","Shouldn't next() throw NoSuchElementException if child is already INVALID_ORDINAL?  It shouldn't ever return INVALID_ORDINAL, right?  Ie, caller screwed up and called next w/o calling hasNext first.","Ie, caller screwed up and called next w/o calling hasNext first.","mikemccand","NULL","1","issue","1","0","0","0","0"
"1094","1093","27836","2710","I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow. Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL. I wished that we had a simple IntIterator interface with only next() for this case...
I don't mind throwing it though. What do you think?","I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow.","shaie","NULL","1","pro","0","0","1","0","0"
"1095","1094","27836","2710","I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow. Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL. I wished that we had a simple IntIterator interface with only next() for this case...
I don't mind throwing it though. What do you think?","Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL.","shaie","NULL","1","pro","0","0","1","0","0"
"1096","1095","27836","2710","I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow. Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL. I wished that we had a simple IntIterator interface with only next() for this case...
I don't mind throwing it though. What do you think?","I wished that we had a simple IntIterator interface with only next() for this case...","shaie","NULL","1","alternative","0","1","0","0","0"
"1097","1096","27836","2710","I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow. Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL. I wished that we had a simple IntIterator interface with only next() for this case...
I don't mind throwing it though. What do you think?","I don't mind throwing it though.","shaie","NULL","1","pro","0","0","1","0","0"
"1098","1097","27836","2710","I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow. Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL. I wished that we had a simple IntIterator interface with only next() for this case...
I don't mind throwing it though. What do you think?","What do you think?","shaie","NULL","0",NULL,"0","0","0","0","0"
"1099","1098","27837","2710","Or we could return a not-Java-iterator, that just has int next() that returns INVALID_ORDINAL when it's done...","Or we could return a not-Java-iterator, that just has int next() that returns INVALID_ORDINAL when it's done...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1100","1099","27838","2710","Woops, our comments crossed...
I wished that we had a simple IntIterator interface with only next() for this case...
+1, I think that's best.","Woops, our comments crossed...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1101","1100","27838","2710","Woops, our comments crossed...
I wished that we had a simple IntIterator interface with only next() for this case...
+1, I think that's best.","I wished that we had a simple IntIterator interface with only next() for this case...
+1, I think that's best.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1102","1101","27839","2710","I wanted to avoid introducing another class (facet collections already use this primitive IntIterator), but maybe a ChildrenIterator with next() is simplest. I'll look into it.","I wanted to avoid introducing another class (facet collections already use this primitive IntIterator), but maybe a ChildrenIterator with next() is simplest.","shaie","NULL","1","alternative, pro, con","0","1","1","1","0"
"1103","1102","27839","2710","I wanted to avoid introducing another class (facet collections already use this primitive IntIterator), but maybe a ChildrenIterator with next() is simplest. I'll look into it.","I'll look into it.","shaie","NULL","0",NULL,"0","0","0","0","0"
"1104","1103","27840","2710","Patch with ChildrenIterator","Patch with ChildrenIterator","shaie","NULL","1","alternative","0","1","0","0","0"
"1105","1104","27841","2710","+1, thanks Shai!","+1, thanks Shai!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1106","1105","27842","2710","[trunk commit] shaie
http://svn.apache.org/viewvc?view=revision&revision=1464730
LUCENE-4897: add a sugar API for traversing categories","[trunk commit] shaie
http://svn.apache.org/viewvc?view=revision&revision=1464730
LUCENE-4897: add a sugar API for traversing categories","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1107","1106","27843","2710","[branch_4x commit] shaie
http://svn.apache.org/viewvc?view=revision&revision=1464743
LUCENE-4897: add a sugar API for traversing categories","[branch_4x commit] shaie
http://svn.apache.org/viewvc?view=revision&revision=1464743
LUCENE-4897: add a sugar API for traversing categories","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1108","1107","27844","2710","Committed to trunk and 4x.","Committed to trunk and 4x.","shaie","NULL","1","decision","0","0","0","0","1"
"1109","1108","27845","2710","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1085","1109","27832","2710","I think that we should use a primitive iterator, e.g. facet collections have IntIterator interface. And so the method should be something like IntIterator getChildren(int ordinal)?","I think that we should use a primitive iterator, e.g.","shaie","NULL","1","alternative","0","1","0","0","0"
"1110","1110","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1111","1111","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","Some questions:

which package should this stuff be in?","romseygeek","NULL","1","issue","1","0","0","0","0"
"1112","1112","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","FieldCache is in o.a.l.search, and the reader is in o.a.l.index.","romseygeek","NULL","1","issue","1","0","0","0","0"
"1113","1113","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","there are a bunch of FieldCache-specific queries and filters.","romseygeek","NULL","1","issue","1","0","0","0","0"
"1114","1114","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","Can these just be reworked to be DV-specific instead?","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1115","1115","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1116","1116","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","this still uses the global FieldCache.","romseygeek","NULL","0",NULL,"0","0","0","0","0"
"1117","1117","27927","2720","Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.
Some questions:

which package should this stuff be in?  FieldCache is in o.a.l.search, and the reader is in o.a.l.index.
there are a bunch of FieldCache-specific queries and filters.  Can these just be reworked to be DV-specific instead?
can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?
this still uses the global FieldCache.  Should the caches be moved to the readers instead?

","Should the caches be moved to the readers instead?","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1118","1118","27929","2720","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.  But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?  That makes things a lot cleaner.  We'd need a way to purge the map, though.
Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.  Should simplify things like SortField a lot though.","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1119","1119","27929","2720","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.  But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?  That makes things a lot cleaner.  We'd need a way to purge the map, though.
Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.  Should simplify things like SortField a lot though.","But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1120","1120","27929","2720","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.  But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?  That makes things a lot cleaner.  We'd need a way to purge the map, though.
Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.  Should simplify things like SortField a lot though.","That makes things a lot cleaner.","romseygeek","NULL","1","pro","0","0","1","0","0"
"1121","1121","27929","2720","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.  But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?  That makes things a lot cleaner.  We'd need a way to purge the map, though.
Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.  Should simplify things like SortField a lot though.","We'd need a way to purge the map, though.","romseygeek","NULL","1","issue","1","0","0","0","0"
"1122","1122","27929","2720","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.  But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?  That makes things a lot cleaner.  We'd need a way to purge the map, though.
Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.  Should simplify things like SortField a lot though.","Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.","romseygeek","NULL","1","pro","0","0","1","0","0"
"1123","1123","27929","2720","But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.  But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?  That makes things a lot cleaner.  We'd need a way to purge the map, though.
Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.  Should simplify things like SortField a lot though.","Should simplify things like SortField a lot though.","romseygeek","NULL","1","pro","0","0","1","0","0"
"1124","1124","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","
I was wondering about how to do this.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1125","1125","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1126","1126","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","Why allow this?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1127","1127","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","I don't think we should do this.","rcmuir","NULL","1","con","0","0","0","1","0"
"1128","1128","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","it also prevents it from working with anything that checks fieldinfos.","rcmuir","NULL","1","con","0","0","0","1","0"
"1129","1129","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1130","1130","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","Unless you think that we'll end up having to make major changes if we don't build this in from the beginning.","rcmuir","NULL","1","con","0","0","0","1","0"
"1131","1131","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1132","1132","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1133","1133","27932","2720","
I was wondering about how to do this. We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity. 
Why allow this? I don't think we should do this. it also prevents it from working with anything that checks fieldinfos.

Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue? Unless you think that we'll end up having to make major changes if we don't build this in from the beginning. I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way. there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.
I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1134","1134","27933","2720","New patch, copying the uninvert logic from FieldCacheImpl into a new Uninverter class (I haven't worked out how to implement the NumericDocValues uninverter yet - pointers welcome), adding a cache to the UninvertingFilterReader itself and checking uninversions against a map of fieldnames to DocValuesTypes passed in as a constructor argument.","New patch, copying the uninvert logic from FieldCacheImpl into a new Uninverter class (I haven't worked out how to implement the NumericDocValues uninverter yet - pointers welcome), adding a cache to the UninvertingFilterReader itself and checking uninversions against a map of fieldnames to DocValuesTypes passed in as a constructor argument.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1135","1135","27934","2720","New patch, with a NumericDocValues uninverter.  Also a basic test.
NumericDocValues doesn't work with 32-bit values yet (the test for IntField fails); I need to somehow detect at uninversion-time how wide the indexed values are.","New patch, with a NumericDocValues uninverter.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1136","1136","27934","2720","New patch, with a NumericDocValues uninverter.  Also a basic test.
NumericDocValues doesn't work with 32-bit values yet (the test for IntField fails); I need to somehow detect at uninversion-time how wide the indexed values are.","Also a basic test.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1137","1137","27934","2720","New patch, with a NumericDocValues uninverter.  Also a basic test.
NumericDocValues doesn't work with 32-bit values yet (the test for IntField fails); I need to somehow detect at uninversion-time how wide the indexed values are.","NumericDocValues doesn't work with 32-bit values yet (the test for IntField fails); I need to somehow detect at uninversion-time how wide the indexed values are.","romseygeek","NULL","1","issue, alternative","1","1","0","0","0"
"1138","1138","27936","2720","Something that will uninvert a LongField or DoubleField into a NumericDocValues representation.","Something that will uninvert a LongField or DoubleField into a NumericDocValues representation.","romseygeek","NULL","1","issue","1","0","0","0","0"
"1139","1139","27937","2720","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.  I don't really like it, but it seems to work, so it's a start.  Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.
This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.  There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.  We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1140","1140","27937","2720","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.  I don't really like it, but it seems to work, so it's a start.  Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.
This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.  There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.  We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","I don't really like it, but it seems to work, so it's a start.","romseygeek","NULL","1","pro, con","0","0","1","1","0"
"1141","1141","27937","2720","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.  I don't really like it, but it seems to work, so it's a start.  Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.
This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.  There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.  We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1142","1142","27937","2720","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.  I don't really like it, but it seems to work, so it's a start.  Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.
This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.  There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.  We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.","romseygeek","NULL","1","con","0","0","0","1","0"
"1143","1143","27937","2720","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.  I don't really like it, but it seems to work, so it's a start.  Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.
This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.  There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.  We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.","romseygeek","NULL","1","issue","1","0","0","0","0"
"1144","1144","27937","2720","OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.  I don't really like it, but it seems to work, so it's a start.  Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.
This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.  There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.  We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.","romseygeek","NULL","1","alternative, con","0","1","0","1","0"
"1145","1145","27938","2720","Alan i didnt look closely, but could GrowableWriter be used to avoid the hack? that way you only take up space relevant to the bits you are actually using...","Alan i didnt look closely, but could GrowableWriter be used to avoid the hack?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1146","1146","27938","2720","Alan i didnt look closely, but could GrowableWriter be used to avoid the hack? that way you only take up space relevant to the bits you are actually using...","that way you only take up space relevant to the bits you are actually using...","rcmuir","NULL","1","pro","0","0","1","0","0"
"1147","1147","27939","2720","Actually I've convinced myself that checking for a NumberFormatException is a better way of doing this...","Actually I've convinced myself that checking for a NumberFormatException is a better way of doing this...","romseygeek","NULL","1","alternative, pro","0","1","1","0","0"
"1148","1148","27940","2720","could GrowableWriter be used to avoid the hack?
I don't think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints.  Unless I've got the wrong end of the stick here.","could GrowableWriter be used to avoid the hack?","romseygeek","NULL","1","alternative","0","1","0","0","0"
"1149","1149","27940","2720","could GrowableWriter be used to avoid the hack?
I don't think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints.  Unless I've got the wrong end of the stick here.","I don't think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints.","romseygeek","NULL","1","con","0","0","0","1","0"
"1150","1150","27940","2720","could GrowableWriter be used to avoid the hack?
I don't think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints.  Unless I've got the wrong end of the stick here.","Unless I've got the wrong end of the stick here.","romseygeek","NULL","0",NULL,"0","0","0","0","0"
"1151","1151","27955","2722","Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int.  I tested basic expected usage.
I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn't exist).","Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int.","dsmiley","NULL","1","alternative","0","1","0","0","0"
"1152","1152","27955","2722","Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int.  I tested basic expected usage.
I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn't exist).","I tested basic expected usage.","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"1153","1153","27955","2722","Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int.  I tested basic expected usage.
I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn't exist).","I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn't exist).","dsmiley","NULL","1","alternative, pro","0","1","1","0","0"
"1154","1154","27956","2722","I forgot to make the methods public, which I'll fix when it gets committed.","I forgot to make the methods public, which I'll fix when it gets committed.","dsmiley","NULL","1","alternative","0","1","0","0","0"
"1155","1155","27957","2722","I don't think we need to mimic java's Iterator?  Ie, it's fine to have only next()...
Also, the iterator is wrong if a rehash happens right?  Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator's remove method (like Java's HashMap)?","I don't think we need to mimic java's Iterator?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1156","1156","27957","2722","I don't think we need to mimic java's Iterator?  Ie, it's fine to have only next()...
Also, the iterator is wrong if a rehash happens right?  Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator's remove method (like Java's HashMap)?","Ie, it's fine to have only next()...
Also, the iterator is wrong if a rehash happens right?","mikemccand","NULL","1","con","0","0","0","1","0"
"1157","1157","27957","2722","I don't think we need to mimic java's Iterator?  Ie, it's fine to have only next()...
Also, the iterator is wrong if a rehash happens right?  Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator's remove method (like Java's HashMap)?","Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator's remove method (like Java's HashMap)?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1158","1158","27958","2722","Mike: sounds good.
But you know what?  Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn't really needed after all, or could be supplied with a comment so it's clear how to do it.

SentinalIntSet set = ...
for (int v : set.keys) {
  if (v == set.emptyVal)
    continue;
  //use v...
}


piece-o-cake","Mike: sounds good.","dsmiley","NULL","1","pro","0","0","1","0","0"
"1159","1159","27958","2722","Mike: sounds good.
But you know what?  Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn't really needed after all, or could be supplied with a comment so it's clear how to do it.

SentinalIntSet set = ...
for (int v : set.keys) {
  if (v == set.emptyVal)
    continue;
  //use v...
}


piece-o-cake","But you know what?","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"1160","1160","27958","2722","Mike: sounds good.
But you know what?  Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn't really needed after all, or could be supplied with a comment so it's clear how to do it.

SentinalIntSet set = ...
for (int v : set.keys) {
  if (v == set.emptyVal)
    continue;
  //use v...
}


piece-o-cake","Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn't really needed after all, or could be supplied with a comment so it's clear how to do it.","dsmiley","NULL","1","alternative, pro","0","1","1","0","0"
"1161","1161","27958","2722","Mike: sounds good.
But you know what?  Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn't really needed after all, or could be supplied with a comment so it's clear how to do it.

SentinalIntSet set = ...
for (int v : set.keys) {
  if (v == set.emptyVal)
    continue;
  //use v...
}


piece-o-cake","SentinalIntSet set = ...
for (int v : set.keys) {
  if (v == set.emptyVal)
    continue;
  //use v...
}


piece-o-cake","dsmiley","NULL","1","alternative","0","1","0","0","0"
"1162","1162","27959","2722","maybe this set iterator isn't really needed after all
+1","maybe this set iterator isn't really needed after all
+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"1163","1163","27960","2722","So instead of adding to the API I decided to enhance the documentation to make it more clear how to use this class.  (attached)
Of note I added a warning of the potential unsuitability of the lack of hashing of the key.","So instead of adding to the API I decided to enhance the documentation to make it more clear how to use this class.","dsmiley","NULL","1","alternative, decision","0","1","0","0","1"
"1164","1164","27960","2722","So instead of adding to the API I decided to enhance the documentation to make it more clear how to use this class.  (attached)
Of note I added a warning of the potential unsuitability of the lack of hashing of the key.","(attached)
Of note I added a warning of the potential unsuitability of the lack of hashing of the key.","dsmiley","NULL","1","alternative","0","1","0","0","0"
"1165","1165","27961","2722","I committed better javadocs, including sample code to iterate the values.
Closing issue as won't fix (for now). If at some point we want an iterator, the patch is here and it can be re-considered.","I committed better javadocs, including sample code to iterate the values.","dsmiley","NULL","1","decision","0","0","0","0","1"
"1166","1166","27961","2722","I committed better javadocs, including sample code to iterate the values.
Closing issue as won't fix (for now). If at some point we want an iterator, the patch is here and it can be re-considered.","Closing issue as won't fix (for now).","dsmiley","NULL","1","decision","0","0","0","0","1"
"1167","1167","27961","2722","I committed better javadocs, including sample code to iterate the values.
Closing issue as won't fix (for now). If at some point we want an iterator, the patch is here and it can be re-considered.","If at some point we want an iterator, the patch is here and it can be re-considered.","dsmiley","NULL","1","alternative","0","1","0","0","0"
"1168","1168","27962","2722","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1205","1169","27991","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.
+1 to commit Robert's patch to trunk and iterate there.","
Should we create a branch.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1206","1170","27991","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.
+1 to commit Robert's patch to trunk and iterate there.","I also have some ideas to fix...
We can, or maybe just use trunk?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1207","1171","27991","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.
+1 to commit Robert's patch to trunk and iterate there.","I don't think the patch makes these factories any worse.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1208","1172","27991","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.
+1 to commit Robert's patch to trunk and iterate there.","+1 to commit Robert's patch to trunk and iterate there.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1209","1173","27992","2726","Or you can assign the issue. I just have family coming into town today and won't have any time to do any of the work to help out with this one. 
I thought i knew what was involved with this stuff pretty well but I grossly underestimated the amount of work to even throw the exception ","Or you can assign the issue.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1210","1174","27992","2726","Or you can assign the issue. I just have family coming into town today and won't have any time to do any of the work to help out with this one. 
I thought i knew what was involved with this stuff pretty well but I grossly underestimated the amount of work to even throw the exception ","I just have family coming into town today and won't have any time to do any of the work to help out with this one.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1211","1175","27992","2726","Or you can assign the issue. I just have family coming into town today and won't have any time to do any of the work to help out with this one. 
I thought i knew what was involved with this stuff pretty well but I grossly underestimated the amount of work to even throw the exception ","I thought i knew what was involved with this stuff pretty well but I grossly underestimated the amount of work to even throw the exception","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1212","1176","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","I have no preference on how to proceed.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1213","1177","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","I just dont want to download such a large patch, modify the sources and upload it again.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1214","1178","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1215","1179","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","So its impossible to see any change in comparison to earlier patches.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1216","1180","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert?","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1217","1181","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","+1 on the current patch.","thetaphi","NULL","1","pro","0","0","1","0","0"
"1218","1182","27993","2726","I have no preference on how to proceed. I just dont want to download such a large patch, modify the sources and upload it again. especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too. So its impossible to see any change in comparison to earlier patches.
As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert? +1 on the current patch. We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1219","1183","27995","2726","I committed this to trunk. Can we be extra careful to use LUCENE-4877: in all commit messages so when its time to backport its easy to find the revisions?
Thanks in advance for any improvements!","I committed this to trunk.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1220","1184","27995","2726","I committed this to trunk. Can we be extra careful to use LUCENE-4877: in all commit messages so when its time to backport its easy to find the revisions?
Thanks in advance for any improvements!","Can we be extra careful to use LUCENE-4877: in all commit messages so when its time to backport its easy to find the revisions?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1221","1185","27995","2726","I committed this to trunk. Can we be extra careful to use LUCENE-4877: in all commit messages so when its time to backport its easy to find the revisions?
Thanks in advance for any improvements!","Thanks in advance for any improvements!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1222","1186","27996","2726","
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
I just remembered. I think the reversewildcardfactory in solr has one of these too. ","getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1223","1187","27996","2726","
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
I just remembered. I think the reversewildcardfactory in solr has one of these too. ","I just remembered.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1224","1188","27996","2726","
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
I just remembered. I think the reversewildcardfactory in solr has one of these too. ","I think the reversewildcardfactory in solr has one of these too.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1225","1189","27997","2726","Patch fixing these minor nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.
Actually, I was wrong about LimitTokenCountFilterFactory - it already has a test in place to insure reporting of missing required maxTokenCount param.
Committing shortly.","Patch fixing these minor nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1226","1190","27997","2726","Patch fixing these minor nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.
Actually, I was wrong about LimitTokenCountFilterFactory - it already has a test in place to insure reporting of missing required maxTokenCount param.
Committing shortly.","Actually, I was wrong about LimitTokenCountFilterFactory - it already has a test in place to insure reporting of missing required maxTokenCount param.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1227","1191","27997","2726","Patch fixing these minor nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.
Actually, I was wrong about LimitTokenCountFilterFactory - it already has a test in place to insure reporting of missing required maxTokenCount param.
Committing shortly.","Committing shortly.","steve_rowe","NULL","0","decision","0","0","0","0","1"
"1228","1192","27998","2726","Thanks Steve!","Thanks Steve!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1229","1193","27999","2726","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I implemented these as require(), requireXXX(), etc.
Tests all pass, and precommit's happy.
Committing shortly.","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.","steve_rowe","NULL","1","alternative, pro","0","1","1","0","0"
"1230","1194","27999","2726","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I implemented these as require(), requireXXX(), etc.
Tests all pass, and precommit's happy.
Committing shortly.","I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???","steve_rowe","NULL","1","con","0","0","0","1","0"
"1231","1195","27999","2726","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I implemented these as require(), requireXXX(), etc.
Tests all pass, and precommit's happy.
Committing shortly.","- maybe these could be converted to getRequiredXXX() ?","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1232","1196","27999","2726","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I implemented these as require(), requireXXX(), etc.
Tests all pass, and precommit's happy.
Committing shortly.","I implemented these as require(), requireXXX(), etc.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1233","1197","27999","2726","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I implemented these as require(), requireXXX(), etc.
Tests all pass, and precommit's happy.
Committing shortly.","Tests all pass, and precommit's happy.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1234","1198","27999","2726","This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I implemented these as require(), requireXXX(), etc.
Tests all pass, and precommit's happy.
Committing shortly.","Committing shortly.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1235","1199","28000","2726","Thanks for cleaning this up Steve, much nicer.
I'd like to merge these commits back to 4.x if there are no objections.","Thanks for cleaning this up Steve, much nicer.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1236","1200","28000","2726","Thanks for cleaning this up Steve, much nicer.
I'd like to merge these commits back to 4.x if there are no objections.","I'd like to merge these commits back to 4.x if there are no objections.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1237","1201","28001","2726","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1169","1202","27979","2726","here's one way we could improve it. There are probably other alternatives that might be better, too.
I only fixed one factory as its better to decide this before going thru all of them.","here's one way we could improve it.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1170","1203","27979","2726","here's one way we could improve it. There are probably other alternatives that might be better, too.
I only fixed one factory as its better to decide this before going thru all of them.","There are probably other alternatives that might be better, too.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1171","1204","27979","2726","here's one way we could improve it. There are probably other alternatives that might be better, too.
I only fixed one factory as its better to decide this before going thru all of them.","I only fixed one factory as its better to decide this before going thru all of them.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1172","1205","27980","2726","+1, patch looks good.","+1, patch looks good.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1173","1206","27981","2726","+1","+1","jpountz","NULL","1","pro","0","0","1","0","0"
"1174","1207","27983","2726","I have fixed all the factories, but currently I just started fixing tests and adding tests for bogus parameters to all factories.
I added some helpers to BaseTokenStreamTestCase and fixed TestArabicFilters.java as a prototype...
slow moving","I have fixed all the factories, but currently I just started fixing tests and adding tests for bogus parameters to all factories.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1175","1208","27983","2726","I have fixed all the factories, but currently I just started fixing tests and adding tests for bogus parameters to all factories.
I added some helpers to BaseTokenStreamTestCase and fixed TestArabicFilters.java as a prototype...
slow moving","I added some helpers to BaseTokenStreamTestCase and fixed TestArabicFilters.java as a prototype...
slow moving","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1176","1209","27984","2726","updated patch: tests in analysis/common are done. I'll look at the other analysis modules and solr tomorrow.","updated patch: tests in analysis/common are done.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1177","1210","27984","2726","updated patch: tests in analysis/common are done. I'll look at the other analysis modules and solr tomorrow.","I'll look at the other analysis modules and solr tomorrow.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1178","1211","27985","2726","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"1179","1212","27986","2726","whew, made it thru the rest. all tests pass.","whew, made it thru the rest.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1180","1213","27986","2726","whew, made it thru the rest. all tests pass.","all tests pass.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1181","1214","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","Really nice de-cluttering in the tests.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1182","1215","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???","steve_rowe","NULL","1","con","0","0","0","1","0"
"1183","1216","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","- maybe these could be converted to getRequiredXXX() ?","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1184","1217","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.","steve_rowe","NULL","1","alternative, pro","0","1","1","0","0"
"1185","1218","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1186","1219","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1187","1220","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.","steve_rowe","NULL","1","alternative, pro","0","1","1","0","0"
"1188","1221","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?","steve_rowe","NULL","1","alternative, pro","0","1","1","0","0"
"1189","1222","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1190","1223","27987","2726","Really nice de-cluttering in the tests.
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:

get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.  Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?

A few nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

I can do the work if you agree with these.","I can do the work if you agree with these.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1191","1224","27988","2726","Should we create a branch. I also have some ideas to fix...","Should we create a branch.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1192","1225","27988","2726","Should we create a branch. I also have some ideas to fix...","I also have some ideas to fix...","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1193","1226","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???","rcmuir","NULL","1","con","0","0","0","1","0"
"1194","1227","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","- maybe these could be converted to getRequiredXXX() ?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1195","1228","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods.","rcmuir","NULL","1","alternative, pro, con","0","1","1","1","0"
"1196","1229","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1197","1230","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1198","1231","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1199","1232","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1200","1233","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1201","1234","27989","2726","
I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required??? - maybe these could be converted to getRequiredXXX() ?
I think AbstractAnalysisFactory could use additional param parsing methods:
    get(args, param [, default]) would be a nice addition for strings, instead of args.remove(), which looks different from all the other getXXX() methods. Maybe also a version that takes a set of acceptable values, as well as a boolean for case insensitivity?
    getEnum(args, param, Enum class [, default] ) - probably case insensitivity could be assumed?
    getChar() should be pulled out of PathHierarchyTokenizerFactory, so that DelimitedPayloadTokenFilterFactory can use it.
    getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?
A few nits:
    TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
    EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
    LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
    PatternTokenizerFactory's group param should use the getInt() method with a default of -1.

+1 to all of this!","+1 to all of this!","rcmuir","NULL","1","pro","0","0","1","0","0"
"1202","1235","27990","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.","
Should we create a branch.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1203","1236","27990","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.","I also have some ideas to fix...
We can, or maybe just use trunk?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1204","1237","27990","2726","
Should we create a branch. I also have some ideas to fix...
We can, or maybe just use trunk? I don't think the patch makes these factories any worse.","I don't think the patch makes these factories any worse.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1238","1238","28231","2751","here's just where i am so far, just scavenging stored tests from where i can find. I still havent ported all the ones from .compressing package yet.","here's just where i am so far, just scavenging stored tests from where i can find.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1239","1239","28231","2751","here's just where i am so far, just scavenging stored tests from where i can find. I still havent ported all the ones from .compressing package yet.","I still havent ported all the ones from .compressing package yet.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1240","1240","28232","2751","updated patch. I think this is close... adrien do you know of any other tests we should steal and put in here?","updated patch.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1241","1241","28232","2751","updated patch. I think this is close... adrien do you know of any other tests we should steal and put in here?","I think this is close... adrien do you know of any other tests we should steal and put in here?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1242","1242","28233","2751","Patch looks good!","Patch looks good!","jpountz","NULL","1","pro","0","0","1","0","0"
"1243","1243","28234","2751","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"1244","1244","28235","2751","[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1457926
LUCENE-4852: BaseStoredFieldsFormatTestCase","[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1457926
LUCENE-4852: BaseStoredFieldsFormatTestCase","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1245","1245","28236","2751","I committed this as a start. Hopefully we add more test methods to this base class!","I committed this as a start.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1246","1246","28236","2751","I committed this as a start. Hopefully we add more test methods to this base class!","Hopefully we add more test methods to this base class!","rcmuir","NULL","1","issue","1","0","0","0","0"
"1247","1247","28237","2751","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1457931
LUCENE-4852: BaseStoredFieldsFormatTestCase","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1457931
LUCENE-4852: BaseStoredFieldsFormatTestCase","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1248","1248","28238","2751","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1249","1249","28849","2816","[branch_4x commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448210
LUCENE-4782 - backported fix to branch_4x","[branch_4x commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448210
LUCENE-4782 - backported fix to branch_4x","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1250","1250","28850","2816","[trunk commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448207
LUCENE-4782 - removed wrong line in build.xml","[trunk commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448207
LUCENE-4782 - removed wrong line in build.xml","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1251","1251","28851","2816","[trunk commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448204
LUCENE-4782 - fixed SNBC docsWithClassSize initialization in case of codec doesn't support Terms#getDocCount","[trunk commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448204
LUCENE-4782 - fixed SNBC docsWithClassSize initialization in case of codec doesn't support Terms#getDocCount","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1252","1252","28852","2816","thanks Robert, that may make sense but I'm not really sure as in 4.x we still support 3x codecs in general.
Also I noticed that other methods used in the classification module that depend on the underlying codec may return -1 (unsupported).
I'll have a deeper look and see if it really make sense to 'fallback' such calls or either it'd be safer and reasonable to follow your suggestion.","thanks Robert, that may make sense but I'm not really sure as in 4.x we still support 3x codecs in general.","teofili","NULL","1","pro, con","0","0","1","1","0"
"1253","1253","28852","2816","thanks Robert, that may make sense but I'm not really sure as in 4.x we still support 3x codecs in general.
Also I noticed that other methods used in the classification module that depend on the underlying codec may return -1 (unsupported).
I'll have a deeper look and see if it really make sense to 'fallback' such calls or either it'd be safer and reasonable to follow your suggestion.","Also I noticed that other methods used in the classification module that depend on the underlying codec may return -1 (unsupported).","teofili","NULL","1","con","0","0","0","1","0"
"1254","1254","28852","2816","thanks Robert, that may make sense but I'm not really sure as in 4.x we still support 3x codecs in general.
Also I noticed that other methods used in the classification module that depend on the underlying codec may return -1 (unsupported).
I'll have a deeper look and see if it really make sense to 'fallback' such calls or either it'd be safer and reasonable to follow your suggestion.","I'll have a deeper look and see if it really make sense to 'fallback' such calls or either it'd be safer and reasonable to follow your suggestion.","teofili","NULL","1","alternative, pro","0","1","1","0","0"
"1255","1255","28853","2816","[trunk commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448932
LUCENE-4782 - suppressing SNBC test for Lucene3x codec for now","[trunk commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448932
LUCENE-4782 - suppressing SNBC test for Lucene3x codec for now","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1256","1256","28854","2816","[branch_4x commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448933
LUCENE-4782 - suppressing SNBC test for Lucene3x codec for now","[branch_4x commit] Tommaso Teofili
http://svn.apache.org/viewvc?view=revision&revision=1448933
LUCENE-4782 - suppressing SNBC test for Lucene3x codec for now","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1257","1257","28855","2816","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1258","1258","30221","2953","I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.
In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don't corrumpt their index. Please note: Policeman Jenkins (before it was shot by some Generics Drug Dealer) was running JRockit with this JVM option.","I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.","thetaphi","NULL","1","issue, alternative","1","1","0","0","0"
"1259","1259","30221","2953","I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.
In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don't corrumpt their index. Please note: Policeman Jenkins (before it was shot by some Generics Drug Dealer) was running JRockit with this JVM option.","In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don't corrumpt their index.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1260","1260","30221","2953","I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.
In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don't corrumpt their index. Please note: Policeman Jenkins (before it was shot by some Generics Drug Dealer) was running JRockit with this JVM option.","Please note: Policeman Jenkins (before it was shot by some Generics Drug Dealer) was running JRockit with this JVM option.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1261","1261","30222","2953","Why does it need to be a system property, Hoss? The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests  wouldn't this be enough to cover your use case?

@SuspiciousJ9Shit
@SuspiciousJRockitShit


The only problem I see is that these need to be provided statically  if you need to detect them at runtime then I'd either need to change the code of the runner or we'd need to switch to assumptions inside a rule, for example.","Why does it need to be a system property, Hoss?","dweiss","NULL","0",NULL,"0","0","0","0","0"
"1262","1262","30222","2953","Why does it need to be a system property, Hoss? The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests  wouldn't this be enough to cover your use case?

@SuspiciousJ9Shit
@SuspiciousJRockitShit


The only problem I see is that these need to be provided statically  if you need to detect them at runtime then I'd either need to change the code of the runner or we'd need to switch to assumptions inside a rule, for example.","The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests  wouldn't this be enough to cover your use case?","dweiss","NULL","1","alternative","0","1","0","0","0"
"1263","1263","30222","2953","Why does it need to be a system property, Hoss? The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests  wouldn't this be enough to cover your use case?

@SuspiciousJ9Shit
@SuspiciousJRockitShit


The only problem I see is that these need to be provided statically  if you need to detect them at runtime then I'd either need to change the code of the runner or we'd need to switch to assumptions inside a rule, for example.","@SuspiciousJ9Shit
@SuspiciousJRockitShit


The only problem I see is that these need to be provided statically  if you need to detect them at runtime then I'd either need to change the code of the runner or we'd need to switch to assumptions inside a rule, for example.","dweiss","NULL","1","alternative, con","0","1","0","1","0"
"1264","1264","30223","2953","[trunk commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421818
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","[trunk commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421818
Revert the revert of the revert.","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1265","1265","30223","2953","[trunk commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421818
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do.","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1266","1266","30223","2953","[trunk commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421818
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx.","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1267","1267","30223","2953","[trunk commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421818
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","Thanks in advance!","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1268","1268","30224","2953","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421819
Merged revision(s) 1421818 from lucene/dev/trunk:
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421819
Merged revision(s) 1421818 from lucene/dev/trunk:
Revert the revert of the revert.","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1269","1269","30224","2953","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421819
Merged revision(s) 1421818 from lucene/dev/trunk:
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do.","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1270","1270","30224","2953","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421819
Merged revision(s) 1421818 from lucene/dev/trunk:
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx.","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1271","1271","30224","2953","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1421819
Merged revision(s) 1421818 from lucene/dev/trunk:
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on LUCENE-4630 what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!","Thanks in advance!","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1272","1272","30227","2953","Bulk move 4.4 issues to 4.5 and 5.0","Bulk move 4.4 issues to 4.5 and 5.0","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1273","1273","30228","2953","Move issue to Lucene 4.9.","Move issue to Lucene 4.9.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1274","1274","30229","2953","<<Non-IBMers please ignore this message>>
Dont see this as a bug against IBM JDK.
Thanks and Regards
Brijesh Nekkare
IBM Java team","<<Non-IBMers please ignore this message>>
Dont see this as a bug against IBM JDK.","bnekkare","NULL","0",NULL,"0","0","0","0","0"
"1275","1275","30229","2953","<<Non-IBMers please ignore this message>>
Dont see this as a bug against IBM JDK.
Thanks and Regards
Brijesh Nekkare
IBM Java team","Thanks and Regards
Brijesh Nekkare
IBM Java team","bnekkare","NULL","0",NULL,"0","0","0","0","0"
"1276","1276","30434","2975","here is a patch that adds #estimateDocCount to DISI. It still has some nocommits mainly related to BitSets and carnality but I this its fine as a start. I removed the TermConjunction specialization and changed the heuristic in FilteredQuery to use the estimated cost.","here is a patch that adds #estimateDocCount to DISI.","simonw","NULL","1","alternative","0","1","0","0","0"
"1277","1277","30434","2975","here is a patch that adds #estimateDocCount to DISI. It still has some nocommits mainly related to BitSets and carnality but I this its fine as a start. I removed the TermConjunction specialization and changed the heuristic in FilteredQuery to use the estimated cost.","It still has some nocommits mainly related to BitSets and carnality but I this its fine as a start.","simonw","NULL","1","pro, con","0","0","1","1","0"
"1278","1278","30434","2975","here is a patch that adds #estimateDocCount to DISI. It still has some nocommits mainly related to BitSets and carnality but I this its fine as a start. I removed the TermConjunction specialization and changed the heuristic in FilteredQuery to use the estimated cost.","I removed the TermConjunction specialization and changed the heuristic in FilteredQuery to use the estimated cost.","simonw","NULL","1","alternative","0","1","0","0","0"
"1279","1279","30435","2975","Nice idea! At least the FilteredQuery code looks fine to me. I agree, the Bits interface and FixedBitSet has a costly cardinality (Bits does not have it at all...), so we should think about that. As far as I see it returns maxDoc as cost.","Nice idea!","thetaphi","NULL","1","pro","0","0","1","0","0"
"1280","1280","30435","2975","Nice idea! At least the FilteredQuery code looks fine to me. I agree, the Bits interface and FixedBitSet has a costly cardinality (Bits does not have it at all...), so we should think about that. As far as I see it returns maxDoc as cost.","At least the FilteredQuery code looks fine to me.","thetaphi","NULL","1","pro","0","0","1","0","0"
"1281","1281","30435","2975","Nice idea! At least the FilteredQuery code looks fine to me. I agree, the Bits interface and FixedBitSet has a costly cardinality (Bits does not have it at all...), so we should think about that. As far as I see it returns maxDoc as cost.","I agree, the Bits interface and FixedBitSet has a costly cardinality (Bits does not have it at all...), so we should think about that.","thetaphi","NULL","1","pro, con","0","0","1","1","0"
"1282","1282","30435","2975","Nice idea! At least the FilteredQuery code looks fine to me. I agree, the Bits interface and FixedBitSet has a costly cardinality (Bits does not have it at all...), so we should think about that. As far as I see it returns maxDoc as cost.","As far as I see it returns maxDoc as cost.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1283","1283","30436","2975","One thing: estimatedDocCount is long, but docIds in Lucene are still int - this makes no sense, because the current scorer/disi interface can never return anything > Integer.MAX_VALUE, so the estimatedDocCount can never be 64 bits.
We should maybe rethink in the future to make docIds in Lucene longs, but until this has happened, we should not mix both datatypes in public APIs, this would cause confusion.","One thing: estimatedDocCount is long, but docIds in Lucene are still int - this makes no sense, because the current scorer/disi interface can never return anything > Integer.MAX_VALUE, so the estimatedDocCount can never be 64 bits.","thetaphi","NULL","1","con","0","0","0","1","0"
"1284","1284","30436","2975","One thing: estimatedDocCount is long, but docIds in Lucene are still int - this makes no sense, because the current scorer/disi interface can never return anything > Integer.MAX_VALUE, so the estimatedDocCount can never be 64 bits.
We should maybe rethink in the future to make docIds in Lucene longs, but until this has happened, we should not mix both datatypes in public APIs, this would cause confusion.","We should maybe rethink in the future to make docIds in Lucene longs, but until this has happened, we should not mix both datatypes in public APIs, this would cause confusion.","thetaphi","NULL","1","alternative, con","0","1","0","1","0"
"1285","1285","30437","2975","When i did the cost estimate patch on LUCENE-4236, i chose a long too. but there it was trying to estimate the number of documents visited,
e.g. the number of postings.
so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.
I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.","When i did the cost estimate patch on LUCENE-4236, i chose a long too.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1286","1286","30437","2975","When i did the cost estimate patch on LUCENE-4236, i chose a long too. but there it was trying to estimate the number of documents visited,
e.g. the number of postings.
so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.
I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.","but there it was trying to estimate the number of documents visited,
e.g.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1287","1287","30437","2975","When i did the cost estimate patch on LUCENE-4236, i chose a long too. but there it was trying to estimate the number of documents visited,
e.g. the number of postings.
so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.
I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.","the number of postings.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1288","1288","30437","2975","When i did the cost estimate patch on LUCENE-4236, i chose a long too. but there it was trying to estimate the number of documents visited,
e.g. the number of postings.
so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.
I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.","so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1289","1289","30437","2975","When i did the cost estimate patch on LUCENE-4236, i chose a long too. but there it was trying to estimate the number of documents visited,
e.g. the number of postings.
so the formula for a conjunction would be min(subscorer cost) * #subscorers, and for a disjunction its just the sum of all the subscorer costs, and so on.
I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.","I felt like for scoring purposes this is more useful than the number of documents, but thats just my opinion.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1290","1290","30438","2975","I tend to agree with robert that using longs makes things a lot easier here too. We don't need to deal with int overflows. Maybe we should rename to estimateDocsVisited?","I tend to agree with robert that using longs makes things a lot easier here too.","simonw","NULL","1","pro","0","0","1","0","0"
"1291","1291","30438","2975","I tend to agree with robert that using longs makes things a lot easier here too. We don't need to deal with int overflows. Maybe we should rename to estimateDocsVisited?","We don't need to deal with int overflows.","simonw","NULL","1","pro","0","0","1","0","0"
"1292","1292","30438","2975","I tend to agree with robert that using longs makes things a lot easier here too. We don't need to deal with int overflows. Maybe we should rename to estimateDocsVisited?","Maybe we should rename to estimateDocsVisited?","simonw","NULL","1","alternative","0","1","0","0","0"
"1293","1293","30442","2975","Uwe the current discussion is about not measuring count of documents, but instead i/o operations.
So advance(), docId(), fixedBitset and so on are totally unrelated to that.","Uwe the current discussion is about not measuring count of documents, but instead i/o operations.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1294","1294","30442","2975","Uwe the current discussion is about not measuring count of documents, but instead i/o operations.
So advance(), docId(), fixedBitset and so on are totally unrelated to that.","So advance(), docId(), fixedBitset and so on are totally unrelated to that.","rcmuir","NULL","1","con","0","0","0","1","0"
"1295","1295","30443","2975","Uwe the current discussion is about not measuring count of documents, but instead i/o operations.
Man, I just wanted to make clear that the current patch and the current issue summary have this problem.","Uwe the current discussion is about not measuring count of documents, but instead i/o operations.","thetaphi","NULL","1","issue","1","0","0","0","0"
"1296","1296","30443","2975","Uwe the current discussion is about not measuring count of documents, but instead i/o operations.
Man, I just wanted to make clear that the current patch and the current issue summary have this problem.","Man, I just wanted to make clear that the current patch and the current issue summary have this problem.","thetaphi","NULL","1","issue","1","0","0","0","0"
"1297","1297","30444","2975","ok: I agree if its a count of documents, the type should be consistent.
But as i suggested i dont think a count of documents is that great for how we will use this (picking conjunction leader, filtering heuristics, maybe minimum-should-match disjunction scoring, maybe cleanup exact phrase scorer / add its optimizations to sloppy phrase scorer, maybe more BS versus BS2 heuristics in BooleanWeight, etc etc)","ok: I agree if its a count of documents, the type should be consistent.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1298","1298","30444","2975","ok: I agree if its a count of documents, the type should be consistent.
But as i suggested i dont think a count of documents is that great for how we will use this (picking conjunction leader, filtering heuristics, maybe minimum-should-match disjunction scoring, maybe cleanup exact phrase scorer / add its optimizations to sloppy phrase scorer, maybe more BS versus BS2 heuristics in BooleanWeight, etc etc)","But as i suggested i dont think a count of documents is that great for how we will use this (picking conjunction leader, filtering heuristics, maybe minimum-should-match disjunction scoring, maybe cleanup exact phrase scorer / add its optimizations to sloppy phrase scorer, maybe more BS versus BS2 heuristics in BooleanWeight, etc etc)","rcmuir","NULL","1","con","0","0","0","1","0"
"1299","1299","30445","2975","Patch updated to trunk (as Stefan needs this for LUCENE-4571).
I carefully reviewed all these costs and also integrated it into spans. Though these queries (e.g. nearquery) dont yet use it, they should.
The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).","Patch updated to trunk (as Stefan needs this for LUCENE-4571).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1300","1300","30445","2975","Patch updated to trunk (as Stefan needs this for LUCENE-4571).
I carefully reviewed all these costs and also integrated it into spans. Though these queries (e.g. nearquery) dont yet use it, they should.
The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).","I carefully reviewed all these costs and also integrated it into spans.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1301","1301","30445","2975","Patch updated to trunk (as Stefan needs this for LUCENE-4571).
I carefully reviewed all these costs and also integrated it into spans. Though these queries (e.g. nearquery) dont yet use it, they should.
The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).","Though these queries (e.g.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1302","1302","30445","2975","Patch updated to trunk (as Stefan needs this for LUCENE-4571).
I carefully reviewed all these costs and also integrated it into spans. Though these queries (e.g. nearquery) dont yet use it, they should.
The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).","nearquery) dont yet use it, they should.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1303","1303","30445","2975","Patch updated to trunk (as Stefan needs this for LUCENE-4571).
I carefully reviewed all these costs and also integrated it into spans. Though these queries (e.g. nearquery) dont yet use it, they should.
The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).","The only query i changed was to make ConjunctionTermScorer algorithm our ConjunctionScorer, sorting subscorers on cost (which is docFreq in the all terms case).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1304","1304","30446","2975","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"1305","1305","30447","2975","Thanks! I will come up with a new prototype impl for LUCENE-4571 soon which will include and build upon this patch.","Thanks!","spo","NULL","0",NULL,"0","0","0","0","0"
"1306","1306","30447","2975","Thanks! I will come up with a new prototype impl for LUCENE-4571 soon which will include and build upon this patch.","I will come up with a new prototype impl for LUCENE-4571 soon which will include and build upon this patch.","spo","NULL","1","alternative","0","1","0","0","0"
"1307","1307","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","The patch might be hard to apply due to the svn replace Stefan.","rcmuir","NULL","1","con","0","0","0","1","0"
"1308","1308","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","I plan on doing final checks on it and committing it today.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1309","1309","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","Really its silly lucene doesnt have this for our scorers already, since e.g.","rcmuir","NULL","1","con","0","0","0","1","0"
"1310","1310","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","textbook IR formulas make use of this stuff.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1311","1311","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1312","1312","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1313","1313","30448","2975","The patch might be hard to apply due to the svn replace Stefan. I plan on doing final checks on it and committing it today.
Really its silly lucene doesnt have this for our scorers already, since e.g. textbook IR formulas make use of this stuff. 
We can open followup issues to improve the other scorers logic (FilteredQuery, SpanNearQuery, etc). I only did the obvious 
conjunction one so we get the logic of ConjunctionTermScorer across any arbitrary scorers.
Just gimme a few hours, sorry for the delay ","Just gimme a few hours, sorry for the delay","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1314","1314","30449","2975","Thanks Robert, I was thinking about this the other day! Cool that you brought this back to life! +1 to the patch. I think you need to add a CHANGES.TXT entry still.
simon","Thanks Robert, I was thinking about this the other day!","simonw","NULL","0",NULL,"0","0","0","0","0"
"1315","1315","30449","2975","Thanks Robert, I was thinking about this the other day! Cool that you brought this back to life! +1 to the patch. I think you need to add a CHANGES.TXT entry still.
simon","Cool that you brought this back to life!","simonw","NULL","1","pro","0","0","1","0","0"
"1316","1316","30449","2975","Thanks Robert, I was thinking about this the other day! Cool that you brought this back to life! +1 to the patch. I think you need to add a CHANGES.TXT entry still.
simon","+1 to the patch.","simonw","NULL","1","pro","0","0","1","0","0"
"1317","1317","30449","2975","Thanks Robert, I was thinking about this the other day! Cool that you brought this back to life! +1 to the patch. I think you need to add a CHANGES.TXT entry still.
simon","I think you need to add a CHANGES.TXT entry still.","simonw","NULL","1","alternative","0","1","0","0","0"
"1318","1318","30449","2975","Thanks Robert, I was thinking about this the other day! Cool that you brought this back to life! +1 to the patch. I think you need to add a CHANGES.TXT entry still.
simon","simon","simonw","NULL","0",NULL,"0","0","0","0","0"
"1319","1319","30450","2975","I'll put it in when committing...I always wait until then, otherwise its just asking for conflicts if you ever merge up the patch ","I'll put it in when committing...I always wait until then, otherwise its just asking for conflicts if you ever merge up the patch","rcmuir","NULL","1","decision","0","0","0","0","1"
"1320","1320","30451","2975","[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1456670
LUCENE-4607: add DISI/Spans.cost","[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1456670
LUCENE-4607: add DISI/Spans.cost","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1321","1321","30452","2975","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1456686
LUCENE-4607: add DISI/Spans.cost","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1456686
LUCENE-4607: add DISI/Spans.cost","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1322","1322","30453","2975","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1323","1323","31217","3035","I tried this on that geonames database since my default indexing (just shoving everything in as a TextField)
creates a huge .nrm file today (150MB: 8M docs * 19 fields). Just as a test I tried a simple similarity
implementation that uses 

@Override
public void computeNorm(FieldInvertState state, Norm norm) {
  norm.setPackedLong(state.getLength());
}



-rw-rw-r--  1 rmuir rmuir  49339454 Nov  5 22:30 _7e_nrm.cfs


If you want to use boosts too, you would have to be careful how you encode, but I think this can be useful.
In this case its 1/3 of the RAM, even though documents lengths are exact vs. lossy (though most fields are 
shortish, some are huge, like alternate names fields for major countries and cities, which have basically every 
language imaginable shoved in the field: thats why it doesnt save more I think)","I tried this on that geonames database since my default indexing (just shoving everything in as a TextField)
creates a huge .nrm file today (150MB: 8M docs * 19 fields).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1324","1324","31217","3035","I tried this on that geonames database since my default indexing (just shoving everything in as a TextField)
creates a huge .nrm file today (150MB: 8M docs * 19 fields). Just as a test I tried a simple similarity
implementation that uses 

@Override
public void computeNorm(FieldInvertState state, Norm norm) {
  norm.setPackedLong(state.getLength());
}



-rw-rw-r--  1 rmuir rmuir  49339454 Nov  5 22:30 _7e_nrm.cfs


If you want to use boosts too, you would have to be careful how you encode, but I think this can be useful.
In this case its 1/3 of the RAM, even though documents lengths are exact vs. lossy (though most fields are 
shortish, some are huge, like alternate names fields for major countries and cities, which have basically every 
language imaginable shoved in the field: thats why it doesnt save more I think)","Just as a test I tried a simple similarity
implementation that uses 

@Override
public void computeNorm(FieldInvertState state, Norm norm) {
  norm.setPackedLong(state.getLength());
}



-rw-rw-r--  1 rmuir rmuir  49339454 Nov  5 22:30 _7e_nrm.cfs


If you want to use boosts too, you would have to be careful how you encode, but I think this can be useful.","rcmuir","NULL","1","alternative, pro, con","0","1","1","1","0"
"1325","1325","31217","3035","I tried this on that geonames database since my default indexing (just shoving everything in as a TextField)
creates a huge .nrm file today (150MB: 8M docs * 19 fields). Just as a test I tried a simple similarity
implementation that uses 

@Override
public void computeNorm(FieldInvertState state, Norm norm) {
  norm.setPackedLong(state.getLength());
}



-rw-rw-r--  1 rmuir rmuir  49339454 Nov  5 22:30 _7e_nrm.cfs


If you want to use boosts too, you would have to be careful how you encode, but I think this can be useful.
In this case its 1/3 of the RAM, even though documents lengths are exact vs. lossy (though most fields are 
shortish, some are huge, like alternate names fields for major countries and cities, which have basically every 
language imaginable shoved in the field: thats why it doesnt save more I think)","In this case its 1/3 of the RAM, even though documents lengths are exact vs. lossy (though most fields are 
shortish, some are huge, like alternate names fields for major countries and cities, which have basically every 
language imaginable shoved in the field: thats why it doesnt save more I think)","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1326","1326","31218","3035","+1 - should we also document that we don't have similarities that can make use of it at this point?","+1 - should we also document that we don't have similarities that can make use of it at this point?","simonw","NULL","1","alternative, pro","0","1","1","0","0"
"1327","1327","31219","3035","+1, very cool!","+1, very cool!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1328","1328","31220","3035","I don't understand the question Simon: all the ones we provide happen to use Norm.setByte
I don't think we need to add documentation to Norm.setFloat,Norm.setDouble saying that we don't
provide any similarities that call these methods: thats not important to anybody.","I don't understand the question Simon: all the ones we provide happen to use Norm.setByte
I don't think we need to add documentation to Norm.setFloat,Norm.setDouble saying that we don't
provide any similarities that call these methods: thats not important to anybody.","rcmuir","NULL","1","con","0","0","0","1","0"
"1329","1329","31221","3035","I don't understand the question Simon: all the ones we provide happen to use Norm.setByte
Just to clarify. Currently if we write packed ints and a similarity calls Source#getArray you get an UOE. I think we should document that our current impls won't handle this. ","I don't understand the question Simon: all the ones we provide happen to use Norm.setByte
Just to clarify.","simonw","NULL","0",NULL,"0","0","0","0","0"
"1330","1330","31221","3035","I don't understand the question Simon: all the ones we provide happen to use Norm.setByte
Just to clarify. Currently if we write packed ints and a similarity calls Source#getArray you get an UOE. I think we should document that our current impls won't handle this. ","Currently if we write packed ints and a similarity calls Source#getArray you get an UOE.","simonw","NULL","1","issue","1","0","0","0","0"
"1331","1331","31221","3035","I don't understand the question Simon: all the ones we provide happen to use Norm.setByte
Just to clarify. Currently if we write packed ints and a similarity calls Source#getArray you get an UOE. I think we should document that our current impls won't handle this. ","I think we should document that our current impls won't handle this.","simonw","NULL","1","alternative","0","1","0","0","0"
"1332","1332","31222","3035","I don't see how its relevant. Issues will happen if you use Norm.setFloat (as they expect a byte).
I'm not going to confuse the documentation. The built-in Similarities at query-time
depend upon their index-time norm implementation: this is documented extensively everywhere!
","I don't see how its relevant.","rcmuir","NULL","1","con","0","0","0","1","0"
"1333","1333","31222","3035","I don't see how its relevant. Issues will happen if you use Norm.setFloat (as they expect a byte).
I'm not going to confuse the documentation. The built-in Similarities at query-time
depend upon their index-time norm implementation: this is documented extensively everywhere!
","Issues will happen if you use Norm.setFloat (as they expect a byte).","rcmuir","NULL","1","issue","1","0","0","0","0"
"1334","1334","31222","3035","I don't see how its relevant. Issues will happen if you use Norm.setFloat (as they expect a byte).
I'm not going to confuse the documentation. The built-in Similarities at query-time
depend upon their index-time norm implementation: this is documented extensively everywhere!
","I'm not going to confuse the documentation.","rcmuir","NULL","1","con","0","0","0","1","0"
"1335","1335","31222","3035","I don't see how its relevant. Issues will happen if you use Norm.setFloat (as they expect a byte).
I'm not going to confuse the documentation. The built-in Similarities at query-time
depend upon their index-time norm implementation: this is documented extensively everywhere!
","The built-in Similarities at query-time
depend upon their index-time norm implementation: this is documented extensively everywhere!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1336","1336","31223","3035","fair enough. I just wanted to mention it..","fair enough.","simonw","NULL","0",NULL,"0","0","0","0","0"
"1337","1337","31223","3035","fair enough. I just wanted to mention it..","I just wanted to mention it..","simonw","NULL","0",NULL,"0","0","0","0","0"
"1338","1338","31224","3035","If someone changes their similarity to use a different norm type at index-time than at query-time,
then he or she is an idiot!","If someone changes their similarity to use a different norm type at index-time than at query-time,
then he or she is an idiot!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1339","1339","31225","3035","I plan to revert this for 4.1 to contain the amount of backwards compatibility code we need to implement for LUCENE-4547.
If someone uses this functionality in its current form, they will easily hit the LUCENE-4547 bug.
I implemented this more efficiently with the new APIs in the lucene4547 branch anyway: when it would save RAM, and the # of values is small, it dereferences the unique values and packs ords. This is typically the case with our smallfloat encoding.","I plan to revert this for 4.1 to contain the amount of backwards compatibility code we need to implement for LUCENE-4547.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1340","1340","31225","3035","I plan to revert this for 4.1 to contain the amount of backwards compatibility code we need to implement for LUCENE-4547.
If someone uses this functionality in its current form, they will easily hit the LUCENE-4547 bug.
I implemented this more efficiently with the new APIs in the lucene4547 branch anyway: when it would save RAM, and the # of values is small, it dereferences the unique values and packs ords. This is typically the case with our smallfloat encoding.","If someone uses this functionality in its current form, they will easily hit the LUCENE-4547 bug.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1341","1341","31225","3035","I plan to revert this for 4.1 to contain the amount of backwards compatibility code we need to implement for LUCENE-4547.
If someone uses this functionality in its current form, they will easily hit the LUCENE-4547 bug.
I implemented this more efficiently with the new APIs in the lucene4547 branch anyway: when it would save RAM, and the # of values is small, it dereferences the unique values and packs ords. This is typically the case with our smallfloat encoding.","I implemented this more efficiently with the new APIs in the lucene4547 branch anyway: when it would save RAM, and the # of values is small, it dereferences the unique values and packs ords.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1342","1342","31225","3035","I plan to revert this for 4.1 to contain the amount of backwards compatibility code we need to implement for LUCENE-4547.
If someone uses this functionality in its current form, they will easily hit the LUCENE-4547 bug.
I implemented this more efficiently with the new APIs in the lucene4547 branch anyway: when it would save RAM, and the # of values is small, it dereferences the unique values and packs ords. This is typically the case with our smallfloat encoding.","This is typically the case with our smallfloat encoding.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1343","1343","31226","3035","[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1432096
LUCENE-4540: revert","[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1432096
LUCENE-4540: revert","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1344","1344","31227","3035","I backed this out of 4.1. When LUCENE-4547 lands, we can resolve it with that implementation.","I backed this out of 4.1.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1345","1345","31227","3035","I backed this out of 4.1. When LUCENE-4547 lands, we can resolve it with that implementation.","When LUCENE-4547 lands, we can resolve it with that implementation.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1346","1346","31228","3035","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1432100
LUCENE-4540: revert","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1432100
LUCENE-4540: revert","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1347","1347","31229","3035","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1406433
LUCENE-4540: allow packed ints norms","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1406433
LUCENE-4540: allow packed ints norms","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1348","1348","31230","3035","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1349","1349","32224","3158","+1 That's pretty damn cool","+1 That's pretty damn cool","cmale","NULL","1","pro","0","0","1","0","0"
"1350","1350","32225","3158","Awesome!  +1","Awesome!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1351","1351","32225","3158","Awesome!  +1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"1352","1352","32226","3158","Almost got this working, two bugs to resolve:

a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html. I'll make the macro use a throwaway directory and delete it.
a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly. I don't know why this is working with 'ant javadocs', but it really shouldnt, since lucene queryparser should not be in its compile classpath. I'll fix it to use docRoot.

","Almost got this working, two bugs to resolve:

a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1353","1353","32226","3158","Almost got this working, two bugs to resolve:

a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html. I'll make the macro use a throwaway directory and delete it.
a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly. I don't know why this is working with 'ant javadocs', but it really shouldnt, since lucene queryparser should not be in its compile classpath. I'll fix it to use docRoot.

","I'll make the macro use a throwaway directory and delete it.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1354","1354","32226","3158","Almost got this working, two bugs to resolve:

a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html. I'll make the macro use a throwaway directory and delete it.
a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly. I don't know why this is working with 'ant javadocs', but it really shouldnt, since lucene queryparser should not be in its compile classpath. I'll fix it to use docRoot.

","a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1355","1355","32226","3158","Almost got this working, two bugs to resolve:

a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html. I'll make the macro use a throwaway directory and delete it.
a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly. I don't know why this is working with 'ant javadocs', but it really shouldnt, since lucene queryparser should not be in its compile classpath. I'll fix it to use docRoot.

","I don't know why this is working with 'ant javadocs', but it really shouldnt, since lucene queryparser should not be in its compile classpath.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1356","1356","32226","3158","Almost got this working, two bugs to resolve:

a bug in eclipse compiler (imo), i tell it to create no class files, but its creating some for spatial (package-info.class processing) because it uses package-info.java instead of package.html. I'll make the macro use a throwaway directory and delete it.
a bug in solrj javadocs: it links to the lucene queryparser syntax incorrectly. I don't know why this is working with 'ant javadocs', but it really shouldnt, since lucene queryparser should not be in its compile classpath. I'll fix it to use docRoot.

","I'll fix it to use docRoot.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1357","1357","32227","3158","updated patch: everything is passing.
Ill run precommit and get this thing in (trunk/4x only): i spent a lot of time cleaning up docs and want to keep the bar high.
we can adjust the properties as needed later as more cleanup happens, but i dont want to let them get any worse.","updated patch: everything is passing.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1358","1358","32227","3158","updated patch: everything is passing.
Ill run precommit and get this thing in (trunk/4x only): i spent a lot of time cleaning up docs and want to keep the bar high.
we can adjust the properties as needed later as more cleanup happens, but i dont want to let them get any worse.","Ill run precommit and get this thing in (trunk/4x only): i spent a lot of time cleaning up docs and want to keep the bar high.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1359","1359","32227","3158","updated patch: everything is passing.
Ill run precommit and get this thing in (trunk/4x only): i spent a lot of time cleaning up docs and want to keep the bar high.
we can adjust the properties as needed later as more cleanup happens, but i dont want to let them get any worse.","we can adjust the properties as needed later as more cleanup happens, but i dont want to let them get any worse.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1360","1360","32228","3158","The test has a whitespace in pathname problem. Also its not nice to permgen. I have a patch fixing those 2 problems. It also prints a nice taskname instead of [javac] on ANT output.","The test has a whitespace in pathname problem.","thetaphi","NULL","1","issue","1","0","0","0","0"
"1361","1361","32228","3158","The test has a whitespace in pathname problem. Also its not nice to permgen. I have a patch fixing those 2 problems. It also prints a nice taskname instead of [javac] on ANT output.","Also its not nice to permgen.","thetaphi","NULL","1","con","0","0","0","1","0"
"1362","1362","32228","3158","The test has a whitespace in pathname problem. Also its not nice to permgen. I have a patch fixing those 2 problems. It also prints a nice taskname instead of [javac] on ANT output.","I have a patch fixing those 2 problems.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1363","1363","32228","3158","The test has a whitespace in pathname problem. Also its not nice to permgen. I have a patch fixing those 2 problems. It also prints a nice taskname instead of [javac] on ANT output.","It also prints a nice taskname instead of [javac] on ANT output.","thetaphi","NULL","1","pro","0","0","1","0","0"
"1364","1364","32230","3158","+1, thanks for cleaning up. taskname is cool, i didn't like the log output but didnt know about this.","+1, thanks for cleaning up.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1365","1365","32230","3158","+1, thanks for cleaning up. taskname is cool, i didn't like the log output but didnt know about this.","taskname is cool, i didn't like the log output but didnt know about this.","rcmuir","NULL","1","pro, con","0","0","1","1","0"
"1366","1366","32231","3158","Committed trunk revision: 1389491
Committed 4.x revision: 1389492","Committed trunk revision: 1389491
Committed 4.x revision: 1389492","thetaphi","NULL","1","decision","0","0","0","0","1"
"1367","1367","32232","3158","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1389492
Merged revision(s) 1389491 from lucene/dev/trunk:
LUCENE-4409: Improve ECJ-Linter (permgen, taskname) + fix whitespace bug","[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1389492
Merged revision(s) 1389491 from lucene/dev/trunk:
LUCENE-4409: Improve ECJ-Linter (permgen, taskname) + fix whitespace bug","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1368","1368","32233","3158","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1389192
LUCENE-4409: implement javadocs linting with eclipse ecj compiler","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1389192
LUCENE-4409: implement javadocs linting with eclipse ecj compiler","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1369","1369","32234","3158","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1370","1370","32306","3167","My latest test for branch_4x finished this test in 2771.02 seconds on Windows. No errors were reported. CPU was relatively idle whenever I checked (25\%). In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)
On trunk, TestIndexWriterWithThreads took 90 seconds.
","My latest test for branch_4x finished this test in 2771.02 seconds on Windows.","jkrupan","NULL","0",NULL,"0","0","0","0","0"
"1371","1371","32306","3167","My latest test for branch_4x finished this test in 2771.02 seconds on Windows. No errors were reported. CPU was relatively idle whenever I checked (25\%). In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)
On trunk, TestIndexWriterWithThreads took 90 seconds.
","No errors were reported.","jkrupan","NULL","1","pro","0","0","1","0","0"
"1372","1372","32306","3167","My latest test for branch_4x finished this test in 2771.02 seconds on Windows. No errors were reported. CPU was relatively idle whenever I checked (25\%). In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)
On trunk, TestIndexWriterWithThreads took 90 seconds.
","CPU was relatively idle whenever I checked (25\%).","jkrupan","NULL","1","pro","0","0","1","0","0"
"1373","1373","32306","3167","My latest test for branch_4x finished this test in 2771.02 seconds on Windows. No errors were reported. CPU was relatively idle whenever I checked (25\%). In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)
On trunk, TestIndexWriterWithThreads took 90 seconds.
","In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)","jkrupan","NULL","1","pro","0","0","1","0","0"
"1374","1374","32306","3167","My latest test for branch_4x finished this test in 2771.02 seconds on Windows. No errors were reported. CPU was relatively idle whenever I checked (25\%). In Eclipse it took 316 seconds (while ant test was still running the rest of the tests.)
On trunk, TestIndexWriterWithThreads took 90 seconds.
","On trunk, TestIndexWriterWithThreads took 90 seconds.","jkrupan","NULL","1","pro","0","0","1","0","0"
"1375","1375","32307","3167","Did you run all these with the same seed?","Did you run all these with the same seed?","dweiss","NULL","0",NULL,"0","0","0","0","0"
"1376","1376","32308","3167","It's weird, the variance on this test is indeed very high. I think it may have something to do with the fact that it spins many threads (that do i/o) so if you're running on a multicore and there are other parallel jvms running tests you're putting a load on the hardware. If ran in isolation things get much faster (for me).
I've replaced some of the random() calls with the non-asserting random; I see some difference but not that much.","It's weird, the variance on this test is indeed very high.","dweiss","NULL","1","issue","1","0","0","0","0"
"1377","1377","32308","3167","It's weird, the variance on this test is indeed very high. I think it may have something to do with the fact that it spins many threads (that do i/o) so if you're running on a multicore and there are other parallel jvms running tests you're putting a load on the hardware. If ran in isolation things get much faster (for me).
I've replaced some of the random() calls with the non-asserting random; I see some difference but not that much.","I think it may have something to do with the fact that it spins many threads (that do i/o) so if you're running on a multicore and there are other parallel jvms running tests you're putting a load on the hardware.","dweiss","NULL","1","issue","1","0","0","0","0"
"1378","1378","32308","3167","It's weird, the variance on this test is indeed very high. I think it may have something to do with the fact that it spins many threads (that do i/o) so if you're running on a multicore and there are other parallel jvms running tests you're putting a load on the hardware. If ran in isolation things get much faster (for me).
I've replaced some of the random() calls with the non-asserting random; I see some difference but not that much.","If ran in isolation things get much faster (for me).","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"1379","1379","32308","3167","It's weird, the variance on this test is indeed very high. I think it may have something to do with the fact that it spins many threads (that do i/o) so if you're running on a multicore and there are other parallel jvms running tests you're putting a load on the hardware. If ran in isolation things get much faster (for me).
I've replaced some of the random() calls with the non-asserting random; I see some difference but not that much.","I've replaced some of the random() calls with the non-asserting random; I see some difference but not that much.","dweiss","NULL","1","alternative, pro, con","0","1","1","1","0"
"1380","1380","32309","3167","here's a patch: there are two things,

the test is too slow in general (too many iterations)
the test is super-slow on windows because of syncd i/o: i wired it to use mmapdirectory.

","here's a patch: there are two things,

the test is too slow in general (too many iterations)
the test is super-slow on windows because of syncd i/o: i wired it to use mmapdirectory.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1381","1381","32310","3167","updated patch: I'd rather just use newDirectory actually. 
This means the test will be occasionally slow on windows, but I think thats ok. I dont want to start the path of losing test coverage because of certain os brokenness.","updated patch: I'd rather just use newDirectory actually.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1382","1382","32310","3167","updated patch: I'd rather just use newDirectory actually. 
This means the test will be occasionally slow on windows, but I think thats ok. I dont want to start the path of losing test coverage because of certain os brokenness.","This means the test will be occasionally slow on windows, but I think thats ok.","rcmuir","NULL","1","pro, con","0","0","1","1","0"
"1383","1383","32310","3167","updated patch: I'd rather just use newDirectory actually. 
This means the test will be occasionally slow on windows, but I think thats ok. I dont want to start the path of losing test coverage because of certain os brokenness.","I dont want to start the path of losing test coverage because of certain os brokenness.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1384","1384","32311","3167","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1387549
LUCENE-4397: speed up test","[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1387549
LUCENE-4397: speed up test","commit-tag-bot","NULL","0",NULL,"0","0","0","0","0"
"1385","1385","33994","3313","Patch.","Patch.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1386","1386","33996","3313","New patch, fixing previous nocommits / downgrading to TODOs.  I also removed the specialized scorers since they seem not to help much.
All tests pass, but I still need to fix all tests that now avoid MemoryPF to also avoid DirectPF.  Otherwise I think it's ready...","New patch, fixing previous nocommits / downgrading to TODOs.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1387","1387","33996","3313","New patch, fixing previous nocommits / downgrading to TODOs.  I also removed the specialized scorers since they seem not to help much.
All tests pass, but I still need to fix all tests that now avoid MemoryPF to also avoid DirectPF.  Otherwise I think it's ready...","I also removed the specialized scorers since they seem not to help much.","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"1388","1388","33996","3313","New patch, fixing previous nocommits / downgrading to TODOs.  I also removed the specialized scorers since they seem not to help much.
All tests pass, but I still need to fix all tests that now avoid MemoryPF to also avoid DirectPF.  Otherwise I think it's ready...","All tests pass, but I still need to fix all tests that now avoid MemoryPF to also avoid DirectPF.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1389","1389","33996","3313","New patch, fixing previous nocommits / downgrading to TODOs.  I also removed the specialized scorers since they seem not to help much.
All tests pass, but I still need to fix all tests that now avoid MemoryPF to also avoid DirectPF.  Otherwise I think it's ready...","Otherwise I think it's ready...","mikemccand","NULL","1","pro","0","0","1","0","0"
"1390","1390","33997","3313","Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
in packed ints (with huper-duper fast options) instead of wasting so much on them?","Would it really be that much slower if it was slightly more reasonable, e.g.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1391","1391","33997","3313","Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
in packed ints (with huper-duper fast options) instead of wasting so much on them?","storing freqs
in packed ints (with huper-duper fast options) instead of wasting so much on them?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1392","1392","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).","Would it really be that much slower if it was slightly more reasonable, e.g.","mikemccand","NULL","1","issue","1","0","0","0","0"
"1393","1393","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).","storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1394","1394","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).","Probably not that much slower?","mikemccand","NULL","1","pro","0","0","1","0","0"
"1395","1395","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).","I think that's a good idea!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1396","1396","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).","But I think we can explore this after committing?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1397","1397","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).","There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1398","1398","33998","3313","
Would it really be that much slower if it was slightly more reasonable, e.g. storing freqs
 in packed ints (with huper-duper fast options) instead of wasting so much on them?
Probably not that much slower?  I think that's a good idea!
But I think we can explore this after committing?  There are other things we can try too (eg collapse skip list into shared int[]: I think this one may give a perf gain, collapse positions, etc.).",").","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1399","1399","34002","3313","New patch, adding scary warning & MIGRATE.txt entry, fixing javadoc errors, and adding lucene.experimental ... still haven't thought of another name yet ...","New patch, adding scary warning & MIGRATE.txt entry, fixing javadoc errors, and adding lucene.experimental ... still haven't thought of another name yet ...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1400","1400","34003","3313","I dont have better name either. Lets just commit it with this one and think about it for later!","I dont have better name either.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1401","1401","34003","3313","I dont have better name either. Lets just commit it with this one and think about it for later!","Lets just commit it with this one and think about it for later!","rcmuir","NULL","1","decision","0","0","0","0","1"
"1402","1402","35439","3459","Escape URI paths for XSL","Escape URI paths for XSL","gbowyer@fastmail.co.uk","NULL","1","alternative","0","1","0","0","0"
"1403","1403","35440","3459","I don't understand the problem, can you provide your build.xml? If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box. If XSL does not like your own and modified build files, they are invalid XML, so fix those. The attached patch seems to work only around your invalid XML.","I don't understand the problem, can you provide your build.xml?","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1404","1404","35440","3459","I don't understand the problem, can you provide your build.xml? If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box. If XSL does not like your own and modified build files, they are invalid XML, so fix those. The attached patch seems to work only around your invalid XML.","If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box.","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"1405","1405","35440","3459","I don't understand the problem, can you provide your build.xml? If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box. If XSL does not like your own and modified build files, they are invalid XML, so fix those. The attached patch seems to work only around your invalid XML.","If XSL does not like your own and modified build files, they are invalid XML, so fix those.","thetaphi","NULL","1","issue","1","0","0","0","0"
"1406","1406","35440","3459","I don't understand the problem, can you provide your build.xml? If you changed the Lucene-provided build files, this is not an issue, because those always build out of the box. If XSL does not like your own and modified build files, they are invalid XML, so fix those. The attached patch seems to work only around your invalid XML.","The attached patch seems to work only around your invalid XML.","thetaphi","NULL","1","alternative, con","0","1","0","1","0"
"1407","1407","35441","3459","My build.xml is the same as upstream, the problem is my checkout path looks like this
/home/buildserver/workspace/builds/
{search-engineering}-solr-lucene-{trunk}

This means that the prepare-webpages target gets its paths in the buildpaths variable as a pipe separated list like so

/home/buildserver/workspace/builds/{search-engineering}
solr-lucene
{trunk}/lucene/analysis/common/build.xml|/home/buildserver/workspace/builds/{search-engineering}-solr-lucene-{trunk}
/lucene/analysis/icu/build.xml|...(and so on)
XSLT picks this up later and tries to load these paths, however XSLT assumes that they are URLS which makes the { character invalid and causes
com.sun.org.apache.xalan.internal.xsltc.TransletException: javax.xml.transform.TransformerException: com.sun.org.apache.xml.internal.utils.URI$MalformedURIException: Path contains invalid character: {
This pattern is infrastructural to where I work and is not likely to change (I would like it too)
Not sure if that makes sense","My build.xml is the same as upstream, the problem is my checkout path looks like this
/home/buildserver/workspace/builds/
{search-engineering}-solr-lucene-{trunk}

This means that the prepare-webpages target gets its paths in the buildpaths variable as a pipe separated list like so

/home/buildserver/workspace/builds/{search-engineering}
solr-lucene
{trunk}/lucene/analysis/common/build.xml|/home/buildserver/workspace/builds/{search-engineering}-solr-lucene-{trunk}
/lucene/analysis/icu/build.xml|...(and so on)
XSLT picks this up later and tries to load these paths, however XSLT assumes that they are URLS which makes the { character invalid and causes
com.sun.org.apache.xalan.internal.xsltc.TransletException: javax.xml.transform.TransformerException: com.sun.org.apache.xml.internal.utils.URI$MalformedURIException: Path contains invalid character: {
This pattern is infrastructural to where I work and is not likely to change (I would like it too)
Not sure if that makes sense","gbowyer@fastmail.co.uk","NULL","1","issue","1","0","0","0","0"
"1408","1408","35442","3459","Hi,
that makes sense, thanks for reporting this! I will fix this, thanks for the nice workaround patch.","Hi,
that makes sense, thanks for reporting this!","thetaphi","NULL","1","pro","0","0","1","0","0"
"1409","1409","35442","3459","Hi,
that makes sense, thanks for reporting this! I will fix this, thanks for the nice workaround patch.","I will fix this, thanks for the nice workaround patch.","thetaphi","NULL","1","pro","0","0","1","0","0"
"1410","1410","35444","3459","Committed rev 1339097. Thanks Greg!","Committed rev 1339097.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1411","1411","35444","3459","Committed rev 1339097. Thanks Greg!","Thanks Greg!","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1412","1412","35588","3474","Draft patch. Added ScoreMode as parameter to JoinUtil#createJoinQuery(...).
Maybe ScoreMode should be a public enum inside the join package.","Draft patch.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1413","1413","35588","3474","Draft patch. Added ScoreMode as parameter to JoinUtil#createJoinQuery(...).
Maybe ScoreMode should be a public enum inside the join package.","Added ScoreMode as parameter to JoinUtil#createJoinQuery(...).","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1414","1414","35588","3474","Draft patch. Added ScoreMode as parameter to JoinUtil#createJoinQuery(...).
Maybe ScoreMode should be a public enum inside the join package.","Maybe ScoreMode should be a public enum inside the join package.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1415","1415","35589","3474","Updated patch.

Started adding randomizing score mode in TestJoinUtil test class.
Made ScoreMode a public enum in join package.

","Updated patch.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1416","1416","35589","3474","Updated patch.

Started adding randomizing score mode in TestJoinUtil test class.
Made ScoreMode a public enum in join package.

","Started adding randomizing score mode in TestJoinUtil test class.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1417","1417","35589","3474","Updated patch.

Started adding randomizing score mode in TestJoinUtil test class.
Made ScoreMode a public enum in join package.

","Made ScoreMode a public enum in join package.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1418","1418","35590","3474","Updated patch. 

Fixed random tests.
Added support for explain.
Added ScoreMode support for documents that relate to more than one document.

I think it is ready to be committed.","Updated patch.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1419","1419","35590","3474","Updated patch. 

Fixed random tests.
Added support for explain.
Added ScoreMode support for documents that relate to more than one document.

I think it is ready to be committed.","Fixed random tests.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1420","1420","35590","3474","Updated patch. 

Fixed random tests.
Added support for explain.
Added ScoreMode support for documents that relate to more than one document.

I think it is ready to be committed.","Added support for explain.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1421","1421","35590","3474","Updated patch. 

Fixed random tests.
Added support for explain.
Added ScoreMode support for documents that relate to more than one document.

I think it is ready to be committed.","Added ScoreMode support for documents that relate to more than one document.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1422","1422","35590","3474","Updated patch. 

Fixed random tests.
Added support for explain.
Added ScoreMode support for documents that relate to more than one document.

I think it is ready to be committed.","I think it is ready to be committed.","martijn.v.groningen","NULL","1","pro","0","0","1","0","0"
"1423","1423","35593","3474","I still see one omitted   Otherwise this looks great: +1 to commit!","I still see one omitted   Otherwise this looks great: +1 to commit!","mikemccand","NULL","1","pro, con","0","0","1","1","0"
"1424","1424","35594","3474","Oops... I see. I'll commit soon!","Oops...","martijn.v.groningen","NULL","0",NULL,"0","0","0","0","0"
"1425","1425","35594","3474","Oops... I see. I'll commit soon!","I see.","martijn.v.groningen","NULL","0",NULL,"0","0","0","0","0"
"1426","1426","35594","3474","Oops... I see. I'll commit soon!","I'll commit soon!","martijn.v.groningen","NULL","1","decision","0","0","0","0","1"
"1427","1427","35595","3474","Committed to trunk and branch4x.","Committed to trunk and branch4x.","martijn.v.groningen","NULL","1","decision","0","0","0","0","1"
"1428","1428","35596","3474","Hi, Is this possible to use in solr I tried setting 
{!scoreMode=Avg}
, but it doesn't seem to have any effect.","Hi, Is this possible to use in solr I tried setting 
{!scoreMode=Avg}
, but it doesn't seem to have any effect.","davidvdd","NULL","1","issue","1","0","0","0","0"
"1429","1429","35597","3474","Solr uses a different joining implementation. Which doesn't support mapping the scores from the `from` side to the `to` side. If you want to use the Lucene joining implementation you could wrap this in a Solr QParserPlugin extension. ","Solr uses a different joining implementation.","martijn.v.groningen","NULL","0",NULL,"0","0","0","0","0"
"1430","1430","35597","3474","Solr uses a different joining implementation. Which doesn't support mapping the scores from the `from` side to the `to` side. If you want to use the Lucene joining implementation you could wrap this in a Solr QParserPlugin extension. ","Which doesn't support mapping the scores from the `from` side to the `to` side.","martijn.v.groningen","NULL","1","issue","1","0","0","0","0"
"1431","1431","35597","3474","Solr uses a different joining implementation. Which doesn't support mapping the scores from the `from` side to the `to` side. If you want to use the Lucene joining implementation you could wrap this in a Solr QParserPlugin extension. ","If you want to use the Lucene joining implementation you could wrap this in a Solr QParserPlugin extension.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1432","1432","35600","3474","Same problem if I use:
IndexSearcher searcher = new IndexSearcher(req.getSearcher().getIndexReader());
I can't override the class since the constructor is private. This is probably to only use the static methods.
I've used an output to see the hashcode and it stays the same. Can I change this behaviour somehow?","Same problem if I use:
IndexSearcher searcher = new IndexSearcher(req.getSearcher().getIndexReader());
I can't override the class since the constructor is private.","davidvdd","NULL","1","issue","1","0","0","0","0"
"1433","1433","35600","3474","Same problem if I use:
IndexSearcher searcher = new IndexSearcher(req.getSearcher().getIndexReader());
I can't override the class since the constructor is private. This is probably to only use the static methods.
I've used an output to see the hashcode and it stays the same. Can I change this behaviour somehow?","This is probably to only use the static methods.","davidvdd","NULL","0",NULL,"0","0","0","0","0"
"1434","1434","35600","3474","Same problem if I use:
IndexSearcher searcher = new IndexSearcher(req.getSearcher().getIndexReader());
I can't override the class since the constructor is private. This is probably to only use the static methods.
I've used an output to see the hashcode and it stays the same. Can I change this behaviour somehow?","I've used an output to see the hashcode and it stays the same.","davidvdd","NULL","0",NULL,"0","0","0","0","0"
"1435","1435","35600","3474","Same problem if I use:
IndexSearcher searcher = new IndexSearcher(req.getSearcher().getIndexReader());
I can't override the class since the constructor is private. This is probably to only use the static methods.
I've used an output to see the hashcode and it stays the same. Can I change this behaviour somehow?","Can I change this behaviour somehow?","davidvdd","NULL","1","issue","1","0","0","0","0"
"1436","1436","35601","3474","I got suggested to extend the Query class and return a Hash myself.
My query class contains:
        @Override
        public int hashCode() 
{
            return q.toString().hashCode();
        }
It seems to work now.","I got suggested to extend the Query class and return a Hash myself.","davidvdd","NULL","1","alternative","0","1","0","0","0"
"1437","1437","35601","3474","I got suggested to extend the Query class and return a Hash myself.
My query class contains:
        @Override
        public int hashCode() 
{
            return q.toString().hashCode();
        }
It seems to work now.","My query class contains:
        @Override
        public int hashCode() 
{
            return q.toString().hashCode();
        }
It seems to work now.","davidvdd","NULL","1","alternative, pro","0","1","1","0","0"
"1438","1438","35602","3474","Hi David, I just committed LUCENE-4704. Query instances returned from JoinUtil will implement equals and hashcode in future versions.","Hi David, I just committed LUCENE-4704.","martijn.v.groningen","NULL","1","decision","0","0","0","0","1"
"1439","1439","35602","3474","Hi David, I just committed LUCENE-4704. Query instances returned from JoinUtil will implement equals and hashcode in future versions.","Query instances returned from JoinUtil will implement equals and hashcode in future versions.","martijn.v.groningen","NULL","1","alternative","0","1","0","0","0"
"1440","1440","35603","3474","Thanks alot!
I'll try the patch when 4.2 get's released.","Thanks alot!","davidvdd","NULL","0",NULL,"0","0","0","0","0"
"1441","1441","35603","3474","Thanks alot!
I'll try the patch when 4.2 get's released.","I'll try the patch when 4.2 get's released.","davidvdd","NULL","0",NULL,"0","0","0","0","0"
"1442","1442","35715","3500","The problem is this guy is configurable via IndexWriterConfig too. Maybe the IWConfig getter/setter here
should also be pkg-private?
This seems pretty expert to expose publicly? We could then also make ThreadAffinityDWPTpool pkg-private too.","The problem is this guy is configurable via IndexWriterConfig too.","rcmuir","NULL","1","con","0","0","0","1","0"
"1443","1443","35715","3500","The problem is this guy is configurable via IndexWriterConfig too. Maybe the IWConfig getter/setter here
should also be pkg-private?
This seems pretty expert to expose publicly? We could then also make ThreadAffinityDWPTpool pkg-private too.","Maybe the IWConfig getter/setter here
should also be pkg-private?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1444","1444","35715","3500","The problem is this guy is configurable via IndexWriterConfig too. Maybe the IWConfig getter/setter here
should also be pkg-private?
This seems pretty expert to expose publicly? We could then also make ThreadAffinityDWPTpool pkg-private too.","This seems pretty expert to expose publicly?","rcmuir","NULL","1","con","0","0","0","1","0"
"1445","1445","35715","3500","The problem is this guy is configurable via IndexWriterConfig too. Maybe the IWConfig getter/setter here
should also be pkg-private?
This seems pretty expert to expose publicly? We could then also make ThreadAffinityDWPTpool pkg-private too.","We could then also make ThreadAffinityDWPTpool pkg-private too.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1446","1446","35716","3500","I made getAndLock pkg-private (since its effectively is anyway, does no harm),
in r1329024 to fix the broken javadocs links... but we should figure out what we want to do here
for a real fix.","I made getAndLock pkg-private (since its effectively is anyway, does no harm),
in r1329024 to fix the broken javadocs links... but we should figure out what we want to do here
for a real fix.","rcmuir","NULL","1","issue, alternative","1","1","0","0","0"
"1447","1447","35717","3500","How about simply removing this from IWC?
It's such an insanely expert thing that anyone wanting to experiment with it can modify Lucene's sources (and submit a patch back if the results are good)...","How about simply removing this from IWC?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1448","1448","35717","3500","How about simply removing this from IWC?
It's such an insanely expert thing that anyone wanting to experiment with it can modify Lucene's sources (and submit a patch back if the results are good)...","It's such an insanely expert thing that anyone wanting to experiment with it can modify Lucene's sources (and submit a patch back if the results are good)...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1449","1449","35718","3500","Patch, just making the IWC set/get package private, and fixing LTC to use reflection to gain access...","Patch, just making the IWC set/get package private, and fixing LTC to use reflection to gain access...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1450","1450","35719","3500","New patch, making a few more classes package private...","New patch, making a few more classes package private...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1451","1451","35720","3500","+1","+1","rcmuir","NULL","1","pro","0","0","1","0","0"
"1452","1452","35721","3501","My first question is, what about backward compatibility requirements? The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.
If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat? APIs are difficult enough
as-is just for java consumers.
This seems similar to the java serialization issue (which we removed for this reason). 
Can't the serialization be totally independent?","My first question is, what about backward compatibility requirements?","rcmuir","NULL","1","issue","1","0","0","0","0"
"1453","1453","35721","3501","My first question is, what about backward compatibility requirements? The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.
If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat? APIs are difficult enough
as-is just for java consumers.
This seems similar to the java serialization issue (which we removed for this reason). 
Can't the serialization be totally independent?","The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.","rcmuir","NULL","1","con","0","0","0","1","0"
"1454","1454","35721","3501","My first question is, what about backward compatibility requirements? The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.
If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat? APIs are difficult enough
as-is just for java consumers.
This seems similar to the java serialization issue (which we removed for this reason). 
Can't the serialization be totally independent?","If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat?","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1455","1455","35721","3501","My first question is, what about backward compatibility requirements? The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.
If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat? APIs are difficult enough
as-is just for java consumers.
This seems similar to the java serialization issue (which we removed for this reason). 
Can't the serialization be totally independent?","APIs are difficult enough
as-is just for java consumers.","rcmuir","NULL","1","con","0","0","0","1","0"
"1456","1456","35721","3501","My first question is, what about backward compatibility requirements? The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.
If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat? APIs are difficult enough
as-is just for java consumers.
This seems similar to the java serialization issue (which we removed for this reason). 
Can't the serialization be totally independent?","This seems similar to the java serialization issue (which we removed for this reason).","rcmuir","NULL","1","issue","1","0","0","0","0"
"1457","1457","35721","3501","My first question is, what about backward compatibility requirements? The problem
is if people start using such a structure ('remote api') that depends upon
the structure of our java source code, then they will be upset if we break it.
If we totally change a Query's API, does that push all the responsibility of the
API designer to deal with serialization backwards compat? APIs are difficult enough
as-is just for java consumers.
This seems similar to the java serialization issue (which we removed for this reason). 
Can't the serialization be totally independent?","Can't the serialization be totally independent?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1458","1458","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Now you've got me thinking.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1459","1459","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1460","1460","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","It looks at the public API of constructors, getters, and setters.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1461","1461","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?","bmargulies","NULL","1","pro","0","0","1","0","0"
"1462","1462","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Second, this is not about long-term persistence.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1463","1463","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","I'd be happy to document it as 'do not expect to move one of these across versions.'","bmargulies","NULL","1","pro","0","0","1","0","0"
"1464","1464","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","It's most obvious application is to move arbitrary queries around in SolrCloud.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1465","1465","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","However, thirdly, you want it independent?","bmargulies","NULL","1","issue","1","0","0","0","0"
"1466","1466","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","I can make it independent.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1467","1467","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Here's how.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1468","1468","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1469","1469","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1470","1470","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1471","1471","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Want to re-wrangle the query classes?","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1472","1472","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1473","1473","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1474","1474","35722","3501","Now you've got me thinking. Several points:
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
Second, this is not about long-term persistence. I'd be happy to document it as 'do not expect to move one of these across versions.' It's most obvious application is to move arbitrary queries around in SolrCloud.
However, thirdly, you want it independent? I can make it independent. Here's how. Let's assume that you are willing to state the same sort of invariants for a query object that are embodied in parser support. Once you make a parser for something, you've promised that it has (at least) a particular set of inputs until the next major version, yes?
So, for each Query object class, make a Jackson mixin class, that maps the 'official inputs' to the current collection of setters/constructor arguments. Want to re-wrangle the query classes? If you actually change the constructors/setters that you would use in a query parser for a particular query, you then have to change the mixin class.
If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1475","1475","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1476","1476","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","It looks at the public API of constructors, getters, and setters.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1477","1477","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?","rcmuir","NULL","1","pro","0","0","1","0","0"
"1478","1478","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1479","1479","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","*/
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g.","rcmuir","NULL","1","con","0","0","0","1","0"
"1480","1480","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1481","1481","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","I'm just bringing this up as a counterexample...","rcmuir","NULL","1","con","0","0","0","1","0"
"1482","1482","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization.","rcmuir","NULL","1","con","0","0","0","1","0"
"1483","1483","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1484","1484","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","Are these annotations in java 6 API?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1485","1485","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","Annotations are no different than other pieces of code, they are an additional
responsibility.","rcmuir","NULL","1","con","0","0","0","1","0"
"1486","1486","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1487","1487","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct?","rcmuir","NULL","1","con","0","0","0","1","0"
"1488","1488","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1489","1489","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","Don't let me get in your way: I'm just the devil's advocate when i hear serialization.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1490","1490","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them.","rcmuir","NULL","1","con","0","0","0","1","0"
"1491","1491","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created.","rcmuir","NULL","1","con","0","0","0","1","0"
"1492","1492","35723","3501","
First, Jackson is far, far, less implementation-sensitive than the built-in Java serialization. It looks at the public API of constructors, getters, and setters. How likely are you to change the API in a way that invalidates the idea that a TermQuery consists of a term name and a value?
TermQuery currently has a bogus constructor today:


/** Expert: constructs a TermQuery that will use the
  *  provided docFreq instead of looking up the docFreq
  *  against the searcher. */
public TermQuery(Term t, int docFreq) {


Its bogus because some scoring models may not use docFreq at all (e.g. language modelling): I know why it exists (for one contrib query), I think we have a plan to fix it, but its just not yet done... like many things 
But thats an example of one I think is fair to remove in a minor release: besides being bogus, its documented as Expert, 
so its fair game.
I'm just bringing this up as a counterexample... I thought serialization was harmless before myself
until I tried to make useful changes (like yanking vector-space model out of the scoring system) and was
totally blocked by serialization. So I'm gonna be suspicious of the word no matter what 

If it were up to me, I'd start by annotating the actually classes themselves with Jackson @nnotations for this purpose, and only create mixins when and if an incompatible change happened, but I'm not really opinionated.
Are these annotations in java 6 API? Annotations are no different than other pieces of code, they are an additional
responsibility. As far as @Deprecated, we fail the build if we add @deprecated javadocs but forget it, as far as
@Override, well nothing automated yet, but we should be somehow enforcing that as well. Besides the fact we don't
want to add any dependencies to the lucene core, how would we know such annotations were correct? 

Would it be sensible to start to concoct a contrib module with all the jackson in it, accompanied by benign 'add a few getters and setters' classes to the queries?
Don't let me get in your way: I'm just the devil's advocate when i hear serialization. As far as adding getters and setters
to queries I'm not sure if they are benign or not without us looking at them. For example, AutomatonQuery and all of its
subclasses: Wildcard, Regexp, etc are totally immutable once created. This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","This is for good reasons so I don't think we should
add any setters to these classes, and the stuff it stores internally shouldn't be accessible via a getter.","rcmuir","NULL","1","pro, con","0","0","1","1","0"
"1493","1493","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","Rob,
These are not Java annotations, they are specific to Jackson.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1494","1494","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","(Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.)","bmargulies","NULL","1","pro, con","0","0","1","1","0"
"1495","1495","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene.","bmargulies","NULL","1","con","0","0","0","1","0"
"1496","1496","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","If I stick to the mixin approach, no annotation are needed at all.","bmargulies","NULL","1","alternative, pro","0","1","1","0","0"
"1497","1497","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","As for the rest, I think that we agree.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1498","1498","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax.","bmargulies","NULL","1","alternative, pro","0","1","1","0","0"
"1499","1499","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","(though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1500","1500","35724","3501","Rob,
These are not Java annotations, they are specific to Jackson. (Though Jackson will pay attention to the JAX-B annotations in Java 6, I fear that this is a cure worse than the disease.) So hanging them on the real classes makes for a hard dependency on Jackson to compile, and I'm not sure myself that this is a good idea in the middle of Lucene. If I stick to the mixin approach, no annotation are needed at all.
As for the rest, I think that we agree. The point of the annotations is to only serialize the stuff that makes up the plausible, stable, sort of attributes that you would support in a parser syntax. (though I suppose someone might want to SolrCloudify some really expert complexity), and the selective annotation avoids the rest.
So I'm going to sketch something out and we'll all see what it looks like.","So I'm going to sketch something out and we'll all see what it looks like.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1501","1501","35725","3501","Yeah a sketch (maybe just Term and Boolean or something ?) would be cool, maybe my paranoia
is unjustified... then we could see what it actually would need to look like and think about
the backwards-compatibility/API costs etc would be.","Yeah a sketch (maybe just Term and Boolean or something ?)","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1502","1502","35725","3501","Yeah a sketch (maybe just Term and Boolean or something ?) would be cool, maybe my paranoia
is unjustified... then we could see what it actually would need to look like and think about
the backwards-compatibility/API costs etc would be.","would be cool, maybe my paranoia
is unjustified... then we could see what it actually would need to look like and think about
the backwards-compatibility/API costs etc would be.","rcmuir","NULL","1","issue, alternative, pro","1","1","1","0","0"
"1503","1503","35726","3501","Add accessor to BooleanQuery.java to allow serializing 'disable-coord'.","Add accessor to BooleanQuery.java to allow serializing 'disable-coord'.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1504","1504","35727","3501","My only comment on that: it would help perpetuate this inverted 'disable/omit' stuff
that i just cant stand. I really wish BQ had 'enableCoord' in the ctor with true as
the default: of course a horrible thing to try to fix but we were able to fix this
for omitTF etc so I'm not yet losing hope. 
","My only comment on that: it would help perpetuate this inverted 'disable/omit' stuff
that i just cant stand.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1505","1505","35727","3501","My only comment on that: it would help perpetuate this inverted 'disable/omit' stuff
that i just cant stand. I really wish BQ had 'enableCoord' in the ctor with true as
the default: of course a horrible thing to try to fix but we were able to fix this
for omitTF etc so I'm not yet losing hope. 
","I really wish BQ had 'enableCoord' in the ctor with true as
the default: of course a horrible thing to try to fix but we were able to fix this
for omitTF etc so I'm not yet losing hope.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1506","1506","35728","3501","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works. No QP yet, that's the easy part.
Apologies for the Maven project, but that's what I do quickly.
No patches to core required at all yet. if you run the unit test, you will see what the json looks like. Concise, no surprise, it's not.","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1507","1507","35728","3501","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works. No QP yet, that's the easy part.
Apologies for the Maven project, but that's what I do quickly.
No patches to core required at all yet. if you run the unit test, you will see what the json looks like. Concise, no surprise, it's not.","No QP yet, that's the easy part.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1508","1508","35728","3501","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works. No QP yet, that's the easy part.
Apologies for the Maven project, but that's what I do quickly.
No patches to core required at all yet. if you run the unit test, you will see what the json looks like. Concise, no surprise, it's not.","Apologies for the Maven project, but that's what I do quickly.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1509","1509","35728","3501","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works. No QP yet, that's the easy part.
Apologies for the Maven project, but that's what I do quickly.
No patches to core required at all yet. if you run the unit test, you will see what the json looks like. Concise, no surprise, it's not.","No patches to core required at all yet.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1510","1510","35728","3501","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works. No QP yet, that's the easy part.
Apologies for the Maven project, but that's what I do quickly.
No patches to core required at all yet. if you run the unit test, you will see what the json looks like. Concise, no surprise, it's not.","if you run the unit test, you will see what the json looks like.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1511","1511","35728","3501","git://github.com/bimargulies/lucene-json-qp.git
Rob, here you will find the promised look at how this works. No QP yet, that's the easy part.
Apologies for the Maven project, but that's what I do quickly.
No patches to core required at all yet. if you run the unit test, you will see what the json looks like. Concise, no surprise, it's not.","Concise, no surprise, it's not.","bmargulies","NULL","1","con","0","0","0","1","0"
"1512","1512","35729","3501","The query integration at a glance looks very non-invasive from my perspective!
I think we should pursue this? We might run into some tricky parts but we could
have this component as a separate pluggable module with probably minimal changes
to the core Query apis right?","The query integration at a glance looks very non-invasive from my perspective!","rcmuir","NULL","1","pro","0","0","1","0","0"
"1513","1513","35729","3501","The query integration at a glance looks very non-invasive from my perspective!
I think we should pursue this? We might run into some tricky parts but we could
have this component as a separate pluggable module with probably minimal changes
to the core Query apis right?","I think we should pursue this?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1514","1514","35729","3501","The query integration at a glance looks very non-invasive from my perspective!
I think we should pursue this? We might run into some tricky parts but we could
have this component as a separate pluggable module with probably minimal changes
to the core Query apis right?","We might run into some tricky parts but we could
have this component as a separate pluggable module with probably minimal changes
to the core Query apis right?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1515","1515","35730","3501","Right. The tiny patch is an example of the sort of change, and I think it's always good to have 'getters'. 
In fact, I don't even need that patch for the code to work, but i thought it was a good idea.
I'll continue to set this up. Let me know when/how/if you want it shaped to go into the tree somewhere.","Right.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1516","1516","35730","3501","Right. The tiny patch is an example of the sort of change, and I think it's always good to have 'getters'. 
In fact, I don't even need that patch for the code to work, but i thought it was a good idea.
I'll continue to set this up. Let me know when/how/if you want it shaped to go into the tree somewhere.","The tiny patch is an example of the sort of change, and I think it's always good to have 'getters'.","bmargulies","NULL","1","alternative, pro","0","1","1","0","0"
"1517","1517","35730","3501","Right. The tiny patch is an example of the sort of change, and I think it's always good to have 'getters'. 
In fact, I don't even need that patch for the code to work, but i thought it was a good idea.
I'll continue to set this up. Let me know when/how/if you want it shaped to go into the tree somewhere.","In fact, I don't even need that patch for the code to work, but i thought it was a good idea.","bmargulies","NULL","1","pro","0","0","1","0","0"
"1518","1518","35730","3501","Right. The tiny patch is an example of the sort of change, and I think it's always good to have 'getters'. 
In fact, I don't even need that patch for the code to work, but i thought it was a good idea.
I'll continue to set this up. Let me know when/how/if you want it shaped to go into the tree somewhere.","I'll continue to set this up.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1519","1519","35730","3501","Right. The tiny patch is an example of the sort of change, and I think it's always good to have 'getters'. 
In fact, I don't even need that patch for the code to work, but i thought it was a good idea.
I'll continue to set this up. Let me know when/how/if you want it shaped to go into the tree somewhere.","Let me know when/how/if you want it shaped to go into the tree somewhere.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1520","1520","35731","3501","I think if it works for you, just iterate in your github and ping the issue when you make progress?
Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.
As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here. One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g. xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).
And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","I think if it works for you, just iterate in your github and ping the issue when you make progress?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1521","1521","35731","3501","I think if it works for you, just iterate in your github and ping the issue when you make progress?
Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.
As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here. One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g. xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).
And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1522","1522","35731","3501","I think if it works for you, just iterate in your github and ping the issue when you make progress?
Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.
As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here. One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g. xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).
And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1523","1523","35731","3501","I think if it works for you, just iterate in your github and ping the issue when you make progress?
Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.
As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here. One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g. xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).
And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1524","1524","35731","3501","I think if it works for you, just iterate in your github and ping the issue when you make progress?
Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.
As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here. One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g. xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).
And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1525","1525","35731","3501","I think if it works for you, just iterate in your github and ping the issue when you make progress?
Otherwise we worry too much about where/how the code sits when that isn't really so important
at this stage.
As far as final integration, I think there are a number of ways to do it but its really
unrelated to making progress here. One suggestion might be to split queryparser/ module
like analyzers/ so we have:

classic/ [including things based on it: complexPhrase, ext, analyzing]
flexible/ [including precedence which is based on it]
xml/
json/

This could probably make things less confusing, as currently queryparser/ is a mix of
different frameworks with different dependencies (e.g. xml depends on queries/ and sandbox/,
but the others dont, and json will depend on jackson and maybe other stuff, etc, etc).
And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","And then finally, probably a followup issue to do solr integration (i'm more fuzzy on that).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1526","1526","37007","3625","Lots of tests write to std streams currently (solr). We could do that fairly easily (in a number of ways) but it'll break a number of tests.","Lots of tests write to std streams currently (solr).","dweiss","NULL","1","issue","1","0","0","0","0"
"1527","1527","37007","3625","Lots of tests write to std streams currently (solr). We could do that fairly easily (in a number of ways) but it'll break a number of tests.","We could do that fairly easily (in a number of ways) but it'll break a number of tests.","dweiss","NULL","1","pro, con","0","0","1","1","0"
"1528","1528","37008","3625","I think it's fine if tests write to the std streams, but not core Lucene code (lucene/core/src/java/*)?","I think it's fine if tests write to the std streams, but not core Lucene code (lucene/core/src/java/*)?","mikemccand","NULL","1","pro, con","0","0","1","1","0"
"1529","1529","37009","3625","This is possible by verifying where System.out takes place at runtime via stack analysis. Alternatively a bytecode woven aspect. But at the same time it could also be a find-and-grep over sources? Comments would have to be removed in the pipeline prior to grepping for sysouts.
Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.","This is possible by verifying where System.out takes place at runtime via stack analysis.","dweiss","NULL","1","alternative","0","1","0","0","0"
"1530","1530","37009","3625","This is possible by verifying where System.out takes place at runtime via stack analysis. Alternatively a bytecode woven aspect. But at the same time it could also be a find-and-grep over sources? Comments would have to be removed in the pipeline prior to grepping for sysouts.
Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.","Alternatively a bytecode woven aspect.","dweiss","NULL","1","alternative","0","1","0","0","0"
"1531","1531","37009","3625","This is possible by verifying where System.out takes place at runtime via stack analysis. Alternatively a bytecode woven aspect. But at the same time it could also be a find-and-grep over sources? Comments would have to be removed in the pipeline prior to grepping for sysouts.
Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.","But at the same time it could also be a find-and-grep over sources?","dweiss","NULL","1","alternative","0","1","0","0","0"
"1532","1532","37009","3625","This is possible by verifying where System.out takes place at runtime via stack analysis. Alternatively a bytecode woven aspect. But at the same time it could also be a find-and-grep over sources? Comments would have to be removed in the pipeline prior to grepping for sysouts.
Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.","Comments would have to be removed in the pipeline prior to grepping for sysouts.","dweiss","NULL","1","alternative","0","1","0","0","0"
"1533","1533","37009","3625","This is possible by verifying where System.out takes place at runtime via stack analysis. Alternatively a bytecode woven aspect. But at the same time it could also be a find-and-grep over sources? Comments would have to be removed in the pipeline prior to grepping for sysouts.
Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.","Checking all sysouts/syserrs and verifying stack traces there seems like an overkill compared to the above.","dweiss","NULL","1","con","0","0","0","1","0"
"1534","1534","37010","3625","findbugs? checkstyle?","findbugs?","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1535","1535","37010","3625","findbugs? checkstyle?","checkstyle?","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1536","1536","37011","3625","Don't we now fail if there is a noncommit in the src? Can't we use the same mechanism?","Don't we now fail if there is a noncommit in the src?","markrmiller@gmail.com","NULL","1","alternative","0","1","0","0","0"
"1537","1537","37011","3625","Don't we now fail if there is a noncommit in the src? Can't we use the same mechanism?","Can't we use the same mechanism?","markrmiller@gmail.com","NULL","1","alternative","0","1","0","0","0"
"1538","1538","37012","3625","Some core code legitimately uses System.out.println, e.g. CheckIndex.
However on review, some of its use cases should actually be System.err...","Some core code legitimately uses System.out.println, e.g.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1539","1539","37012","3625","Some core code legitimately uses System.out.println, e.g. CheckIndex.
However on review, some of its use cases should actually be System.err...","CheckIndex.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1540","1540","37012","3625","Some core code legitimately uses System.out.println, e.g. CheckIndex.
However on review, some of its use cases should actually be System.err...","However on review, some of its use cases should actually be System.err...","rcmuir","NULL","1","issue","1","0","0","0","0"
"1541","1541","37015","3625","I removed the std prints in lucene/core/src/java that I could find on quick grepping.
I'll leave this open so we can somehow automatically catch this...","I removed the std prints in lucene/core/src/java that I could find on quick grepping.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1542","1542","37015","3625","I removed the std prints in lucene/core/src/java that I could find on quick grepping.
I'll leave this open so we can somehow automatically catch this...","I'll leave this open so we can somehow automatically catch this...","mikemccand","NULL","1","decision","0","0","0","0","1"
"1543","1543","37016","3625","I took a guess that one way would be to replace the System IO streams with ones that will throw exceptions.
It requires cglib, and might (due to the abuse of sun.reflect.Reflection to get the callee efficiently) be sun hotspot specific.
I guess this is not perfect, it would only error out if the code is called
If there is thought to this being a good thing I could look into how to wire it up to the unit-tests ","I took a guess that one way would be to replace the System IO streams with ones that will throw exceptions.","gbowyer@fastmail.co.uk","NULL","1","alternative","0","1","0","0","0"
"1544","1544","37016","3625","I took a guess that one way would be to replace the System IO streams with ones that will throw exceptions.
It requires cglib, and might (due to the abuse of sun.reflect.Reflection to get the callee efficiently) be sun hotspot specific.
I guess this is not perfect, it would only error out if the code is called
If there is thought to this being a good thing I could look into how to wire it up to the unit-tests ","It requires cglib, and might (due to the abuse of sun.reflect.Reflection to get the callee efficiently) be sun hotspot specific.","gbowyer@fastmail.co.uk","NULL","1","alternative","0","1","0","0","0"
"1545","1545","37016","3625","I took a guess that one way would be to replace the System IO streams with ones that will throw exceptions.
It requires cglib, and might (due to the abuse of sun.reflect.Reflection to get the callee efficiently) be sun hotspot specific.
I guess this is not perfect, it would only error out if the code is called
If there is thought to this being a good thing I could look into how to wire it up to the unit-tests ","I guess this is not perfect, it would only error out if the code is called
If there is thought to this being a good thing I could look into how to wire it up to the unit-tests","gbowyer@fastmail.co.uk","NULL","1","con","0","0","0","1","0"
"1546","1546","37017","3625","Helps if I upload the right version","Helps if I upload the right version","gbowyer@fastmail.co.uk","NULL","1","pro","0","0","1","0","0"
"1547","1547","37018","3625","You can just as well substitute your own implementation of PrintStream using System.setOut/setErr and check stacks on printlns... But I agree with Benson that a static analysis approach is much cleaner. Don't know if there's anything out of the box in findbugs/ pmd, but even if not then this can be done as a 10-liner by applying an aspect to classes via aspectj and parsing the output logs detecting if an aspect has been applied (it shouldn't match anywhere). ","You can just as well substitute your own implementation of PrintStream using System.setOut/setErr and check stacks on printlns...","dweiss","NULL","1","alternative","0","1","0","0","0"
"1548","1548","37018","3625","You can just as well substitute your own implementation of PrintStream using System.setOut/setErr and check stacks on printlns... But I agree with Benson that a static analysis approach is much cleaner. Don't know if there's anything out of the box in findbugs/ pmd, but even if not then this can be done as a 10-liner by applying an aspect to classes via aspectj and parsing the output logs detecting if an aspect has been applied (it shouldn't match anywhere). ","But I agree with Benson that a static analysis approach is much cleaner.","dweiss","NULL","1","pro","0","0","1","0","0"
"1549","1549","37018","3625","You can just as well substitute your own implementation of PrintStream using System.setOut/setErr and check stacks on printlns... But I agree with Benson that a static analysis approach is much cleaner. Don't know if there's anything out of the box in findbugs/ pmd, but even if not then this can be done as a 10-liner by applying an aspect to classes via aspectj and parsing the output logs detecting if an aspect has been applied (it shouldn't match anywhere). ","Don't know if there's anything out of the box in findbugs/ pmd, but even if not then this can be done as a 10-liner by applying an aspect to classes via aspectj and parsing the output logs detecting if an aspect has been applied (it shouldn't match anywhere).","dweiss","NULL","1","alternative, pro, con","0","1","1","1","0"
"1550","1550","37019","3625","Thats a good point and started bugging me last night when I was thinking about it.
As as result attached is a static analysis version that will hate any GETSTATIC java/lang/System::(err|out) (I briefly looked at findbugs and did not find a working version for this idea) 
This one uses ASM 3.3
Its dumb output looks like 
/opt/sun-jdk-1.7.0/bin/java -classpath /tmp/test.jar:/tmp/out/production:/tmp/asm-all-3.2.jar SystemPrintCheck /tmp/test.jar
SystemPrintCheck$SystemOutMethodVisitor.visitMethodInsn @ SystemPrintCheck.java +42
SystemPrintCheck.main @ SystemPrintCheck.java +102
Process finished with exit code 1","Thats a good point and started bugging me last night when I was thinking about it.","gbowyer@fastmail.co.uk","NULL","1","pro","0","0","1","0","0"
"1551","1551","37019","3625","Thats a good point and started bugging me last night when I was thinking about it.
As as result attached is a static analysis version that will hate any GETSTATIC java/lang/System::(err|out) (I briefly looked at findbugs and did not find a working version for this idea) 
This one uses ASM 3.3
Its dumb output looks like 
/opt/sun-jdk-1.7.0/bin/java -classpath /tmp/test.jar:/tmp/out/production:/tmp/asm-all-3.2.jar SystemPrintCheck /tmp/test.jar
SystemPrintCheck$SystemOutMethodVisitor.visitMethodInsn @ SystemPrintCheck.java +42
SystemPrintCheck.main @ SystemPrintCheck.java +102
Process finished with exit code 1","As as result attached is a static analysis version that will hate any GETSTATIC java/lang/System::(err|out) (I briefly looked at findbugs and did not find a working version for this idea) 
This one uses ASM 3.3
Its dumb output looks like 
/opt/sun-jdk-1.7.0/bin/java -classpath /tmp/test.jar:/tmp/out/production:/tmp/asm-all-3.2.jar SystemPrintCheck /tmp/test.jar
SystemPrintCheck$SystemOutMethodVisitor.visitMethodInsn @ SystemPrintCheck.java +42
SystemPrintCheck.main @ SystemPrintCheck.java +102
Process finished with exit code 1","gbowyer@fastmail.co.uk","NULL","1","alternative","0","1","0","0","0"
"1552","1552","37020","3625","ASM static test version that dislikes System.out / System.err","ASM static test version that dislikes System.out / System.err","gbowyer@fastmail.co.uk","NULL","1","alternative, con","0","1","0","1","0"
"1553","1553","37021","3625","fyi. PMD has a rule for this  SystemPrintln.
http://pmd.sourceforge.net/rules/index.html
Didn't check the details though.","fyi.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"1554","1554","37021","3625","fyi. PMD has a rule for this  SystemPrintln.
http://pmd.sourceforge.net/rules/index.html
Didn't check the details though.","PMD has a rule for this  SystemPrintln.","dweiss","NULL","1","alternative","0","1","0","0","0"
"1555","1555","37021","3625","fyi. PMD has a rule for this  SystemPrintln.
http://pmd.sourceforge.net/rules/index.html
Didn't check the details though.","http://pmd.sourceforge.net/rules/index.html
Didn't check the details though.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"1556","1556","37022","3625","Interesting I didnt look at PMD although that is more down to my personal dislike of code lint tools that do source code analysis
shrugs It was a fun distraction anyhow","Interesting I didnt look at PMD although that is more down to my personal dislike of code lint tools that do source code analysis
shrugs It was a fun distraction anyhow","gbowyer@fastmail.co.uk","NULL","1","pro","0","0","1","0","0"
"1557","1557","37024","3625","Well would it sway the argument if I said that the ASM code near directly translates into a findbugs rule (I have done this before)
I was also not wanting to suggest findbug custom rules at the start because thats a bigger change in including an entire code lint tool (unless its included with lucene already, in which case forgive my stupidity I am still finding my way around)
Something doesn't fit right in my mind with the AspectJ approach, I have seen it not work in the past for obscure reasons and it feels that running the weaving in pretend to check the verbose output is not much further on from checking the source code in the first place.","Well would it sway the argument if I said that the ASM code near directly translates into a findbugs rule (I have done this before)
I was also not wanting to suggest findbug custom rules at the start because thats a bigger change in including an entire code lint tool (unless its included with lucene already, in which case forgive my stupidity I am still finding my way around)
Something doesn't fit right in my mind with the AspectJ approach, I have seen it not work in the past for obscure reasons and it feels that running the weaving in pretend to check the verbose output is not much further on from checking the source code in the first place.","gbowyer@fastmail.co.uk","NULL","1","alternative, con","0","1","0","1","0"
"1558","1558","37027","3625","Oh, btw. I think a FindBugs rule for detecting sysouts/syserrs would be a great addition to FindBugs  you should definitely file it as an improvement there. In reality at least class-level exclusions will be needed to avoid legitimate matches like the ones shown above (main methods, exception handlers), but these can be lived with.","Oh, btw.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"1559","1559","37027","3625","Oh, btw. I think a FindBugs rule for detecting sysouts/syserrs would be a great addition to FindBugs  you should definitely file it as an improvement there. In reality at least class-level exclusions will be needed to avoid legitimate matches like the ones shown above (main methods, exception handlers), but these can be lived with.","I think a FindBugs rule for detecting sysouts/syserrs would be a great addition to FindBugs  you should definitely file it as an improvement there.","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"1560","1560","37027","3625","Oh, btw. I think a FindBugs rule for detecting sysouts/syserrs would be a great addition to FindBugs  you should definitely file it as an improvement there. In reality at least class-level exclusions will be needed to avoid legitimate matches like the ones shown above (main methods, exception handlers), but these can be lived with.","In reality at least class-level exclusions will be needed to avoid legitimate matches like the ones shown above (main methods, exception handlers), but these can be lived with.","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"1561","1561","37028","3625","Issue is marked 3.6 and actively being discussed but has no assignee - assigning to most active committer contributing patches/discussion so far to triage wether this can/should be pushed to 4.0 or not.","Issue is marked 3.6 and actively being discussed but has no assignee - assigning to most active committer contributing patches/discussion so far to triage wether this can/should be pushed to 4.0 or not.","hossman","NULL","1","issue","1","0","0","0","0"
"1562","1562","37029","3625","I'd push it to 4.0 (automation in whatever form).","I'd push it to 4.0 (automation in whatever form).","dweiss","NULL","1","alternative","0","1","0","0","0"
"1563","1563","37030","3625","Yeah sorry I have been busy at work for a spell, I was going to craft it into a findbugs rule and see if I can get accepted into the findbugs project,
Just been a bit tied up thats all","Yeah sorry I have been busy at work for a spell, I was going to craft it into a findbugs rule and see if I can get accepted into the findbugs project,
Just been a bit tied up thats all","gbowyer@fastmail.co.uk","NULL","0",NULL,"0","0","0","0","0"
"1564","1564","37031","3625","No worries Greg, really. For 3.x I think manual check will do (or what I've done above with AspectJ). For 4.x it'd be nice to have findbugs lint anyway (for this and other issues). It'll most likely require some rules tuning too, so it can be a separate issue.","No worries Greg, really.","dweiss","NULL","0",NULL,"0","0","0","0","0"
"1565","1565","37031","3625","No worries Greg, really. For 3.x I think manual check will do (or what I've done above with AspectJ). For 4.x it'd be nice to have findbugs lint anyway (for this and other issues). It'll most likely require some rules tuning too, so it can be a separate issue.","For 3.x I think manual check will do (or what I've done above with AspectJ).","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"1566","1566","37031","3625","No worries Greg, really. For 3.x I think manual check will do (or what I've done above with AspectJ). For 4.x it'd be nice to have findbugs lint anyway (for this and other issues). It'll most likely require some rules tuning too, so it can be a separate issue.","For 4.x it'd be nice to have findbugs lint anyway (for this and other issues).","dweiss","NULL","1","alternative, pro","0","1","1","0","0"
"1567","1567","37031","3625","No worries Greg, really. For 3.x I think manual check will do (or what I've done above with AspectJ). For 4.x it'd be nice to have findbugs lint anyway (for this and other issues). It'll most likely require some rules tuning too, so it can be a separate issue.","It'll most likely require some rules tuning too, so it can be a separate issue.","dweiss","NULL","1","issue","1","0","0","0","0"
"1568","1568","37032","3625","I think if we fix LUCENE-4202, we could just have a separate task for this that only runs on a fileset of non-test code? We could also exclude some tools like CheckIndex from it.
We could add things like:
System#in
System#out
System#err
Throwable#printStackTrace() <-- eclipse stupid-stubs
...","I think if we fix LUCENE-4202, we could just have a separate task for this that only runs on a fileset of non-test code?","rcmuir","NULL","1","issue, alternative","1","1","0","0","0"
"1569","1569","37032","3625","I think if we fix LUCENE-4202, we could just have a separate task for this that only runs on a fileset of non-test code? We could also exclude some tools like CheckIndex from it.
We could add things like:
System#in
System#out
System#err
Throwable#printStackTrace() <-- eclipse stupid-stubs
...","We could also exclude some tools like CheckIndex from it.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1570","1570","37032","3625","I think if we fix LUCENE-4202, we could just have a separate task for this that only runs on a fileset of non-test code? We could also exclude some tools like CheckIndex from it.
We could add things like:
System#in
System#out
System#err
Throwable#printStackTrace() <-- eclipse stupid-stubs
...","We could add things like:
System#in
System#out
System#err
Throwable#printStackTrace() <-- eclipse stupid-stubs
...","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1571","1571","37033","3625","Here's a patch implementing this check with LUCENE-4202. It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.
Current list looks like:

check-system-out:
[forbidden-apis] Reading inline API signatures...
[forbidden-apis] Reading API signatures: /home/rmuir/workspace/lucene-trunk/lucene/tools/forbiddenApis/system-out.txt
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.HyphenationTree (HyphenationTree.java:467)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:408)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:412)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:416)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:637)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:638)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:640)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:658)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:659)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:660)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:292)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:302)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:312)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:326)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:356)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:366)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:376)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:386)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:395)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:404)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:413)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:529)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:534)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:542)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:314)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:320)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:382)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSetExceptions (RSLPStemmerBase.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSuffixExceptions (RSLPStemmerBase.java:159)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.tartarus.snowball.SnowballProgram (SnowballProgram.java:438)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:69)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:71)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:73)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Compile (Compile.java:126)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:107)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:301)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:303)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Trie (Trie.java:379)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:95)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:102)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:106)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:127)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:129)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:134)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ContentItemsSource (ContentItemsSource.java:178)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser (DemoHTMLParser.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker (EnwikiQueryMaker.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:84)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:93)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker (ReutersQueryMaker.java:90)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:195)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:204)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:227)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:242)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:243)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.HTMLParserTokenManager (HTMLParserTokenManager.java:12)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:35)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:37)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.programmatic.Sample (Sample.java:72)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseReaderTask (CloseReaderTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseTaxonomyReaderTask (CloseTaxonomyReaderTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:181)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:183)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:108)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask (NewAnalyzerTask.java:81)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewCollationAnalyzerTask (NewCollationAnalyzerTask.java:88)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewLocaleTask (NewLocaleTask.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewShingleAnalyzerTask (NewShingleAnalyzerTask.java:83)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:138)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PrintReaderTask (PrintReaderTask.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:140)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:141)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:142)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:146)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:121)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:307)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:308)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:309)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:48)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:49)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:65)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:152)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:100)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:145)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:155)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:157)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1963)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1965)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1973)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1975)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1980)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1985)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1992)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:150)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:154)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:180)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:210)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:223)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:255)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:344)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:378)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:387)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:398)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:402)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:405)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:408)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:354)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:388)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:398)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:600)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:660)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1208)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1249)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1368)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1602)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1691)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1700)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1707)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1714)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1723)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1724)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1753)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1758)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1762)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1771)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1778)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1787)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1789)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1790)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1793)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1795)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1797)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1798)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1801)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:56)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:57)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:59)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:61)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:62)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:82)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   i","Here's a patch implementing this check with LUCENE-4202.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1572","1572","37033","3625","Here's a patch implementing this check with LUCENE-4202. It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.
Current list looks like:

check-system-out:
[forbidden-apis] Reading inline API signatures...
[forbidden-apis] Reading API signatures: /home/rmuir/workspace/lucene-trunk/lucene/tools/forbiddenApis/system-out.txt
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.HyphenationTree (HyphenationTree.java:467)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:408)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:412)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:416)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:637)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:638)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:640)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:658)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:659)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:660)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:292)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:302)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:312)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:326)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:356)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:366)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:376)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:386)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:395)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:404)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:413)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:529)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:534)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:542)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:314)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:320)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:382)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSetExceptions (RSLPStemmerBase.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSuffixExceptions (RSLPStemmerBase.java:159)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.tartarus.snowball.SnowballProgram (SnowballProgram.java:438)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:69)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:71)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:73)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Compile (Compile.java:126)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:107)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:301)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:303)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Trie (Trie.java:379)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:95)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:102)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:106)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:127)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:129)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:134)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ContentItemsSource (ContentItemsSource.java:178)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser (DemoHTMLParser.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker (EnwikiQueryMaker.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:84)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:93)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker (ReutersQueryMaker.java:90)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:195)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:204)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:227)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:242)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:243)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.HTMLParserTokenManager (HTMLParserTokenManager.java:12)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:35)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:37)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.programmatic.Sample (Sample.java:72)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseReaderTask (CloseReaderTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseTaxonomyReaderTask (CloseTaxonomyReaderTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:181)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:183)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:108)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask (NewAnalyzerTask.java:81)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewCollationAnalyzerTask (NewCollationAnalyzerTask.java:88)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewLocaleTask (NewLocaleTask.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewShingleAnalyzerTask (NewShingleAnalyzerTask.java:83)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:138)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PrintReaderTask (PrintReaderTask.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:140)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:141)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:142)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:146)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:121)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:307)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:308)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:309)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:48)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:49)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:65)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:152)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:100)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:145)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:155)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:157)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1963)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1965)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1973)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1975)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1980)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1985)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1992)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:150)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:154)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:180)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:210)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:223)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:255)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:344)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:378)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:387)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:398)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:402)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:405)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:408)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:354)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:388)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:398)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:600)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:660)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1208)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1249)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1368)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1602)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1691)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1700)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1707)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1714)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1723)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1724)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1753)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1758)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1762)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1771)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1778)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1787)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1789)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1790)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1793)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1795)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1797)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1798)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1801)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:56)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:57)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:59)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:61)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:62)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:82)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   i","It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1573","1573","37033","3625","Here's a patch implementing this check with LUCENE-4202. It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.
Current list looks like:

check-system-out:
[forbidden-apis] Reading inline API signatures...
[forbidden-apis] Reading API signatures: /home/rmuir/workspace/lucene-trunk/lucene/tools/forbiddenApis/system-out.txt
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.HyphenationTree (HyphenationTree.java:467)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:408)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:412)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:416)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:637)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:638)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:640)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:658)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:659)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:660)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:292)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:302)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:312)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:326)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:356)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:366)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:376)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:386)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:395)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:404)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:413)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:529)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:534)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:542)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:314)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:320)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:382)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSetExceptions (RSLPStemmerBase.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSuffixExceptions (RSLPStemmerBase.java:159)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.tartarus.snowball.SnowballProgram (SnowballProgram.java:438)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:69)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:71)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:73)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Compile (Compile.java:126)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:107)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:301)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:303)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Trie (Trie.java:379)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:95)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:102)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:106)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:127)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:129)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:134)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ContentItemsSource (ContentItemsSource.java:178)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser (DemoHTMLParser.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker (EnwikiQueryMaker.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:84)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:93)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker (ReutersQueryMaker.java:90)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:195)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:204)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:227)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:242)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:243)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.HTMLParserTokenManager (HTMLParserTokenManager.java:12)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:35)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:37)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.programmatic.Sample (Sample.java:72)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseReaderTask (CloseReaderTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseTaxonomyReaderTask (CloseTaxonomyReaderTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:181)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:183)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:108)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask (NewAnalyzerTask.java:81)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewCollationAnalyzerTask (NewCollationAnalyzerTask.java:88)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewLocaleTask (NewLocaleTask.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewShingleAnalyzerTask (NewShingleAnalyzerTask.java:83)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:138)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PrintReaderTask (PrintReaderTask.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:140)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:141)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:142)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:146)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:121)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:307)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:308)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:309)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:48)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:49)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:65)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:152)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:100)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:145)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:155)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:157)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1963)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1965)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1973)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1975)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1980)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1985)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1992)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:150)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:154)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:180)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:210)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:223)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:255)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:344)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:378)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:387)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:398)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:402)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:405)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:408)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:354)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:388)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:398)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:600)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:660)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1208)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1249)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1368)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1602)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1691)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1700)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1707)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1714)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1723)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1724)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1753)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1758)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1762)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1771)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1778)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1787)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1789)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1790)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1793)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1795)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1797)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1798)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1801)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:56)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:57)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:59)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:61)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:62)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:82)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   i","Current list looks like:

check-system-out:
[forbidden-apis] Reading inline API signatures...
[forbidden-apis] Reading API signatures: /home/rmuir/workspace/lucene-trunk/lucene/tools/forbiddenApis/system-out.txt
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.HyphenationTree (HyphenationTree.java:467)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:408)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:412)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:416)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:637)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:638)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:640)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:658)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:659)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:660)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:292)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:302)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:312)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:326)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:356)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:366)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:376)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:386)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:395)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:404)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:413)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:529)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:534)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:542)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:314)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:320)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:382)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSetExceptions (RSLPStemmerBase.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSuffixExceptions (RSLPStemmerBase.java:159)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.tartarus.snowball.SnowballProgram (SnowballProgram.java:438)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:69)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:71)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:73)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Compile (Compile.java:126)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:107)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:301)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:303)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Trie (Trie.java:379)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:95)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:102)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:106)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:127)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:129)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:134)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ContentItemsSource (ContentItemsSource.java:178)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser (DemoHTMLParser.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker (EnwikiQueryMaker.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:84)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:93)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker (ReutersQueryMaker.java:90)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:195)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:204)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:227)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:242)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:243)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.HTMLParserTokenManager (HTMLParserTokenManager.java:12)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:35)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:37)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.programmatic.Sample (Sample.java:72)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseReaderTask (CloseReaderTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseTaxonomyReaderTask (CloseTaxonomyReaderTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:181)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:183)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:108)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask (NewAnalyzerTask.java:81)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewCollationAnalyzerTask (NewCollationAnalyzerTask.java:88)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewLocaleTask (NewLocaleTask.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewShingleAnalyzerTask (NewShingleAnalyzerTask.java:83)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:138)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PrintReaderTask (PrintReaderTask.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:140)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:141)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:142)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:146)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:121)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:307)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:308)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:309)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:48)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:49)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:65)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:152)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:100)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:145)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:155)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:157)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1963)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1965)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1973)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1975)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1980)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1985)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1992)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:150)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:154)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:180)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:210)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:223)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:255)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:344)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:378)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:387)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:398)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:402)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:405)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:408)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:354)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:388)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:398)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:600)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:660)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1208)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1249)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1368)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1602)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1691)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1700)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1707)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1714)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1723)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1724)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1753)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1758)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1762)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1771)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1778)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1787)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1789)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1790)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1793)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1795)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1797)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1798)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1801)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:56)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:57)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:59)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:61)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:62)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:82)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:87)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1574","1574","37033","3625","Here's a patch implementing this check with LUCENE-4202. It excludes any test code, but I didnt add any exceptions for legitimate command-line tools.
Current list looks like:

check-system-out:
[forbidden-apis] Reading inline API signatures...
[forbidden-apis] Reading API signatures: /home/rmuir/workspace/lucene-trunk/lucene/tools/forbiddenApis/system-out.txt
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.HyphenationTree (HyphenationTree.java:467)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:408)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:412)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.PatternParser (PatternParser.java:416)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:637)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:638)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:640)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:658)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:659)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.compound.hyphenation.TernaryTree (TernaryTree.java:660)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:292)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:302)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:312)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:326)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:356)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:366)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:376)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:386)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:395)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:404)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.KStemmer (KStemmer.java:413)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:529)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:534)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.en.PorterStemmer (PorterStemmer.java:542)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:314)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:320)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:336)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:346)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.analysis.hunspell.HunspellStemmer (HunspellStemmer.java:382)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSetExceptions (RSLPStemmerBase.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.pt.RSLPStemmerBase$RuleWithSuffixExceptions (RSLPStemmerBase.java:159)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.tartarus.snowball.SnowballProgram (SnowballProgram.java:438)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:69)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:71)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.analysis.cn.smart.AnalyzerProfile (AnalyzerProfile.java:73)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Compile (Compile.java:126)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:107)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.DiffIt (DiffIt.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:301)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Row (Row.java:303)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.egothor.stemmer.Trie (Trie.java:379)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:95)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:102)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:106)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:127)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.Benchmark (Benchmark.java:129)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:134)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.PerfRunData (PerfRunData.java:135)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ContentItemsSource (ContentItemsSource.java:178)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser (DemoHTMLParser.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.EnwikiQueryMaker (EnwikiQueryMaker.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:84)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker (FileBasedQueryMaker.java:93)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker (ReutersQueryMaker.java:90)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:195)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:204)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:227)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:242)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.TrecContentSource (TrecContentSource.java:243)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.HTMLParserTokenManager (HTMLParserTokenManager.java:12)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:35)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.feeds.demohtml.ParserThread (ParserThread.java:37)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.programmatic.Sample (Sample.java:72)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseReaderTask (CloseReaderTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CloseTaxonomyReaderTask (CloseTaxonomyReaderTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:181)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.CreateIndexTask (CreateIndexTask.java:183)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:108)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:110)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NearRealtimeReaderTask (NearRealtimeReaderTask.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask (NewAnalyzerTask.java:81)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewCollationAnalyzerTask (NewCollationAnalyzerTask.java:88)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewLocaleTask (NewLocaleTask.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.NewShingleAnalyzerTask (NewShingleAnalyzerTask.java:83)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:138)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PerfTask (PerfTask.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.PrintReaderTask (PrintReaderTask.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:140)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:141)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:142)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.ReadTask (ReadTask.java:146)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepAllTask (RepAllTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:40)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSelectByPrefTask (RepSelectByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameRoundTask (RepSumByNameRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask (RepSumByNameTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:41)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:42)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefRoundTask (RepSumByPrefRoundTask.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:43)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.tasks.RepSumByPrefTask (RepSumByPrefTask.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:117)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:121)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:123)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:307)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:308)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.byTask.utils.Config (Config.java:309)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:44)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:45)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:47)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:48)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:49)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.trec.QueryDriver (QueryDriver.java:65)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:54)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.quality.utils.QualityQueriesFinder (QualityQueriesFinder.java:112)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:46)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractReuters (ExtractReuters.java:152)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:50)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:100)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:111)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:145)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:155)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.benchmark.utils.ExtractWikipedia (ExtractWikipedia.java:157)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1963)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1965)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1973)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1975)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1980)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1985)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum (BlockTreeTermsReader.java:1992)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:116)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:128)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:150)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:154)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:180)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:210)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:223)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:255)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:271)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:344)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:378)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:387)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:398)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:402)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:405)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.codecs.pulsing.PulsingPostingsWriter (PulsingPostingsWriter.java:408)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:354)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:388)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:398)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:600)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:660)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1208)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1249)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1368)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1602)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1691)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1700)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1707)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1714)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1723)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1724)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1753)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1758)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1762)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1771)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden method invocation: java.lang.Throwable#printStackTrace(java.io.PrintStream)
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1772)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1778)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1787)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1789)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1790)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1793)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1795)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1797)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1798)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.CheckIndex (CheckIndex.java:1801)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:56)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:57)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:58)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:59)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:60)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:61)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:62)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:63)
[forbidden-apis] Forbidden field access: java.lang.System#err
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:66)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   in org.apache.lucene.index.IndexUpgrader (IndexUpgrader.java:82)
[forbidden-apis] Forbidden field access: java.lang.System#out
[forbidden-apis]   i","BUILD FAILED
/home/rmuir/workspace/lucene-trunk/lucene/build.xml:190: Check for forbidden API calls failed, see log.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1575","1575","37034","3625","Here's a patch: including fixes.
I think its ready to commit","Here's a patch: including fixes.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1576","1576","37034","3625","Here's a patch: including fixes.
I think its ready to commit","I think its ready to commit","rcmuir","NULL","1","decision","0","0","0","0","1"
"1577","1577","37035","3625","+1, thanks Robert!","+1, thanks Robert!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1578","1578","37038","3626","Hmm I think we need a separate check in FreqProxTermsWriterPerField?
yeah I agree. I just made this patch up to show the problem though!","Hmm I think we need a separate check in FreqProxTermsWriterPerField?","simonw","NULL","1","alternative","0","1","0","0","0"
"1579","1579","37038","3626","Hmm I think we need a separate check in FreqProxTermsWriterPerField?
yeah I agree. I just made this patch up to show the problem though!","yeah I agree.","simonw","NULL","1","pro","0","0","1","0","0"
"1580","1580","37038","3626","Hmm I think we need a separate check in FreqProxTermsWriterPerField?
yeah I agree. I just made this patch up to show the problem though!","I just made this patch up to show the problem though!","simonw","NULL","1","alternative","0","1","0","0","0"
"1581","1581","37039","3626","it shoudl check exactly at the point before shifts any bits: and the exception should be UOE ","it shoudl check exactly at the point before shifts any bits: and the exception should be UOE","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1582","1582","37040","3626","By the way, the reason this doesnt fail always but only for certain codecs:
some codecs have assertions that get tripped, so they fail the test.
other codecs don't have these asserts, so they pass the test, and checkindex happens to pass.
but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!
attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index. I havent tested this yet on 3.x but i suspect it fails!","By the way, the reason this doesnt fail always but only for certain codecs:
some codecs have assertions that get tripped, so they fail the test.","rcmuir","NULL","1","con","0","0","0","1","0"
"1583","1583","37040","3626","By the way, the reason this doesnt fail always but only for certain codecs:
some codecs have assertions that get tripped, so they fail the test.
other codecs don't have these asserts, so they pass the test, and checkindex happens to pass.
but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!
attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index. I havent tested this yet on 3.x but i suspect it fails!","other codecs don't have these asserts, so they pass the test, and checkindex happens to pass.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1584","1584","37040","3626","By the way, the reason this doesnt fail always but only for certain codecs:
some codecs have assertions that get tripped, so they fail the test.
other codecs don't have these asserts, so they pass the test, and checkindex happens to pass.
but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!
attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index. I havent tested this yet on 3.x but i suspect it fails!","but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!","rcmuir","NULL","1","issue","1","0","0","0","0"
"1585","1585","37040","3626","By the way, the reason this doesnt fail always but only for certain codecs:
some codecs have assertions that get tripped, so they fail the test.
other codecs don't have these asserts, so they pass the test, and checkindex happens to pass.
but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!
attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index. I havent tested this yet on 3.x but i suspect it fails!","attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1586","1586","37040","3626","By the way, the reason this doesnt fail always but only for certain codecs:
some codecs have assertions that get tripped, so they fail the test.
other codecs don't have these asserts, so they pass the test, and checkindex happens to pass.
but this is only because checkindex ignores deleted docs in testPostings, the index really is corrumpt in those cases!
attached is a new test demonstrating this: for some codecs it triggers an assert, for others it makes a corrumpt index. I havent tested this yet on 3.x but i suspect it fails!","I havent tested this yet on 3.x but i suspect it fails!","rcmuir","NULL","1","issue","1","0","0","0","0"
"1587","1587","37041","3626","See my comment and test (which produces a corrumpt index on 3.x)
The fact this test doesnt fail on 3.x is a bad thing ","See my comment and test (which produces a corrumpt index on 3.x)
The fact this test doesnt fail on 3.x is a bad thing","rcmuir","NULL","1","issue","1","0","0","0","0"
"1588","1588","37042","3626","attached is a fix. I want to commit soon.
We just used the wrong shift. Our sign bit is free here to steal for payloads. So we don't need to limit positions to Integer.MAX_VALUE/2","attached is a fix.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1589","1589","37042","3626","attached is a fix. I want to commit soon.
We just used the wrong shift. Our sign bit is free here to steal for payloads. So we don't need to limit positions to Integer.MAX_VALUE/2","I want to commit soon.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1590","1590","37042","3626","attached is a fix. I want to commit soon.
We just used the wrong shift. Our sign bit is free here to steal for payloads. So we don't need to limit positions to Integer.MAX_VALUE/2","We just used the wrong shift.","rcmuir","NULL","1","con","0","0","0","1","0"
"1591","1591","37042","3626","attached is a fix. I want to commit soon.
We just used the wrong shift. Our sign bit is free here to steal for payloads. So we don't need to limit positions to Integer.MAX_VALUE/2","Our sign bit is free here to steal for payloads.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1592","1592","37042","3626","attached is a fix. I want to commit soon.
We just used the wrong shift. Our sign bit is free here to steal for payloads. So we don't need to limit positions to Integer.MAX_VALUE/2","So we don't need to limit positions to Integer.MAX_VALUE/2","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1593","1593","37043","3626","+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"1594","1594","37044","3626","thanks everyone!","thanks everyone!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1595","1595","37592","3675","Hi Benson,
This looks like a duplicate of SOLR-2634.
On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.","Hi Benson,
This looks like a duplicate of SOLR-2634.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1596","1596","37592","3675","Hi Benson,
This looks like a duplicate of SOLR-2634.
On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.","On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1597","1597","37593","3675","On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.
Or rather, you never reported on that issue that you did.","On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1598","1598","37593","3675","On that issue, you said you would research using Jenkins credentials to upload to the Nexus snapshot repo, but you never did.
Or rather, you never reported on that issue that you did.","Or rather, you never reported on that issue that you did.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1599","1599","37594","3675","FYI, on lucene.zones.apache.org, where all of the Lucene/Solr Jenkins jobs run, /home/hudson/.m2/settings.xml does not exist (Jenkins jobs run under the hudson user account), so we can't depend on pre-existing Jenkins credentials.","FYI, on lucene.zones.apache.org, where all of the Lucene/Solr Jenkins jobs run, /home/hudson/.m2/settings.xml does not exist (Jenkins jobs run under the hudson user account), so we can't depend on pre-existing Jenkins credentials.","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1600","1600","37595","3675","This time for sure. https://issues.apache.org/jira/browse/INFRA-4496.","This time for sure.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1601","1601","37595","3675","This time for sure. https://issues.apache.org/jira/browse/INFRA-4496.","https://issues.apache.org/jira/browse/INFRA-4496.","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1602","1602","37596","3675","This time for sure. https://issues.apache.org/jira/browse/INFRA-4496.
Righteous to the max.","This time for sure.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1603","1603","37596","3675","This time for sure. https://issues.apache.org/jira/browse/INFRA-4496.
Righteous to the max.","https://issues.apache.org/jira/browse/INFRA-4496.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1604","1604","37596","3675","This time for sure. https://issues.apache.org/jira/browse/INFRA-4496.
Righteous to the max.","Righteous to the max.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1605","1605","37597","3675","See INFRA-4497 for request to enable Nexus access for Lucene and Solr.","See INFRA-4497 for request to enable Nexus access for Lucene and Solr.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1606","1606","37598","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...
Anyway, this patch fixes the SCM definition problem for trunk POMs.  I'll do the same for branch_3x too.
I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.
Committing shortly.","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1607","1607","37598","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...
Anyway, this patch fixes the SCM definition problem for trunk POMs.  I'll do the same for branch_3x too.
I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.
Committing shortly.","I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...","steve_rowe","NULL","1","alternative, con","0","1","0","1","0"
"1608","1608","37598","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...
Anyway, this patch fixes the SCM definition problem for trunk POMs.  I'll do the same for branch_3x too.
I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.
Committing shortly.","Anyway, this patch fixes the SCM definition problem for trunk POMs.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1609","1609","37598","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...
Anyway, this patch fixes the SCM definition problem for trunk POMs.  I'll do the same for branch_3x too.
I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.
Committing shortly.","I'll do the same for branch_3x too.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1610","1610","37598","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...
Anyway, this patch fixes the SCM definition problem for trunk POMs.  I'll do the same for branch_3x too.
I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.
Committing shortly.","I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.","steve_rowe","NULL","1","alternative, pro","0","1","1","0","0"
"1611","1611","37598","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
I had previously thought that a single SCM section in the lucene-solr-grandparent POM would do the trick, but upon inspection of the output from mvn help:effective-pom, I can see that the grandparent POM's section doesn't properly interpolate ${module-directory} (since the property isn't defined in that POM), and the values inherited in other POMs are all wrong, because path steps are added with artifact names instead of directory names, in addition to ${module-directory} interpolation...
Anyway, this patch fixes the SCM definition problem for trunk POMs.  I'll do the same for branch_3x too.
I didn't include SCM sections in the few aggregation-only POMs, since these are never deployed/released.
Committing shortly.","Committing shortly.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1612","1612","37599","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
Committed to trunk and branch_3x.","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1613","1613","37599","3675","According to http://www.apache.org/dev/publishing-maven-artifacts.html#prepare-poms item #3, all POMs should have SCM sections.
Committed to trunk and branch_3x.","Committed to trunk and branch_3x.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1614","1614","37600","3675","Is the result of this blocked on the jail break? ","Is the result of this blocked on the jail break?","bmargulies","NULL","0",NULL,"0","0","0","0","0"
"1615","1615","37601","3675","Is the result of this blocked on the jail break? 
I don't know what you mean by jail break?
I'm waiting for someone to enable Nexus access for Lucene: INFRA-4497 - I had planned to hassle them after a week had gone by with no action (it's only been a day or two so far).","Is the result of this blocked on the jail break?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1616","1616","37601","3675","Is the result of this blocked on the jail break? 
I don't know what you mean by jail break?
I'm waiting for someone to enable Nexus access for Lucene: INFRA-4497 - I had planned to hassle them after a week had gone by with no action (it's only been a day or two so far).","I don't know what you mean by jail break?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1617","1617","37601","3675","Is the result of this blocked on the jail break? 
I don't know what you mean by jail break?
I'm waiting for someone to enable Nexus access for Lucene: INFRA-4497 - I had planned to hassle them after a week had gone by with no action (it's only been a day or two so far).","I'm waiting for someone to enable Nexus access for Lucene: INFRA-4497 - I had planned to hassle them after a week had gone by with no action (it's only been a day or two so far).","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1618","1618","37602","3675","I was referring to Uwe's email about stopping all builds due to 1.6 versus 1.6 issues in the jail.","I was referring to Uwe's email about stopping all builds due to 1.6 versus 1.6 issues in the jail.","bmargulies","NULL","1","alternative","0","1","0","0","0"
"1619","1619","37603","3675","Not that it's especially my business, but how did the snapshots get pushed historically to nexus if you didn't have access to nexus?","Not that it's especially my business, but how did the snapshots get pushed historically to nexus if you didn't have access to nexus?","bmargulies","NULL","0","issue","1","0","0","0","0"
"1620","1620","37604","3675","I don't know - I was not involved with Lucene's Maven stuff, or with Hudson, at that time.  Maybe others know?","I don't know - I was not involved with Lucene's Maven stuff, or with Hudson, at that time.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1621","1621","37604","3675","I don't know - I was not involved with Lucene's Maven stuff, or with Hudson, at that time.  Maybe others know?","Maybe others know?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1622","1622","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1623","1623","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","However, I can't do any further deploys, apparently because Nexus doesn't like me.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1624","1624","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1625","1625","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1626","1626","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).","steve_rowe","NULL","1","alternative, con","0","1","0","1","0"
"1627","1627","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.","steve_rowe","NULL","1","issue, alternative","1","1","0","0","0"
"1628","1628","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1629","1629","37605","3675","I deployed all of Lucene's trunk artifacts to the Apache snapshot repository using my personal account.
However, I can't do any further deploys, apparently because Nexus doesn't like me.  Here's what I asked on #asfinfra 15 minutes ago:

Hi, I'm working on publishing Lucene's and Solr's Maven snapshots to the Apache Snapshot repo.  Brian Demers enabled o.a.lucene and o.a.solr in the Nexus repo (https://issues.apache.org/jira/browse/INFRA-4497), and I successfully uploaded Lucene snapshot artifacts for 4.0.0-SNAPSHOT using Maven Ant Tasks.  I attempted to use an encrypted password in ~/.m2/settings.xml, but that failed, apparently because Maven Ant Tasks doesn't (yet) grok encrypted passwords (http://jira.codehaus.org/browse/MANTTASKS-177).  So I switched back to a plaintext password, but now I'm getting 401 errors from Nexus when I try to deploy Solr snapshots, or to re-deploy the Lucene snapshots.  I suspect Nexus has locked my account out (sarowe), but I'm not sure - can anybody here help?
No response yet.","No response yet.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1630","1630","37606","3675","the nexus log says, 'unable to authenticate sarowe'.","the nexus log says, 'unable to authenticate sarowe'.","bmargulies","NULL","1","issue","1","0","0","0","0"
"1631","1631","37608","3675","the nexus log says, 'unable to authenticate sarowe'.
Yeah, the same credentials worked earlier, and now don't work.
I can log into lucene.zones.apache.org using these same credentials, and I assume Nexus and FreeBSD are pulling from the Apache LDAP DB.
So as I wrote to #asfinfra (see above), I'm thinking I've been locked out.","the nexus log says, 'unable to authenticate sarowe'.","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1632","1632","37608","3675","the nexus log says, 'unable to authenticate sarowe'.
Yeah, the same credentials worked earlier, and now don't work.
I can log into lucene.zones.apache.org using these same credentials, and I assume Nexus and FreeBSD are pulling from the Apache LDAP DB.
So as I wrote to #asfinfra (see above), I'm thinking I've been locked out.","Yeah, the same credentials worked earlier, and now don't work.","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1633","1633","37608","3675","the nexus log says, 'unable to authenticate sarowe'.
Yeah, the same credentials worked earlier, and now don't work.
I can log into lucene.zones.apache.org using these same credentials, and I assume Nexus and FreeBSD are pulling from the Apache LDAP DB.
So as I wrote to #asfinfra (see above), I'm thinking I've been locked out.","I can log into lucene.zones.apache.org using these same credentials, and I assume Nexus and FreeBSD are pulling from the Apache LDAP DB.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1634","1634","37608","3675","the nexus log says, 'unable to authenticate sarowe'.
Yeah, the same credentials worked earlier, and now don't work.
I can log into lucene.zones.apache.org using these same credentials, and I assume Nexus and FreeBSD are pulling from the Apache LDAP DB.
So as I wrote to #asfinfra (see above), I'm thinking I've been locked out.","So as I wrote to #asfinfra (see above), I'm thinking I've been locked out.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1635","1635","37609","3675","Lucene.Zones login is local only (i added you to passwd file). You should better check people.ao or SVN for credentials.","Lucene.Zones login is local only (i added you to passwd file).","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1636","1636","37609","3675","Lucene.Zones login is local only (i added you to passwd file). You should better check people.ao or SVN for credentials.","You should better check people.ao or SVN for credentials.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1637","1637","37611","3675","
I am now in the process of deploying Solr trunk snapshot artifacts.
I'll do the same for branch_3x Lucene&Solr.
These are done.
I'll work on switching the Jenkins jobs now.","I am now in the process of deploying Solr trunk snapshot artifacts.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1638","1638","37611","3675","
I am now in the process of deploying Solr trunk snapshot artifacts.
I'll do the same for branch_3x Lucene&Solr.
These are done.
I'll work on switching the Jenkins jobs now.","I'll do the same for branch_3x Lucene&Solr.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1639","1639","37611","3675","
I am now in the process of deploying Solr trunk snapshot artifacts.
I'll do the same for branch_3x Lucene&Solr.
These are done.
I'll work on switching the Jenkins jobs now.","These are done.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1640","1640","37611","3675","
I am now in the process of deploying Solr trunk snapshot artifacts.
I'll do the same for branch_3x Lucene&Solr.
These are done.
I'll work on switching the Jenkins jobs now.","I'll work on switching the Jenkins jobs now.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1641","1641","37612","3675","Lucene.Zones login is local only (i added you to passwd file). You should better check people.ao or SVN for credentials.
Okay, that makes sense.  I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.","Lucene.Zones login is local only (i added you to passwd file).","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1642","1642","37612","3675","Lucene.Zones login is local only (i added you to passwd file). You should better check people.ao or SVN for credentials.
Okay, that makes sense.  I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.","You should better check people.ao or SVN for credentials.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1643","1643","37612","3675","Lucene.Zones login is local only (i added you to passwd file). You should better check people.ao or SVN for credentials.
Okay, that makes sense.  I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.","Okay, that makes sense.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1644","1644","37612","3675","Lucene.Zones login is local only (i added you to passwd file). You should better check people.ao or SVN for credentials.
Okay, that makes sense.  I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.","I knew it was the Apache LDAP server, and not specifically Nexus, when SVN logins failed when I tried to commit the patch listed above.","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1645","1645","37613","3675","The Jenkins Maven trunk and branch_3x builds are now configured to deploy snapshot artifacts to the Apache snapshot repository, and both have successfully done so.","The Jenkins Maven trunk and branch_3x builds are now configured to deploy snapshot artifacts to the Apache snapshot repository, and both have successfully done so.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1646","1646","37614","3675","Not that it's especially my business, but how did the snapshots get pushed historically to nexus if you didn't have access to nexus?
I found the following comment on SOLR-586 that described the process by which Maven snapshots made it into the Apache snapshot repository: https://issues.apache.org/jira/browse/SOLR-586?focusedCommentId=12623985&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12623985.","Not that it's especially my business, but how did the snapshots get pushed historically to nexus if you didn't have access to nexus?","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1647","1647","37614","3675","Not that it's especially my business, but how did the snapshots get pushed historically to nexus if you didn't have access to nexus?
I found the following comment on SOLR-586 that described the process by which Maven snapshots made it into the Apache snapshot repository: https://issues.apache.org/jira/browse/SOLR-586?focusedCommentId=12623985&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12623985.","I found the following comment on SOLR-586 that described the process by which Maven snapshots made it into the Apache snapshot repository: https://issues.apache.org/jira/browse/SOLR-586?focusedCommentId=12623985&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12623985.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1648","1648","37615","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?  I would imagine a proper repo manager is a more suitable official location than Jenkins.
As an aside, do you know why SSL is mandatory to access nexus?  I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.  I'm not saying its impossible, but just tricky and took trying different things.  If you don't know the answer, who/how should I inquire further?","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?","dsmiley","NULL","1","alternative","0","1","0","0","0"
"1649","1649","37615","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?  I would imagine a proper repo manager is a more suitable official location than Jenkins.
As an aside, do you know why SSL is mandatory to access nexus?  I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.  I'm not saying its impossible, but just tricky and took trying different things.  If you don't know the answer, who/how should I inquire further?","I would imagine a proper repo manager is a more suitable official location than Jenkins.","dsmiley","NULL","1","alternative, pro","0","1","1","0","0"
"1650","1650","37615","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?  I would imagine a proper repo manager is a more suitable official location than Jenkins.
As an aside, do you know why SSL is mandatory to access nexus?  I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.  I'm not saying its impossible, but just tricky and took trying different things.  If you don't know the answer, who/how should I inquire further?","As an aside, do you know why SSL is mandatory to access nexus?","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"1651","1651","37615","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?  I would imagine a proper repo manager is a more suitable official location than Jenkins.
As an aside, do you know why SSL is mandatory to access nexus?  I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.  I'm not saying its impossible, but just tricky and took trying different things.  If you don't know the answer, who/how should I inquire further?","I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.","dsmiley","NULL","1","con","0","0","0","1","0"
"1652","1652","37615","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?  I would imagine a proper repo manager is a more suitable official location than Jenkins.
As an aside, do you know why SSL is mandatory to access nexus?  I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.  I'm not saying its impossible, but just tricky and took trying different things.  If you don't know the answer, who/how should I inquire further?","I'm not saying its impossible, but just tricky and took trying different things.","dsmiley","NULL","1","pro, con","0","0","1","1","0"
"1653","1653","37615","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?  I would imagine a proper repo manager is a more suitable official location than Jenkins.
As an aside, do you know why SSL is mandatory to access nexus?  I've found https repos to be very problematic for maven when the client needs to use an HTTP proxy such as is common in corporate environments.  I'm not saying its impossible, but just tricky and took trying different things.  If you don't know the answer, who/how should I inquire further?","If you don't know the answer, who/how should I inquire further?","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"1654","1654","37616","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?
Done - see r1296722 and r1296723.
As an aside, do you know why SSL is mandatory to access nexus? [...] If you don't know the answer, who/how should I inquire further?
Sorry, I don't know the answer to either question.  I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1655","1655","37616","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?
Done - see r1296722 and r1296723.
As an aside, do you know why SSL is mandatory to access nexus? [...] If you don't know the answer, who/how should I inquire further?
Sorry, I don't know the answer to either question.  I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","Done - see r1296722 and r1296723.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1656","1656","37616","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?
Done - see r1296722 and r1296723.
As an aside, do you know why SSL is mandatory to access nexus? [...] If you don't know the answer, who/how should I inquire further?
Sorry, I don't know the answer to either question.  I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","As an aside, do you know why SSL is mandatory to access nexus?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1657","1657","37616","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?
Done - see r1296722 and r1296723.
As an aside, do you know why SSL is mandatory to access nexus? [...] If you don't know the answer, who/how should I inquire further?
Sorry, I don't know the answer to either question.  I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","[...] If you don't know the answer, who/how should I inquire further?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1658","1658","37616","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?
Done - see r1296722 and r1296723.
As an aside, do you know why SSL is mandatory to access nexus? [...] If you don't know the answer, who/how should I inquire further?
Sorry, I don't know the answer to either question.  I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","Sorry, I don't know the answer to either question.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1659","1659","37616","3675","Perhaps the last step is to now modify README.maven to point to https://repository.apache.org/content/repositories/snapshots/ instead of Jenkins?
Done - see r1296722 and r1296723.
As an aside, do you know why SSL is mandatory to access nexus? [...] If you don't know the answer, who/how should I inquire further?
Sorry, I don't know the answer to either question.  I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","I would start asking either on #asfinfra or infrastructure@apache.org, and they should be able to point you to the right person.","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1660","1660","37950","3710","Removing 3.6 Fix version. If I'll make it by the release, I'll put it back.","Removing 3.6 Fix version.","shaie","NULL","1","decision","0","0","0","0","1"
"1661","1661","37950","3710","Removing 3.6 Fix version. If I'll make it by the release, I'll put it back.","If I'll make it by the release, I'll put it back.","shaie","NULL","1","decision","0","0","0","0","1"
"1662","1662","37951","3710","Patch w/ test.","Patch w/ test.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1663","1663","37953","3710","Here's the patch I ended up with when working on this on top of 3.x (don't remember if it was 3.5 or 3.6). This is not intended for commit, but you many want to look at the manager and its validation logic. Not sure how much of it is still relevant, except the recreate scenario.","Here's the patch I ended up with when working on this on top of 3.x (don't remember if it was 3.5 or 3.6).","shaie","NULL","1","alternative","0","1","0","0","0"
"1664","1664","37953","3710","Here's the patch I ended up with when working on this on top of 3.x (don't remember if it was 3.5 or 3.6). This is not intended for commit, but you many want to look at the manager and its validation logic. Not sure how much of it is still relevant, except the recreate scenario.","This is not intended for commit, but you many want to look at the manager and its validation logic.","shaie","NULL","1","alternative, con","0","1","0","1","0"
"1665","1665","37953","3710","Here's the patch I ended up with when working on this on top of 3.x (don't remember if it was 3.5 or 3.6). This is not intended for commit, but you many want to look at the manager and its validation logic. Not sure how much of it is still relevant, except the recreate scenario.","Not sure how much of it is still relevant, except the recreate scenario.","shaie","NULL","1","alternative","0","1","0","0","0"
"1666","1666","37955","3710","I think it's OK to add IOE to the signature?
Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through. I will review the patch after you post a new version.","I think it's OK to add IOE to the signature?","shaie","NULL","1","alternative","0","1","0","0","0"
"1667","1667","37955","3710","I think it's OK to add IOE to the signature?
Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through. I will review the patch after you post a new version.","Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?","shaie","NULL","1","alternative","0","1","0","0","0"
"1668","1668","37955","3710","I think it's OK to add IOE to the signature?
Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through. I will review the patch after you post a new version.","So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right?","shaie","NULL","1","alternative","0","1","0","0","0"
"1669","1669","37955","3710","I think it's OK to add IOE to the signature?
Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through. I will review the patch after you post a new version.","Therefore there's no point to even tryIncRef?","shaie","NULL","1","alternative, con","0","1","0","1","0"
"1670","1670","37955","3710","I think it's OK to add IOE to the signature?
Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through. I will review the patch after you post a new version.","Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through.","shaie","NULL","0",NULL,"0","0","0","0","0"
"1671","1671","37955","3710","I think it's OK to add IOE to the signature?
Ok.
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
Just because it's using the LineFileDocs
Ahh ok. As I said, I didn't read the test through. I will review the patch after you post a new version.","I will review the patch after you post a new version.","shaie","NULL","0",NULL,"0","0","0","0","0"
"1672","1672","37956","3710","New patch, just handling the NRT case.","New patch, just handling the NRT case.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1673","1673","37957","3710","
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
You're right ... so I just left the two decRefs in the patch ...","that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1674","1674","37957","3710","
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
You're right ... so I just left the two decRefs in the patch ...","So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1675","1675","37957","3710","
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
You're right ... so I just left the two decRefs in the patch ...","Therefore there's no point to even tryIncRef?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1676","1676","37957","3710","
that decRef could have closed the reader
Hmm ... if we assume that this TR/IR pair is managed only by that manager, then an IOE thrown from decRef could only be caused by closing the reader, right?
So if you successfully IR.decRef() but fail to TR.decRef(), it means that IR is closed already right? Therefore there's no point to even tryIncRef?
You're right ... so I just left the two decRefs in the patch ...","You're right ... so I just left the two decRefs in the patch ...","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1677","1677","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty.","shaie","NULL","1","con","0","0","0","1","0"
"1678","1678","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?","shaie","NULL","1","alternative","0","1","0","0","0"
"1679","1679","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Maybe change the end of the test to a single-line IOUtils.close()?","shaie","NULL","1","alternative","0","1","0","0","0"
"1680","1680","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","You wrote previously that the test uses LineFileDocs, but I don't see it.","shaie","NULL","1","alternative, con","0","1","0","1","0"
"1681","1681","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","It seems it only adds facets to documents?","shaie","NULL","0",NULL,"0","0","0","0","0"
"1682","1682","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","If so, can it go back to newDirectory()?","shaie","NULL","0",NULL,"0","0","0","0","0"
"1683","1683","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","It's good that you identify replaceTaxonomy, makes the code safer.","shaie","NULL","1","pro","0","0","1","0","0"
"1684","1684","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)?","shaie","NULL","1","alternative","0","1","0","0","0"
"1685","1685","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it.","shaie","NULL","1","con","0","0","0","1","0"
"1686","1686","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much.","shaie","NULL","1","con","0","0","0","1","0"
"1687","1687","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo.","shaie","NULL","1","con","0","0","0","1","0"
"1688","1688","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","But I think that's ok since it means the check will fail on the next refresh attempt.","shaie","NULL","1","pro","0","0","1","0","0"
"1689","1689","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Really, if ever DTW.epoch changes, we should fail.","shaie","NULL","1","alternative, con","0","1","0","1","0"
"1690","1690","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"1691","1691","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed).","shaie","NULL","1","alternative","0","1","0","0","0"
"1692","1692","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"1693","1693","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Otherwise this looks great!","shaie","NULL","1","pro","0","0","1","0","0"
"1694","1694","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare.","shaie","NULL","1","alternative, con","0","1","0","1","0"
"1695","1695","37958","3710","Few comments:

This assert in the test assertEquals(1, results.size()); is kinda moot because we always return a FacetResult, even if empty. Perhaps you can assert that if the acquired reader.maxDoc is > 0, the returned FacetResult.rootNode has at least one child with count that is at least 1?


Maybe change the end of the test to a single-line IOUtils.close()?


You wrote previously that the test uses LineFileDocs, but I don't see it. It seems it only adds facets to documents? If so, can it go back to newDirectory()?


It's good that you identify replaceTaxonomy, makes the code safer.


TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.


I don't know how important it is, but perhaps given the short discussion we had above, it would be good to add a 1-liner to decRef why the method seems unprotected, but in reality it's the best we can do?


In refreshIfNeeded, I understand this code newReader.decRef() is equivalent to closing newReader (if epoch has changed). But after I received a question yesterday from a someone who did not understand why we don't call close(), perhaps we should, for clarity?

Otherwise this looks great! When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!","Making it NRT really simplified this manager!","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"1696","1696","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1697","1697","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it.","mikemccand","NULL","1","con","0","0","0","1","0"
"1698","1698","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much.","mikemccand","NULL","1","con","0","0","0","1","0"
"1699","1699","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo.","mikemccand","NULL","1","con","0","0","0","1","0"
"1700","1700","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","But I think that's ok since it means the check will fail on the next refresh attempt.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1701","1701","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","Really, if ever DTW.epoch changes, we should fail.","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"1702","1702","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?","mikemccand","NULL","1","con","0","0","0","1","0"
"1703","1703","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","The added Expert/@lucene.internal method seems minor ...","mikemccand","NULL","1","con","0","0","0","1","0"
"1704","1704","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare.","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"1705","1705","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","Making it NRT really simplified this manager!","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1706","1706","37959","3710","Thanks for all the feedback Shai, I incorporated it all except for
this one:
TR.getTaxoEpoch: maybe instead of adding it to TR you can use the one on DTW (make it public, @lucene.internal)? It's odd that it documents that this epoch is returned only for an NRT TR, because the epoch is recorded on the taxo index commit data, so conceptually there's no reason why it shouldn't always return it. Yet, since this epoch is used internally, between TW and TR, I prefer not to expose it too much. Hmmm, but then you may hit a false positive where the returned TR is valid, yet just in between the checks the app called replaceTaxo. But I think that's ok since it means the check will fail on the next refresh attempt. Really, if ever DTW.epoch changes, we should fail.
I don't like that cutting over to DTW would open up the thread hazard
that we fail to catch the replace ... admittedly it'd be rare but why
open it up?  The added Expert/@lucene.internal method seems minor ...
When I worked on it in the past, DTR wasn't NRT and the sync was a nightmare. Making it NRT really simplified this manager!
Thank you for doing all the hard work first (making DTR NRT) ","Thank you for doing all the hard work first (making DTR NRT)","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1707","1707","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1708","1708","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","Good, I'll fix that.","mikemccand","NULL","1","pro, decision","0","0","1","0","1"
"1709","1709","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","Also test() also has these 5 close() statements which can be folded into one IOUtils.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1710","1710","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","But that's just style.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1711","1711","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","Woops, I missed that one ...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1712","1712","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","I'll fix.","mikemccand","NULL","1","decision","0","0","0","0","1"
"1713","1713","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!)","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1714","1714","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","get into that hazard.","mikemccand","NULL","1","con","0","0","0","1","0"
"1715","1715","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","Ahh, right, as long as I check taxoWriter after the reopen: good!","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1716","1716","37961","3710","I reviewed again, and now that you switch to calling close() instead of decRef(), I think the close() should be done via IOUtils.close, to avoid a potential close failure (newReader.close()) and leave behind a dangling TR?
Good, I'll fix that.
Also test() also has these 5 close() statements which can be folded into one IOUtils. But that's just style.
Woops, I missed that one ... I'll fix.
What I meant is that if instead of checking epoch on TR you check on DTW, you won't (I think!) get into that hazard. 
Ahh, right, as long as I check taxoWriter after the reopen: good!  I'll fix to just use DTW...","I'll fix to just use DTW...","mikemccand","NULL","1","alternative, decision","0","1","0","0","1"
"1717","1717","37962","3710","New patch w/ last round of changes ... thanks Shai!","New patch w/ last round of changes ... thanks Shai!","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1718","1718","37963","3710","Looks good, +1. Thanks for doing the work Mike!","Looks good, +1.","shaie","NULL","1","pro","0","0","1","0","0"
"1719","1719","37963","3710","Looks good, +1. Thanks for doing the work Mike!","Thanks for doing the work Mike!","shaie","NULL","0",NULL,"0","0","0","0","0"
"1720","1720","37964","3710","Closed after release.","Closed after release.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1721","1721","38756","3789","Forgot to mention that Doron actually discovered the bug, I just had the time to provide the fix .","Forgot to mention that Doron actually discovered the bug, I just had the time to provide the fix .","shaie","NULL","0",NULL,"0","0","0","0","0"
"1722","1722","38757","3789","Patch fixes the bug by moving to track reference count by DTR. Also, added a test which covers that bug.
On the go, fixed close() to synchronize on this if the instance is not already closed. Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().
I think it's ready to commit, will wait until tomorrow for review.","Patch fixes the bug by moving to track reference count by DTR.","shaie","NULL","1","alternative","0","1","0","0","0"
"1723","1723","38757","3789","Patch fixes the bug by moving to track reference count by DTR. Also, added a test which covers that bug.
On the go, fixed close() to synchronize on this if the instance is not already closed. Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().
I think it's ready to commit, will wait until tomorrow for review.","Also, added a test which covers that bug.","shaie","NULL","1","alternative","0","1","0","0","0"
"1724","1724","38757","3789","Patch fixes the bug by moving to track reference count by DTR. Also, added a test which covers that bug.
On the go, fixed close() to synchronize on this if the instance is not already closed. Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().
I think it's ready to commit, will wait until tomorrow for review.","On the go, fixed close() to synchronize on this if the instance is not already closed.","shaie","NULL","1","alternative","0","1","0","0","0"
"1725","1725","38757","3789","Patch fixes the bug by moving to track reference count by DTR. Also, added a test which covers that bug.
On the go, fixed close() to synchronize on this if the instance is not already closed. Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().
I think it's ready to commit, will wait until tomorrow for review.","Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().","shaie","NULL","1","con","0","0","0","1","0"
"1726","1726","38757","3789","Patch fixes the bug by moving to track reference count by DTR. Also, added a test which covers that bug.
On the go, fixed close() to synchronize on this if the instance is not already closed. Otherwise, two threads that call close() concurrently might fail (one of them) in decRef().
I think it's ready to commit, will wait until tomorrow for review.","I think it's ready to commit, will wait until tomorrow for review.","shaie","NULL","1","decision","0","0","0","0","1"
"1727","1727","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","Patch looks good, builds and passes for me, thanks for fixing this Shai.","doronc","NULL","1","pro","0","0","1","0","0"
"1728","1728","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","Few comments:

CHANGES: rephrase the e.g.","doronc","NULL","1","alternative","0","1","0","0","0"
"1729","1729","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","part like this: (e.g.","doronc","NULL","1","alternative","0","1","0","0","0"
"1730","1730","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","if application called incRef/decRef).","doronc","NULL","1","alternative","0","1","0","0","0"
"1731","1731","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","New test:
	
LTC.newDirectory() instead of new RAMDirectory().","doronc","NULL","1","alternative","0","1","0","0","0"
"1732","1732","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","text messages in the asserts.","doronc","NULL","1","alternative","0","1","0","0","0"
"1733","1733","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one?","doronc","NULL","1","alternative","0","1","0","0","0"
"1734","1734","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","This way we continue to delegate that logic to IR, and do not duplicate it.","doronc","NULL","1","pro","0","0","1","0","0"
"1735","1735","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","Current patch removes the ensureOpen() check from getRefCount().","doronc","NULL","1","alternative","0","1","0","0","0"
"1736","1736","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","I think this is correct - in fact I needed that when debugging this.","doronc","NULL","1","pro","0","0","1","0","0"
"1737","1737","38758","3789","Patch looks good, builds and passes for me, thanks for fixing this Shai.
Few comments:

CHANGES: rephrase the e.g. part like this: (e.g. if application called incRef/decRef).
New test:
	
LTC.newDirectory() instead of new RAMDirectory().
text messages in the asserts.


DTR:
	
Would it be simpler to make close() synchronized (just like IR.close())
Would it - again - be simpler to keep maintaining the ref-counts in the internal IR and just, in refresh, decRef as needed in the old one and incRef accordingly in the new one? This way we continue to delegate that logic to IR, and do not duplicate it.
Current patch removes the ensureOpen() check from getRefCount(). I think this is correct - in fact I needed that when debugging this. Perhaps should document about it in CHANGES entry.



","Perhaps should document about it in CHANGES entry.","doronc","NULL","1","alternative","0","1","0","0","0"
"1738","1738","38760","3789","Patch addresses Doron's comments.","Patch addresses Doron's comments.","shaie","NULL","1","alternative","0","1","0","0","0"
"1739","1739","38761","3789","Missed that test comment about no need for random directory.
About the decRef dup code, yeah, that's what I meant, but okay.
I think this is ready to commit.","Missed that test comment about no need for random directory.","doronc","NULL","0",NULL,"0","0","0","0","0"
"1740","1740","38761","3789","Missed that test comment about no need for random directory.
About the decRef dup code, yeah, that's what I meant, but okay.
I think this is ready to commit.","About the decRef dup code, yeah, that's what I meant, but okay.","doronc","NULL","0",NULL,"0","0","0","0","0"
"1741","1741","38761","3789","Missed that test comment about no need for random directory.
About the decRef dup code, yeah, that's what I meant, but okay.
I think this is ready to commit.","I think this is ready to commit.","doronc","NULL","1","pro","0","0","1","0","0"
"1742","1742","38762","3789","Committed revision 1234450 (3x), 1234451 (trunk).
Thanks Doron !","Committed revision 1234450 (3x), 1234451 (trunk).","shaie","NULL","1","decision","0","0","0","0","1"
"1743","1743","38762","3789","Committed revision 1234450 (3x), 1234451 (trunk).
Thanks Doron !","Thanks Doron !","shaie","NULL","0",NULL,"0","0","0","0","0"
"1744","1744","38796","3797","In the LUCENE-3453 branch I've removed DocValuesField.setXXX methods: they are all inherited from Field.setValue instead.  Likewise for NumericField (which also had its own setters for numeric values, with different names)...","In the LUCENE-3453 branch I've removed DocValuesField.setXXX methods: they are all inherited from Field.setValue instead.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1745","1745","38796","3797","In the LUCENE-3453 branch I've removed DocValuesField.setXXX methods: they are all inherited from Field.setValue instead.  Likewise for NumericField (which also had its own setters for numeric values, with different names)...","Likewise for NumericField (which also had its own setters for numeric values, with different names)...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1746","1746","38797","3797","Fixed with LUCENE-3453.","Fixed with LUCENE-3453.","mikemccand","NULL","1","decision","0","0","0","0","1"
"1747","1747","38798","3797","seems like this issue still exists. Now we overload Field#setValue(int|long|short|...)","seems like this issue still exists.","simonw","NULL","1","issue","1","0","0","0","0"
"1748","1748","38798","3797","seems like this issue still exists. Now we overload Field#setValue(int|long|short|...)","Now we overload Field#setValue(int|long|short|...)","simonw","NULL","1","issue","1","0","0","0","0"
"1749","1749","38799","3797","Wait  DocValuesField.java doesn't overload any setters now right?  I'm confused.","Wait  DocValuesField.java doesn't overload any setters now right?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1750","1750","38799","3797","Wait  DocValuesField.java doesn't overload any setters now right?  I'm confused.","I'm confused.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1751","1751","38800","3797","I am talking about Field.java


  public void setValue(BytesRef value) {
     //....
  }

  public void setValue(int value) {
      //....
  }

  public void setValue(long value) {
    //....
  }

  public void setValue(float value) {
      //....
  }

  public void setValue(double value) {
   //....
  }

","I am talking about Field.java


  public void setValue(BytesRef value) {
     //....
  }

  public void setValue(int value) {
      //....
  }

  public void setValue(long value) {
    //....
  }

  public void setValue(float value) {
      //....
  }

  public void setValue(double value) {
   //....
  }","simonw","NULL","1","issue","1","0","0","0","0"
"1752","1752","38801","3797","Right, Field.java has setters.
DocValuesField no longer does (nor does NumericField), ie we fixed this issue (that these classes were overloading the setters from Field.java).
Or... are you saying this is naming issue?  Ie we can work out the naming (do we use setValue(T value) or setT(T value) for Field.java and for the new Norm class) here...","Right, Field.java has setters.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1753","1753","38801","3797","Right, Field.java has setters.
DocValuesField no longer does (nor does NumericField), ie we fixed this issue (that these classes were overloading the setters from Field.java).
Or... are you saying this is naming issue?  Ie we can work out the naming (do we use setValue(T value) or setT(T value) for Field.java and for the new Norm class) here...","DocValuesField no longer does (nor does NumericField), ie we fixed this issue (that these classes were overloading the setters from Field.java).","mikemccand","NULL","1","decision","0","0","0","0","1"
"1754","1754","38801","3797","Right, Field.java has setters.
DocValuesField no longer does (nor does NumericField), ie we fixed this issue (that these classes were overloading the setters from Field.java).
Or... are you saying this is naming issue?  Ie we can work out the naming (do we use setValue(T value) or setT(T value) for Field.java and for the new Norm class) here...","Or... are you saying this is naming issue?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1755","1755","38801","3797","Right, Field.java has setters.
DocValuesField no longer does (nor does NumericField), ie we fixed this issue (that these classes were overloading the setters from Field.java).
Or... are you saying this is naming issue?  Ie we can work out the naming (do we use setValue(T value) or setT(T value) for Field.java and for the new Norm class) here...","Ie we can work out the naming (do we use setValue(T value) or setT(T value) for Field.java and for the new Norm class) here...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1756","1756","39974","3938","Patch.  I now pass DataInput down to IndexFormatTooNew/OldExc, and
.toString() it, and impl'd .toString in the all the IndexInput impls I
could find.","Patch.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1757","1757","39974","3938","Patch.  I now pass DataInput down to IndexFormatTooNew/OldExc, and
.toString() it, and impl'd .toString in the all the IndexInput impls I
could find.","I now pass DataInput down to IndexFormatTooNew/OldExc, and
.toString() it, and impl'd .toString in the all the IndexInput impls I
could find.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1758","1758","39975","3938","I dont think i like all the duplication/extra tracking in every indexinput impl... 
In my opinion its not worth it!
can this be done in a cleaner way?","I dont think i like all the duplication/extra tracking in every indexinput impl...","rcmuir","NULL","1","con","0","0","0","1","0"
"1759","1759","39975","3938","I dont think i like all the duplication/extra tracking in every indexinput impl... 
In my opinion its not worth it!
can this be done in a cleaner way?","In my opinion its not worth it!","rcmuir","NULL","1","con","0","0","0","1","0"
"1760","1760","39975","3938","I dont think i like all the duplication/extra tracking in every indexinput impl... 
In my opinion its not worth it!
can this be done in a cleaner way?","can this be done in a cleaner way?","rcmuir","NULL","1","issue","1","0","0","0","0"
"1761","1761","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","I agree with Robert, adding this to IndexInput is stupid.","thetaphi","NULL","1","con","0","0","0","1","0"
"1762","1762","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","I think, only adding the file name as done before should be fine.","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"1763","1763","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","There are only few places where we pass null as file name to the exception (whoch may be fixed).","thetaphi","NULL","1","pro","0","0","1","0","0"
"1764","1764","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","Passing the whole directory name is in my opinion useless.","thetaphi","NULL","1","alternative, con","0","1","0","1","0"
"1765","1765","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1766","1766","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1767","1767","39976","3938","I agree with Robert, adding this to IndexInput is stupid.
I think, only adding the file name as done before should be fine. There are only few places where we pass null as file name to the exception (whoch may be fixed). Passing the whole directory name is in my opinion useless. Its up to the implementation using lucene to keep track of its directory name (when it opens a IndexReader it already knows its dir name).
I would close this as won't fix and maybe only fix the remaining places that misses the file name (e.g. SegmentTermsEnumReader).","SegmentTermsEnumReader).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1768","1768","39977","3938","you might be able to work me down to a partial path rather than a full path...
like if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.
then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.
But i'm still not sure how useful this is. 
It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old. who cares what the file name is?","you might be able to work me down to a partial path rather than a full path...
like if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1769","1769","39977","3938","you might be able to work me down to a partial path rather than a full path...
like if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.
then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.
But i'm still not sure how useful this is. 
It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old. who cares what the file name is?","then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"1770","1770","39977","3938","you might be able to work me down to a partial path rather than a full path...
like if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.
then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.
But i'm still not sure how useful this is. 
It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old. who cares what the file name is?","But i'm still not sure how useful this is.","rcmuir","NULL","1","con","0","0","0","1","0"
"1771","1771","39977","3938","you might be able to work me down to a partial path rather than a full path...
like if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.
then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.
But i'm still not sure how useful this is. 
It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old. who cares what the file name is?","It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old.","rcmuir","NULL","1","con","0","0","0","1","0"
"1772","1772","39977","3938","you might be able to work me down to a partial path rather than a full path...
like if IndexInput takes String name in its ctor (the same one passed to Directory.openInput NOT the full path, keeps String as a private variable), and implements toString itself.
then we wouldnt have to track additional variables in each indexinput impl, only change openinput and the ctors to pass this information.
But i'm still not sure how useful this is. 
It really seems like an implementation detail that we check the stored fields to determine if an indexformat is too old. who cares what the file name is?","who cares what the file name is?","rcmuir","NULL","1","con","0","0","0","1","0"
"1773","1773","39979","3938","+1, I like that solution, Robert.  I'll rework the patch...","+1, I like that solution, Robert.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1774","1774","39979","3938","+1, I like that solution, Robert.  I'll rework the patch...","I'll rework the patch...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1775","1775","39980","3938","New patch folding in Robert's idea....
I added final String resourceDescription to II, returned from
toString, made it required arg to the ctor, and fix all II subclasses
to pass something reasonable.
When our II impls originate an exception (eg from EOF), I also include
II.toString(); if a method they call throws IOE (eg file.read(...)
inside SimpleFSII), then I catch & rethrow w/ II.toString() included.
I also include the sub-file name when inside a sliced II (CFS/CFX);
I added a required arg (sliceDescription) to the sliceInput method
for this.","New patch folding in Robert's idea....","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1776","1776","39980","3938","New patch folding in Robert's idea....
I added final String resourceDescription to II, returned from
toString, made it required arg to the ctor, and fix all II subclasses
to pass something reasonable.
When our II impls originate an exception (eg from EOF), I also include
II.toString(); if a method they call throws IOE (eg file.read(...)
inside SimpleFSII), then I catch & rethrow w/ II.toString() included.
I also include the sub-file name when inside a sliced II (CFS/CFX);
I added a required arg (sliceDescription) to the sliceInput method
for this.","I added final String resourceDescription to II, returned from
toString, made it required arg to the ctor, and fix all II subclasses
to pass something reasonable.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1777","1777","39980","3938","New patch folding in Robert's idea....
I added final String resourceDescription to II, returned from
toString, made it required arg to the ctor, and fix all II subclasses
to pass something reasonable.
When our II impls originate an exception (eg from EOF), I also include
II.toString(); if a method they call throws IOE (eg file.read(...)
inside SimpleFSII), then I catch & rethrow w/ II.toString() included.
I also include the sub-file name when inside a sliced II (CFS/CFX);
I added a required arg (sliceDescription) to the sliceInput method
for this.","When our II impls originate an exception (eg from EOF), I also include
II.toString(); if a method they call throws IOE (eg file.read(...)
inside SimpleFSII), then I catch & rethrow w/ II.toString() included.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1778","1778","39980","3938","New patch folding in Robert's idea....
I added final String resourceDescription to II, returned from
toString, made it required arg to the ctor, and fix all II subclasses
to pass something reasonable.
When our II impls originate an exception (eg from EOF), I also include
II.toString(); if a method they call throws IOE (eg file.read(...)
inside SimpleFSII), then I catch & rethrow w/ II.toString() included.
I also include the sub-file name when inside a sliced II (CFS/CFX);
I added a required arg (sliceDescription) to the sliceInput method
for this.","I also include the sub-file name when inside a sliced II (CFS/CFX);
I added a required arg (sliceDescription) to the sliceInput method
for this.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1779","1779","39982","3938","OK, new patch; I changed to EOFE, and I just use in.toString() instead of special casing DI vs II.
I think it's ready!","OK, new patch; I changed to EOFE, and I just use in.toString() instead of special casing DI vs II.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1780","1780","39982","3938","OK, new patch; I changed to EOFE, and I just use in.toString() instead of special casing DI vs II.
I think it's ready!","I think it's ready!","mikemccand","NULL","1","pro","0","0","1","0","0"
"1781","1781","39983","3938","mike can we close LUCENE-3138 too?","mike can we close LUCENE-3138 too?","simonw","NULL","0",NULL,"0","0","0","0","0"
"1782","1782","39985","3938","Bulk close after release of 3.5","Bulk close after release of 3.5","thetaphi","NULL","1","decision","0","0","0","0","1"
"1783","1783","41160","4052","+1
We now use Lucene Hunspell for a few customer deployments, and it would be great to have it the analysis module, since it supports some 70-80 languages out of the box, and gives great flexibility since you can edit - or augment - the dictionaries to change behaviour and fix stemming bugs.
As a side benefit I also expect that when the Ooo dictionaries get more use in Lucene, users will over time be able to extend and improve the dictionaries, and contribute their changes back, benefiting also Ooo users.","+1
We now use Lucene Hunspell for a few customer deployments, and it would be great to have it the analysis module, since it supports some 70-80 languages out of the box, and gives great flexibility since you can edit - or augment - the dictionaries to change behaviour and fix stemming bugs.","janhoy","NULL","1","pro","0","0","1","0","0"
"1784","1784","41160","4052","+1
We now use Lucene Hunspell for a few customer deployments, and it would be great to have it the analysis module, since it supports some 70-80 languages out of the box, and gives great flexibility since you can edit - or augment - the dictionaries to change behaviour and fix stemming bugs.
As a side benefit I also expect that when the Ooo dictionaries get more use in Lucene, users will over time be able to extend and improve the dictionaries, and contribute their changes back, benefiting also Ooo users.","As a side benefit I also expect that when the Ooo dictionaries get more use in Lucene, users will over time be able to extend and improve the dictionaries, and contribute their changes back, benefiting also Ooo users.","janhoy","NULL","1","pro","0","0","1","0","0"
"1785","1785","41161","4052","Patch with a port of the code.
Because most of the dictionaries are L/GPL, I've written my own dumb stupid dictionary for test purposes.
During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.
Code now is also version aware, as required by the CharArray* data structures.","Patch with a port of the code.","cmale","NULL","1","alternative","0","1","0","0","0"
"1786","1786","41161","4052","Patch with a port of the code.
Because most of the dictionaries are L/GPL, I've written my own dumb stupid dictionary for test purposes.
During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.
Code now is also version aware, as required by the CharArray* data structures.","Because most of the dictionaries are L/GPL, I've written my own dumb stupid dictionary for test purposes.","cmale","NULL","1","alternative, con","0","1","0","1","0"
"1787","1787","41161","4052","Patch with a port of the code.
Because most of the dictionaries are L/GPL, I've written my own dumb stupid dictionary for test purposes.
During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.
Code now is also version aware, as required by the CharArray* data structures.","During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.","cmale","NULL","1","issue, decision","1","0","0","0","1"
"1788","1788","41161","4052","Patch with a port of the code.
Because most of the dictionaries are L/GPL, I've written my own dumb stupid dictionary for test purposes.
During testing I discovered a long standing bug to do with recursive application of rules This has now been fixed.
Code now is also version aware, as required by the CharArray* data structures.","Code now is also version aware, as required by the CharArray* data structures.","cmale","NULL","1","pro","0","0","1","0","0"
"1789","1789","41162","4052","Thanks Chris for adding this to Lucene Analysis module. We did lots of work on Google Code, so it should really be in Lucene, except the dictionaries. We should only add links to web pages where to get them.","Thanks Chris for adding this to Lucene Analysis module.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1790","1790","41162","4052","Thanks Chris for adding this to Lucene Analysis module. We did lots of work on Google Code, so it should really be in Lucene, except the dictionaries. We should only add links to web pages where to get them.","We did lots of work on Google Code, so it should really be in Lucene, except the dictionaries.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1791","1791","41162","4052","Thanks Chris for adding this to Lucene Analysis module. We did lots of work on Google Code, so it should really be in Lucene, except the dictionaries. We should only add links to web pages where to get them.","We should only add links to web pages where to get them.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1792","1792","41163","4052","...so it should really be in Lucene, except the dictionaries.
how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?","...so it should really be in Lucene, except the dictionaries.","simonw","NULL","1","alternative","0","1","0","0","0"
"1793","1793","41163","4052","...so it should really be in Lucene, except the dictionaries.
how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project?","simonw","NULL","1","issue","1","0","0","0","0"
"1794","1794","41163","4052","...so it should really be in Lucene, except the dictionaries.
how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?","Maybe the dictionaries are under ASL eventually?","simonw","NULL","1","alternative","0","1","0","0","0"
"1795","1795","41164","4052","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.  I guess thats something they will have to sort out during incubation.
I don't see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project?","cmale","NULL","1","issue","1","0","0","0","0"
"1796","1796","41164","4052","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.  I guess thats something they will have to sort out during incubation.
I don't see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.","Maybe the dictionaries are under ASL eventually?","cmale","NULL","1","alternative","0","1","0","0","0"
"1797","1797","41164","4052","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.  I guess thats something they will have to sort out during incubation.
I don't see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.","Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.","cmale","NULL","1","alternative, con","0","1","0","1","0"
"1798","1798","41164","4052","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.  I guess thats something they will have to sort out during incubation.
I don't see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.","I guess thats something they will have to sort out during incubation.","cmale","NULL","1","issue","1","0","0","0","0"
"1799","1799","41164","4052","how is OpenOffice dealing with those dictionaries since they are now an ASF incubation project? Maybe the dictionaries are under ASL eventually?
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.  I guess thats something they will have to sort out during incubation.
I don't see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.","I don't see the licenses changing since the dictionaries tend to be developed by national language organisations, but maybe the ASF will negotiate.","cmale","NULL","0",NULL,"0","0","0","0","0"
"1800","1800","41165","4052","
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license. 
I don't think we should read too much into that text file: its not even obvious which of the many dictionaries in that folder it applies to!
I know for a fact that some of the files in there are NOT GPL, for example the en_US dictionary: http://svn.apache.org/viewvc/incubator/ooo/trunk/main/dictionaries/en/README_en_US.txt?revision=1162288&view=markup","Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"1801","1801","41165","4052","
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license. 
I don't think we should read too much into that text file: its not even obvious which of the many dictionaries in that folder it applies to!
I know for a fact that some of the files in there are NOT GPL, for example the en_US dictionary: http://svn.apache.org/viewvc/incubator/ooo/trunk/main/dictionaries/en/README_en_US.txt?revision=1162288&view=markup","I don't think we should read too much into that text file: its not even obvious which of the many dictionaries in that folder it applies to!","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1802","1802","41165","4052","
Bizarrely, from what I can see in the OpenOffice SVN, they are still under their original license. 
I don't think we should read too much into that text file: its not even obvious which of the many dictionaries in that folder it applies to!
I know for a fact that some of the files in there are NOT GPL, for example the en_US dictionary: http://svn.apache.org/viewvc/incubator/ooo/trunk/main/dictionaries/en/README_en_US.txt?revision=1162288&view=markup","I know for a fact that some of the files in there are NOT GPL, for example the en_US dictionary: http://svn.apache.org/viewvc/incubator/ooo/trunk/main/dictionaries/en/README_en_US.txt?revision=1162288&view=markup","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1803","1803","41166","4052","Okay good spotting. so how do we want to proceed? Do we want to bring some of the dictionaries in? Should we address that in a later issue once its become clearer in OO what they're doing?","Okay good spotting.","cmale","NULL","0",NULL,"0","0","0","0","0"
"1804","1804","41166","4052","Okay good spotting. so how do we want to proceed? Do we want to bring some of the dictionaries in? Should we address that in a later issue once its become clearer in OO what they're doing?","so how do we want to proceed?","cmale","NULL","1","issue","1","0","0","0","0"
"1805","1805","41166","4052","Okay good spotting. so how do we want to proceed? Do we want to bring some of the dictionaries in? Should we address that in a later issue once its become clearer in OO what they're doing?","Do we want to bring some of the dictionaries in?","cmale","NULL","1","alternative","0","1","0","0","0"
"1806","1806","41166","4052","Okay good spotting. so how do we want to proceed? Do we want to bring some of the dictionaries in? Should we address that in a later issue once its become clearer in OO what they're doing?","Should we address that in a later issue once its become clearer in OO what they're doing?","cmale","NULL","1","issue","1","0","0","0","0"
"1807","1807","41168","4052","Patch now includes a package.html linking to a PDF about hunspell and suggesting dictionaries are sourced from the OpenOffice wiki.
Committing tomorrow.","Patch now includes a package.html linking to a PDF about hunspell and suggesting dictionaries are sourced from the OpenOffice wiki.","cmale","NULL","1","alternative","0","1","0","0","0"
"1808","1808","41168","4052","Patch now includes a package.html linking to a PDF about hunspell and suggesting dictionaries are sourced from the OpenOffice wiki.
Committing tomorrow.","Committing tomorrow.","cmale","NULL","1","decision","0","0","0","0","1"
"1809","1809","41169","4052","Committed revision 1167467.","Committed revision 1167467.","cmale","NULL","1","decision","0","0","0","0","1"
"1810","1810","41170","4052","Reopening for 3x backport.","Reopening for 3x backport.","cmale","NULL","1","decision","0","0","0","0","1"
"1811","1811","41171","4052","3x back port:
Committed revision 1167505.","3x back port:
Committed revision 1167505.","cmale","NULL","1","decision","0","0","0","0","1"
"1812","1812","41172","4052","Is there a JIRA for adding HunspellStemFilterFactory to Solr?","Is there a JIRA for adding HunspellStemFilterFactory to Solr?","janhoy","NULL","1","issue","1","0","0","0","0"
"1813","1813","41173","4052","Nope, its on my mental TODO but go for it.","Nope, its on my mental TODO but go for it.","cmale","NULL","0",NULL,"0","0","0","0","0"
"1814","1814","41174","4052","SOLR-2769","SOLR-2769","janhoy","NULL","0",NULL,"0","0","0","0","0"
"1815","1815","41175","4052","Bulk close after release of 3.5","Bulk close after release of 3.5","thetaphi","NULL","1","decision","0","0","0","0","1"
"1816","1816","42700","4218","as a start, i installed the two freebsd ports for java doc on hudson into /usr/local/share/doc/jdk1.5 and jdk1.6
I'll see if i can add the hooks to the build scripts now","as a start, i installed the two freebsd ports for java doc on hudson into /usr/local/share/doc/jdk1.5 and jdk1.6
I'll see if i can add the hooks to the build scripts now","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1817","1817","42701","4218","As a partial solution, I setup the 30 minute builds to just directly override javadoc.link (and javadoc.link.java for Solr) for our 30 minute builds... we don't care about the actual javadoc artifacts or where the links actually point to, only that there are no warnings.
This is in r1138418","As a partial solution, I setup the 30 minute builds to just directly override javadoc.link (and javadoc.link.java for Solr) for our 30 minute builds... we don't care about the actual javadoc artifacts or where the links actually point to, only that there are no warnings.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1818","1818","42701","4218","As a partial solution, I setup the 30 minute builds to just directly override javadoc.link (and javadoc.link.java for Solr) for our 30 minute builds... we don't care about the actual javadoc artifacts or where the links actually point to, only that there are no warnings.
This is in r1138418","This is in r1138418","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1819","1819","42703","4218","+1
I think we should allow you optionally set a sysprop using linkoffline
hell, why bother with the sysprop? .. lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.
","+1
I think we should allow you optionally set a sysprop using linkoffline
hell, why bother with the sysprop?","hossman","NULL","1","alternative, pro, con","0","1","1","1","0"
"1820","1820","42703","4218","+1
I think we should allow you optionally set a sysprop using linkoffline
hell, why bother with the sysprop? .. lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.
",".. lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.","hossman","NULL","1","alternative","0","1","0","0","0"
"1821","1821","42704","4218","lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.
+1
Hitting build failures because we can't download these package lists is silly.","lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1822","1822","42704","4218","lets just commit the package-list files for all third party libs we use into dev-tools and completely eliminate the need for net when building javadocs.
+1
Hitting build failures because we can't download these package lists is silly.","+1
Hitting build failures because we can't download these package lists is silly.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1823","1823","42705","4218","I agree with hossman too. I'm just a javadocs dummy and was doing what I could to stop the 30minute builds.
I cant figure out this linkoffline (at least with my experiments its confusing)... but this sounds great.","I agree with hossman too.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1824","1824","42705","4218","I agree with hossman too. I'm just a javadocs dummy and was doing what I could to stop the 30minute builds.
I cant figure out this linkoffline (at least with my experiments its confusing)... but this sounds great.","I'm just a javadocs dummy and was doing what I could to stop the 30minute builds.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1825","1825","42705","4218","I agree with hossman too. I'm just a javadocs dummy and was doing what I could to stop the 30minute builds.
I cant figure out this linkoffline (at least with my experiments its confusing)... but this sounds great.","I cant figure out this linkoffline (at least with my experiments its confusing)... but this sounds great.","rcmuir","NULL","1","pro, con","0","0","1","1","0"
"1826","1826","42708","4218","I propose switching to the oracle.com link suggested by Chris Male:
http://download.oracle.com/javase/6/docs/api/package-list apparently works reliably.
This would be lots simpler than trying to figure out dev-tools etc., assuming that this link is indeed reliable.","I propose switching to the oracle.com link suggested by Chris Male:
http://download.oracle.com/javase/6/docs/api/package-list apparently works reliably.","steve_rowe","NULL","1","alternative, pro","0","1","1","0","0"
"1827","1827","42708","4218","I propose switching to the oracle.com link suggested by Chris Male:
http://download.oracle.com/javase/6/docs/api/package-list apparently works reliably.
This would be lots simpler than trying to figure out dev-tools etc., assuming that this link is indeed reliable.","This would be lots simpler than trying to figure out dev-tools etc., assuming that this link is indeed reliable.","steve_rowe","NULL","1","pro","0","0","1","0","0"
"1828","1828","42709","4218","+1","+1","cmale","NULL","1","pro","0","0","1","0","0"
"1829","1829","42710","4218","+1 let's try this and see if it is indeed reliable.","+1 let's try this and see if it is indeed reliable.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1830","1830","42711","4218","Just one other idea:

We already have JAVA_HOME set (direct or implicitely set by ANT)
The Javadocs are always at same location in $JAVA_HOME

Could we not use this to point to the package list (at least fpr the JDK part). I don't like the hardcoded package list.","Just one other idea:

We already have JAVA_HOME set (direct or implicitely set by ANT)
The Javadocs are always at same location in $JAVA_HOME

Could we not use this to point to the package list (at least fpr the JDK part).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1831","1831","42711","4218","Just one other idea:

We already have JAVA_HOME set (direct or implicitely set by ANT)
The Javadocs are always at same location in $JAVA_HOME

Could we not use this to point to the package list (at least fpr the JDK part). I don't like the hardcoded package list.","I don't like the hardcoded package list.","thetaphi","NULL","1","con","0","0","0","1","0"
"1832","1832","42712","4218","The Javadocs are always at same location in $JAVA_HOME
I looked at JAVA_HOME on Windows for 1.5.0_22 and 1.6.0_23 (both 64 bit JDKs), and neither included Javadocs.  Maybe they're separately downloadable?","The Javadocs are always at same location in $JAVA_HOME
I looked at JAVA_HOME on Windows for 1.5.0_22 and 1.6.0_23 (both 64 bit JDKs), and neither included Javadocs.","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1833","1833","42712","4218","The Javadocs are always at same location in $JAVA_HOME
I looked at JAVA_HOME on Windows for 1.5.0_22 and 1.6.0_23 (both 64 bit JDKs), and neither included Javadocs.  Maybe they're separately downloadable?","Maybe they're separately downloadable?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1834","1834","42713","4218","Sorry, you are right. I have it here, but the README.html in JDK's root folder says:

JDK(TM) Documentation
The on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information. This documentation is also available in a download bundle which you can install on your machine. To obtain the documentation bundle, see the download page. For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.","Sorry, you are right.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1835","1835","42713","4218","Sorry, you are right. I have it here, but the README.html in JDK's root folder says:

JDK(TM) Documentation
The on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information. This documentation is also available in a download bundle which you can install on your machine. To obtain the documentation bundle, see the download page. For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.","I have it here, but the README.html in JDK's root folder says:

JDK(TM) Documentation
The on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1836","1836","42713","4218","Sorry, you are right. I have it here, but the README.html in JDK's root folder says:

JDK(TM) Documentation
The on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information. This documentation is also available in a download bundle which you can install on your machine. To obtain the documentation bundle, see the download page. For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.","This documentation is also available in a download bundle which you can install on your machine.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1837","1837","42713","4218","Sorry, you are right. I have it here, but the README.html in JDK's root folder says:

JDK(TM) Documentation
The on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information. This documentation is also available in a download bundle which you can install on your machine. To obtain the documentation bundle, see the download page. For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.","To obtain the documentation bundle, see the download page.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1838","1838","42713","4218","Sorry, you are right. I have it here, but the README.html in JDK's root folder says:

JDK(TM) Documentation
The on-line JavaTM Platform, Standard Edition (Java SE) Documentation contains API specifications, feature descriptions, developer guides, reference pages for JDKTM tools and utilities, demos, and links to related information. This documentation is also available in a download bundle which you can install on your machine. To obtain the documentation bundle, see the download page. For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.","For API documentation, refer to the The JavaTM Platform, Standard Edition API Specification This provides brief descriptions of the API with an emphasis on specifications, not on code examples.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1839","1839","42714","4218","sarowe asked about this issue in LUCENE-3587.
FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn't post them in Jira.
Problems with the patch i attached (that i know of):
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.
I don't think there's anything left here but to resolve as Won't Fix","sarowe asked about this issue in LUCENE-3587.","hossman","NULL","0",NULL,"0","0","0","0","0"
"1840","1840","42714","4218","sarowe asked about this issue in LUCENE-3587.
FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn't post them in Jira.
Problems with the patch i attached (that i know of):
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.
I don't think there's anything left here but to resolve as Won't Fix","FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn't post them in Jira.","hossman","NULL","1","issue","1","0","0","0","0"
"1841","1841","42714","4218","sarowe asked about this issue in LUCENE-3587.
FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn't post them in Jira.
Problems with the patch i attached (that i know of):
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.
I don't think there's anything left here but to resolve as Won't Fix","Problems with the patch i attached (that i know of):
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.","hossman","NULL","1","alternative, con","0","1","0","1","0"
"1842","1842","42714","4218","sarowe asked about this issue in LUCENE-3587.
FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn't post them in Jira.
Problems with the patch i attached (that i know of):
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.
I don't think there's anything left here but to resolve as Won't Fix","2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.","hossman","NULL","1","alternative, con","0","1","0","1","0"
"1843","1843","42714","4218","sarowe asked about this issue in LUCENE-3587.
FWIW, i thought this issue (LUCENE-3228) had been resolved a long time ago based on some comments i remember people making, but evidently those comments where on irc/mail and folks didn't post them in Jira.
Problems with the patch i attached (that i know of):
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
2) the Java documentation from Oracle has some licensing/restrictions  that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
...we could still use the ideas in this patch to deal with package-list files for non java/oracle distrobutions, but at the time this patch was written the only other extneral javadocs we linked to where that might be useful was junit, and since this patch was created, that link has just been removed outright from our build.xml files.
I don't think there's anything left here but to resolve as Won't Fix","I don't think there's anything left here but to resolve as Won't Fix","hossman","NULL","1","con, decision","0","0","0","1","1"
"1844","1844","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","I don't think there's anything left here but to resolve as Won't Fix
I disagree.","steve_rowe","NULL","1","con","0","0","0","1","0"
"1845","1845","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.","steve_rowe","NULL","1","con","0","0","0","1","0"
"1846","1846","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","How does lucene/src/tools/javadoc/ grab you?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"1847","1847","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.","steve_rowe","NULL","1","con","0","0","0","1","0"
"1848","1848","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1849","1849","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1850","1850","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","I can't find any associated discussion of legal ramifications, though.","steve_rowe","NULL","1","issue","1","0","0","0","0"
"1851","1851","42715","4218","I don't think there's anything left here but to resolve as Won't Fix
I disagree.
1) we don't distribute dev-tools in our releases, so at a minimum we'd need to find a new home for any package-list files we wanted to ship.
How does lucene/src/tools/javadoc/ grab you?
2) the Java documentation from Oracle has some licensing/restrictions that affect redistribution which don't seem to be compatible with ASF 3rd party licensing policy so we can't include the java package-list files in our releases
Here is a direct link to the Sun/Oracle documentation redistribution restrictions: http://java.sun.com/docs/redist.html.
We can include the Java javadoc package-list file in Subversion but exclude it from our source releases, and include a mechanism to download it when it's absent (i.e., in the source release).
There is an ASF precedent for redistributing Java javadoc package-list files in the Maven project's javadoc plugin: http://jira.codehaus.org/browse/MJAVADOC-315 and https://jira.codehaus.org/browse/MJAVADOC-327 - in Subversion at http://svn.apache.org/viewvc/maven/plugins/trunk/maven-javadoc-plugin/src/main/resources/org/apache/maven/plugin/javadoc/.  I can't find any associated discussion of legal ramifications, though.
I'll put up a patch shortly implementing these ideas.","I'll put up a patch shortly implementing these ideas.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1852","1852","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","Patch for branch_3x.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1853","1853","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1854","1854","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1855","1855","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1856","1856","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1857","1857","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1858","1858","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","Links from Solr javadocs to Lucene's javadocs are enabled.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1859","1859","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1860","1860","42716","4218","Patch for branch_3x.  Features:

Adds package-list files for Oracle Java javadocs and JUnit javadocs to subversion.
When building Lucene and Solr source releases, the Oracle Java javadocs package-list file is removed.
When connected or disconnected from the network, javadocs built from a subversion checkout contain links to Oracle javadocs.
When connected to the network, javadocs built from a source release will attempt to download the Oracle Java package-list file.
When the Oracle Java package-list file is not present, either because the user is building from a source release while disconnected from the network, or because the package-list file for Oracle Java javadocs is not downloadable for some other reason, javadocs will be built and the build will not fail, though an error will appear in the build log.
Links from Solr javadocs to Lucene's javadocs are enabled.  When building a non-release version, the links are to the most recently built nightly Jenkins javadocs, as in Hoss's patch on this issue.  When building a release version, links are to the same-versioned Lucene release javadocs.

","When building a release version, links are to the same-versioned Lucene release javadocs.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1861","1861","42717","4218","Trunk for patch with the same changes.","Trunk for patch with the same changes.","steve_rowe","NULL","1","alternative","0","1","0","0","0"
"1862","1862","42718","4218","If there are no objections I will commit this tomorrow.","If there are no objections I will commit this tomorrow.","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1863","1863","42719","4218","Committed:

r1210020: trunk
r1210022: branch_3x

","Committed:

r1210020: trunk
r1210022: branch_3x","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1864","1864","43263","4269","
    [junit] Testsuite: org.apache.lucene.index.TestNRTThreads
    [junit] Testcase: testNRTThreads(org.apache.lucene.index.TestNRTThreads):	FAILED
    [junit] expected:<8> but was:<18>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<18>
    [junit] 	at org.apache.lucene.index.TestNRTThreads.testNRTThreads(TestNRTThreads.java:515)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 19.812 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] doc id=157 is supposed to be deleted, but got docID=119
    [junit] doc id=82 is supposed to be deleted, but got docID=68
    [junit] doc id=83 is supposed to be deleted, but got docID=38
    [junit] doc id=80 is supposed to be deleted, but got docID=36
    [junit] doc id=81 is supposed to be deleted, but got docID=37
    [junit] doc id=67 is supposed to be deleted, but got docID=24
    [junit] doc id=69 is supposed to be deleted, but got docID=26
    [junit] doc id=68 is supposed to be deleted, but got docID=25
    [junit] doc id=672 is supposed to be deleted, but got docID=430
    [junit] doc id=444 is supposed to be deleted, but got docID=344
    [junit] doc id=441 is supposed to be deleted, but got docID=766
    [junit] doc id=442 is supposed to be deleted, but got docID=343
    [junit] doc id=443 is supposed to be deleted, but got docID=767
    [junit] doc id=70 is supposed to be deleted, but got docID=67
    [junit] doc id=71 is supposed to be deleted, but got docID=27
    [junit] doc id=72 is supposed to be deleted, but got docID=28
    [junit] doc id=73 is supposed to be deleted, but got docID=29
    [junit] doc id=74 is supposed to be deleted, but got docID=30
    [junit] doc id=75 is supposed to be deleted, but got docID=31
    [junit] doc id=76 is supposed to be deleted, but got docID=32
    [junit] doc id=219 is supposed to be deleted, but got docID=175
    [junit] doc id=662 is supposed to be deleted, but got docID=425
    [junit] doc id=663 is supposed to be deleted, but got docID=426
    [junit] doc id=218 is supposed to be deleted, but got docID=174
    [junit] doc id=361 is supposed to be deleted, but got docID=286
    [junit] doc id=362 is supposed to be deleted, but got docID=287
    [junit] doc id=360 is supposed to be deleted, but got docID=285
    [junit] doc id=366 is supposed to be deleted, but got docID=291
    [junit] doc id=365 is supposed to be deleted, but got docID=290
    [junit] doc id=364 is supposed to be deleted, but got docID=289
    [junit] doc id=363 is supposed to be deleted, but got docID=288
    [junit] doc id=368 is supposed to be deleted, but got docID=293
    [junit] doc id=367 is supposed to be deleted, but got docID=292
    [junit] doc id=518 is supposed to be deleted, but got docID=361
    [junit] doc id=517 is supposed to be deleted, but got docID=805
    [junit] doc id=220 is supposed to be deleted, but got docID=176
    [junit] doc id=324 is supposed to be deleted, but got docID=269
    [junit] doc id=322 is supposed to be deleted, but got docID=268
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestNRTThreads -Dtestmethod=testNRTThreads -Dtests.seed=0:0
    [junit] NOTE: test params are: codec=RandomCodecProvider: {extra8=MockFixedIntBlock(blockSize=1054), extra9=MockVariableIntBlock(baseBlockSize=87), body=MockSep, extra0=MockVariableIntBlock(baseBlockSize=87), packID=Pulsing(freqCutoff=16), extra1=MockRandom, extra2=Standard, extra3=SimpleText, date=MockVariableIntBlock(baseBlockSize=87), extra4=MockSep, extra5=Pulsing(freqCutoff=16), extra6=MockFixedIntBlock(blockSize=1054), extra7=MockVariableIntBlock(baseBlockSize=87), docid=MockVariableIntBlock(baseBlockSize=87), title=SimpleText, titleTokenized=Standard}, locale=ar_JO, timezone=Europe/Oslo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSearchForDuplicates, TestMockAnalyzer, TestCheckIndex, TestDoc, TestFlex, TestIndexReaderCloneNorms, TestIndexWriterExceptions, TestIndexWriterUnicode, TestMultiLevelSkipList, TestNRTThreads]
    [junit] NOTE: Mac OS X 10.6.7 x86_64/Apple Inc. 1.6.0_24 (64-bit)/cpus=4,threads=1,free=41147720,total=85000192
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestNRTThreads FAILED

","
    [junit] Testsuite: org.apache.lucene.index.TestNRTThreads
    [junit] Testcase: testNRTThreads(org.apache.lucene.index.TestNRTThreads):	FAILED
    [junit] expected:<8> but was:<18>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<18>
    [junit] 	at org.apache.lucene.index.TestNRTThreads.testNRTThreads(TestNRTThreads.java:515)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 19.812 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] doc id=157 is supposed to be deleted, but got docID=119
    [junit] doc id=82 is supposed to be deleted, but got docID=68
    [junit] doc id=83 is supposed to be deleted, but got docID=38
    [junit] doc id=80 is supposed to be deleted, but got docID=36
    [junit] doc id=81 is supposed to be deleted, but got docID=37
    [junit] doc id=67 is supposed to be deleted, but got docID=24
    [junit] doc id=69 is supposed to be deleted, but got docID=26
    [junit] doc id=68 is supposed to be deleted, but got docID=25
    [junit] doc id=672 is supposed to be deleted, but got docID=430
    [junit] doc id=444 is supposed to be deleted, but got docID=344
    [junit] doc id=441 is supposed to be deleted, but got docID=766
    [junit] doc id=442 is supposed to be deleted, but got docID=343
    [junit] doc id=443 is supposed to be deleted, but got docID=767
    [junit] doc id=70 is supposed to be deleted, but got docID=67
    [junit] doc id=71 is supposed to be deleted, but got docID=27
    [junit] doc id=72 is supposed to be deleted, but got docID=28
    [junit] doc id=73 is supposed to be deleted, but got docID=29
    [junit] doc id=74 is supposed to be deleted, but got docID=30
    [junit] doc id=75 is supposed to be deleted, but got docID=31
    [junit] doc id=76 is supposed to be deleted, but got docID=32
    [junit] doc id=219 is supposed to be deleted, but got docID=175
    [junit] doc id=662 is supposed to be deleted, but got docID=425
    [junit] doc id=663 is supposed to be deleted, but got docID=426
    [junit] doc id=218 is supposed to be deleted, but got docID=174
    [junit] doc id=361 is supposed to be deleted, but got docID=286
    [junit] doc id=362 is supposed to be deleted, but got docID=287
    [junit] doc id=360 is supposed to be deleted, but got docID=285
    [junit] doc id=366 is supposed to be deleted, but got docID=291
    [junit] doc id=365 is supposed to be deleted, but got docID=290
    [junit] doc id=364 is supposed to be deleted, but got docID=289
    [junit] doc id=363 is supposed to be deleted, but got docID=288
    [junit] doc id=368 is supposed to be deleted, but got docID=293
    [junit] doc id=367 is supposed to be deleted, but got docID=292
    [junit] doc id=518 is supposed to be deleted, but got docID=361
    [junit] doc id=517 is supposed to be deleted, but got docID=805
    [junit] doc id=220 is supposed to be deleted, but got docID=176
    [junit] doc id=324 is supposed to be deleted, but got docID=269
    [junit] doc id=322 is supposed to be deleted, but got docID=268
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestNRTThreads -Dtestmethod=testNRTThreads -Dtests.seed=0:0
    [junit] NOTE: test params are: codec=RandomCodecProvider: {extra8=MockFixedIntBlock(blockSize=1054), extra9=MockVariableIntBlock(baseBlockSize=87), body=MockSep, extra0=MockVariableIntBlock(baseBlockSize=87), packID=Pulsing(freqCutoff=16), extra1=MockRandom, extra2=Standard, extra3=SimpleText, date=MockVariableIntBlock(baseBlockSize=87), extra4=MockSep, extra5=Pulsing(freqCutoff=16), extra6=MockFixedIntBlock(blockSize=1054), extra7=MockVariableIntBlock(baseBlockSize=87), docid=MockVariableIntBlock(baseBlockSize=87), title=SimpleText, titleTokenized=Standard}, locale=ar_JO, timezone=Europe/Oslo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSearchForDuplicates, TestMockAnalyzer, TestCheckIndex, TestDoc, TestFlex, TestIndexReaderCloneNorms, TestIndexWriterExceptions, TestIndexWriterUnicode, TestMultiLevelSkipList, TestNRTThreads]
    [junit] NOTE: Mac OS X 10.6.7 x86_64/Apple Inc. 1.6.0_24 (64-bit)/cpus=4,threads=1,free=41147720,total=85000192
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestNRTThreads FAILED","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1865","1865","43265","4269","It's probably the new DWPT code.  There was a specific issue to fix this problem LUCENE-2956.","It's probably the new DWPT code.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"1866","1866","43265","4269","It's probably the new DWPT code.  There was a specific issue to fix this problem LUCENE-2956.","There was a specific issue to fix this problem LUCENE-2956.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"1867","1867","43266","4269","phew! This seems like a delete issue. I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....
I will look after buzzwords","phew!","simonw","NULL","0",NULL,"0","0","0","0","0"
"1868","1868","43266","4269","phew! This seems like a delete issue. I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....
I will look after buzzwords","This seems like a delete issue.","simonw","NULL","1","issue","1","0","0","0","0"
"1869","1869","43266","4269","phew! This seems like a delete issue. I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....
I will look after buzzwords","I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....","simonw","NULL","1","issue","1","0","0","0","0"
"1870","1870","43266","4269","phew! This seems like a delete issue. I only looked at the output robert posted so far but it seems that a FrozenDelPackage gets lost somewhere here....
I will look after buzzwords","I will look after buzzwords","simonw","NULL","0",NULL,"0","0","0","0","0"
"1871","1871","43267","4269","I can reproduce this easily and even if I set search threads to 0 and index threads to 1. I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore. It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.
this is not a DWPT issue, phew!","I can reproduce this easily and even if I set search threads to 0 and index threads to 1.","simonw","NULL","1","issue","1","0","0","0","0"
"1872","1872","43267","4269","I can reproduce this easily and even if I set search threads to 0 and index threads to 1. I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore. It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.
this is not a DWPT issue, phew!","I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore.","simonw","NULL","1","alternative, pro","0","1","1","0","0"
"1873","1873","43267","4269","I can reproduce this easily and even if I set search threads to 0 and index threads to 1. I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore. It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.
this is not a DWPT issue, phew!","It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.","simonw","NULL","1","issue","1","0","0","0","0"
"1874","1874","43267","4269","I can reproduce this easily and even if I set search threads to 0 and index threads to 1. I forced the IW to use OpenMode.CREATE and suddenly the tests are not failing anymore. It seems that the tempdir is not cleaned up since always the second test fails for me but never the first run.
this is not a DWPT issue, phew!","this is not a DWPT issue, phew!","simonw","NULL","1","issue","1","0","0","0","0"
"1875","1875","43268","4269","the test cleans itself up in afterClass(), so there is in fact an issue.","the test cleans itself up in afterClass(), so there is in fact an issue.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1876","1876","43269","4269","taking a look at this, I don't like the way _TestUtil.getTempDir(String desc) was working before... it was basically desc + LTC.random.nextInt(xxx), so if you wired the seed like I did, and somehow stuff doesnt totally clean up, then its easy to see how it could return an already-created dir.
I changed this method to use _TestUtil.createTempFile... I think this is much safer.","taking a look at this, I don't like the way _TestUtil.getTempDir(String desc) was working before... it was basically desc + LTC.random.nextInt(xxx), so if you wired the seed like I did, and somehow stuff doesnt totally clean up, then its easy to see how it could return an already-created dir.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1877","1877","43269","4269","taking a look at this, I don't like the way _TestUtil.getTempDir(String desc) was working before... it was basically desc + LTC.random.nextInt(xxx), so if you wired the seed like I did, and somehow stuff doesnt totally clean up, then its easy to see how it could return an already-created dir.
I changed this method to use _TestUtil.createTempFile... I think this is much safer.","I changed this method to use _TestUtil.createTempFile...","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1878","1878","43269","4269","taking a look at this, I don't like the way _TestUtil.getTempDir(String desc) was working before... it was basically desc + LTC.random.nextInt(xxx), so if you wired the seed like I did, and somehow stuff doesnt totally clean up, then its easy to see how it could return an already-created dir.
I changed this method to use _TestUtil.createTempFile... I think this is much safer.","I think this is much safer.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1879","1879","43270","4269","robert can you still reproduce or can we close this issue here?","robert can you still reproduce or can we close this issue here?","simonw","NULL","0",NULL,"0","0","0","0","0"
"1880","1880","43271","4269","i could never really reproduce... but sometimes if i ran all tests with -Dtests.seed=0:0 it would happen.
the reason this test is not reproducible is that this test uses 'n seconds' as a limit.
so whether it passes or fails depends upon what your computer is doing at the moment.
I think we must change it to limit itself by number of docs instead.","i could never really reproduce... but sometimes if i ran all tests with -Dtests.seed=0:0 it would happen.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1881","1881","43271","4269","i could never really reproduce... but sometimes if i ran all tests with -Dtests.seed=0:0 it would happen.
the reason this test is not reproducible is that this test uses 'n seconds' as a limit.
so whether it passes or fails depends upon what your computer is doing at the moment.
I think we must change it to limit itself by number of docs instead.","the reason this test is not reproducible is that this test uses 'n seconds' as a limit.","rcmuir","NULL","1","con","0","0","0","1","0"
"1882","1882","43271","4269","i could never really reproduce... but sometimes if i ran all tests with -Dtests.seed=0:0 it would happen.
the reason this test is not reproducible is that this test uses 'n seconds' as a limit.
so whether it passes or fails depends upon what your computer is doing at the moment.
I think we must change it to limit itself by number of docs instead.","so whether it passes or fails depends upon what your computer is doing at the moment.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1883","1883","43271","4269","i could never really reproduce... but sometimes if i ran all tests with -Dtests.seed=0:0 it would happen.
the reason this test is not reproducible is that this test uses 'n seconds' as a limit.
so whether it passes or fails depends upon what your computer is doing at the moment.
I think we must change it to limit itself by number of docs instead.","I think we must change it to limit itself by number of docs instead.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1884","1884","43272","4269","I think we must change it to limit itself by number of docs instead.
I agree: let's fix that.","I think we must change it to limit itself by number of docs instead.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1885","1885","43272","4269","I think we must change it to limit itself by number of docs instead.
I agree: let's fix that.","I agree: let's fix that.","mikemccand","NULL","1","pro","0","0","1","0","0"
"1886","1886","43273","4269","this was a temp file issue - fixed","this was a temp file issue - fixed","simonw","NULL","1","issue, decision","1","0","0","0","1"
"1887","1887","43312","4274","Patch.  The basic impl is working, I think (the random test passes),
but I have alot of nocommits still!","Patch.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1888","1888","43312","4274","Patch.  The basic impl is working, I think (the random test passes),
but I have alot of nocommits still!","The basic impl is working, I think (the random test passes),
but I have alot of nocommits still!","mikemccand","NULL","1","con","0","0","0","1","0"
"1889","1889","43313","4274","New patch, I think it's ready to commit!","New patch, I think it's ready to commit!","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"1890","1890","43314","4274","BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes. When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.
I would prefer to use single pass and for now I only need the parent docs. That means that I have no preference for 2454 or this one.","BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes.","paul.elschot@xs4all.nl","NULL","1","alternative, con","0","1","0","1","0"
"1891","1891","43314","4274","BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes. When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.
I would prefer to use single pass and for now I only need the parent docs. That means that I have no preference for 2454 or this one.","When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"1892","1892","43314","4274","BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes. When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.
I would prefer to use single pass and for now I only need the parent docs. That means that I have no preference for 2454 or this one.","This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"1893","1893","43314","4274","BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes. When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.
I would prefer to use single pass and for now I only need the parent docs. That means that I have no preference for 2454 or this one.","I would prefer to use single pass and for now I only need the parent docs.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"1894","1894","43314","4274","BlockJoinQuery still needs hashCode/equals, and a javadoc note (as I remarked earlier at 2454) about the possible inefficiency of the use of OpenBitSet for larger group sizes. When the typical group size gets a lot bigger than the number of bits in a long, another implementation might be faster. This remark the in javadocs would allow us to wait for someone to come along with bigger group sizes and a real performance problem here.
I would prefer to use single pass and for now I only need the parent docs. That means that I have no preference for 2454 or this one.","That means that I have no preference for 2454 or this one.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"1895","1895","43316","4274","Patch, adding equals and hashCode and clone to BlockJoinQuery.  Also, I now throw UOE from get/setBoost, stating that you should do so against the child query instead.","Patch, adding equals and hashCode and clone to BlockJoinQuery.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1896","1896","43316","4274","Patch, adding equals and hashCode and clone to BlockJoinQuery.  Also, I now throw UOE from get/setBoost, stating that you should do so against the child query instead.","Also, I now throw UOE from get/setBoost, stating that you should do so against the child query instead.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1897","1897","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"1898","1898","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"1899","1899","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","32), and all integer values relative to the minimum determined by the data for the pointer from the parent.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"1900","1900","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"1901","1901","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"1902","1902","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","Compression at higher levels might not be necessary.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"1903","1903","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","For now, there is no code for this, except for the frame of reference.","paul.elschot@xs4all.nl","NULL","0",NULL,"0","0","0","0","0"
"1904","1904","43317","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Another implementation (should be another issue, but since you asked...) could be a set of increasing integers, based on a balanced tree structure with a moderate fanout (e.g. 32), and all integer values relative to the minimum determined by the data for the pointer from the parent. The whole thing could be stored in one int[], the pointers would be (forward) indexes into this one array, and each internal node would consist of two rows of integers (one data, one pointers), and each row would be compressed as a frame of reference into the array.
This thing can implement 

int next(int x)

 and 

int previous(int x)

 easily, and an iterator over this can implement 

advance(target)

 for a DocIdSetIterator, and because of the symmetry it can also do that in the reverse direction as needed here.
Compression at higher levels might not be necessary.
For now, there is no code for this, except for the frame of reference.
Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...
","Occasionaly the need for a more space efficient filter shows up on the mailing lists, so if anyone wants to give this a try...","paul.elschot@xs4all.nl","NULL","1","issue","1","0","0","0","0"
"1905","1905","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.","mikemccand","NULL","1","con","0","0","0","1","0"
"1906","1906","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","Ahh, OK.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1907","1907","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?","mikemccand","NULL","1","pro","0","0","1","0","0"
"1908","1908","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?","mikemccand","NULL","1","pro","0","0","1","0","0"
"1909","1909","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","(Unless the query hits few results).","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1910","1910","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?","mikemccand","NULL","1","alternative, pro, con","0","1","1","1","0"
"1911","1911","43318","4274","The possible inefficiency is the same as the one for a any sparsely filled OpenBitSet.
Ahh, OK.  Though, I suspect this (the linear scan OBS does for next/prevSetBit) is a minor cost overall, if indeed the app has so many child docs per parent that a sparse bit set would be warranted?  Ie, the Query/Collector would still be visiting these many child docs per parent, I guess?  (Unless the query hits few results).
I don't think a jdoc warning is really required for this... but I'm fine if you want to add one?
I'll commit this soon and resolve LUCENE-2454 as duplicate!","I'll commit this soon and resolve LUCENE-2454 as duplicate!","mikemccand","NULL","1","decision","0","0","0","0","1"
"1912","1912","43319","4274","Is there a wiki page on how to use this?  I need to implement an index with nested docs and an example scheme and query would be awesome. Thanks!","Is there a wiki page on how to use this?","dwebb","NULL","0",NULL,"0","0","0","0","0"
"1913","1913","43319","4274","Is there a wiki page on how to use this?  I need to implement an index with nested docs and an example scheme and query would be awesome. Thanks!","I need to implement an index with nested docs and an example scheme and query would be awesome.","dwebb","NULL","1","issue","1","0","0","0","0"
"1914","1914","43319","4274","Is there a wiki page on how to use this?  I need to implement an index with nested docs and an example scheme and query would be awesome. Thanks!","Thanks!","dwebb","NULL","0",NULL,"0","0","0","0","0"
"1915","1915","43320","4274","I wrote this blog post giving a quick overview: http://blog.mikemccandless.com/2012/01/searching-relational-content-with.html","I wrote this blog post giving a quick overview: http://blog.mikemccandless.com/2012/01/searching-relational-content-with.html","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1916","1916","43610","4306","+1","+1","rcmuir","NULL","1","pro","0","0","1","0","0"
"1917","1917","43611","4306","Perhaps we should also fail the test if that happens? Was there reason why only the stacktrace printed, but tests were considered successful?","Perhaps we should also fail the test if that happens?","shaie","NULL","1","alternative","0","1","0","0","0"
"1918","1918","43611","4306","Perhaps we should also fail the test if that happens? Was there reason why only the stacktrace printed, but tests were considered successful?","Was there reason why only the stacktrace printed, but tests were considered successful?","shaie","NULL","1","issue","1","0","0","0","0"
"1919","1919","43612","4306","some tests are still problematic, at least on windows... I think perhaps some of the crazier ones like DiskFull, TestCrash, anything that has to disable MockDirectoryWrappers's checks because they must create corrupt indexes or other scary things.","some tests are still problematic, at least on windows...","rcmuir","NULL","1","issue","1","0","0","0","0"
"1920","1920","43612","4306","some tests are still problematic, at least on windows... I think perhaps some of the crazier ones like DiskFull, TestCrash, anything that has to disable MockDirectoryWrappers's checks because they must create corrupt indexes or other scary things.","I think perhaps some of the crazier ones like DiskFull, TestCrash, anything that has to disable MockDirectoryWrappers's checks because they must create corrupt indexes or other scary things.","rcmuir","NULL","1","issue","1","0","0","0","0"
"1921","1921","43613","4306","Patch adds registerTempFile to LTC plus prints stack information if rmDir fails.
I think we should also fail the test if that happens?","Patch adds registerTempFile to LTC plus prints stack information if rmDir fails.","shaie","NULL","1","alternative","0","1","0","0","0"
"1922","1922","43613","4306","Patch adds registerTempFile to LTC plus prints stack information if rmDir fails.
I think we should also fail the test if that happens?","I think we should also fail the test if that happens?","shaie","NULL","1","alternative","0","1","0","0","0"
"1923","1923","43614","4306","some tests are still problematic, at least on windows... 
Ok, I didn't notice your comment when posted the patch. So let's keep it as-is.
I think it's ready to commit","some tests are still problematic, at least on windows... 
Ok, I didn't notice your comment when posted the patch.","shaie","NULL","0",NULL,"0","0","0","0","0"
"1924","1924","43614","4306","some tests are still problematic, at least on windows... 
Ok, I didn't notice your comment when posted the patch. So let's keep it as-is.
I think it's ready to commit","So let's keep it as-is.","shaie","NULL","0",NULL,"0","0","0","0","0"
"1925","1925","43614","4306","some tests are still problematic, at least on windows... 
Ok, I didn't notice your comment when posted the patch. So let's keep it as-is.
I think it's ready to commit","I think it's ready to commit","shaie","NULL","1","pro","0","0","1","0","0"
"1926","1926","43615","4306","Patch applies the same changes to backwards' LTC.","Patch applies the same changes to backwards' LTC.","shaie","NULL","1","alternative","0","1","0","0","0"
"1927","1927","43617","4306","I think I've found the problem - MockIndexOutputWrapper did not close delegate if dir.maybeThrowEx actually threw an exception. Here's a patch that fixes it:


Index: lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java
===================================================================
--- lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (revision 1127062)
+++ lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (working copy)
@@ -45,20 +45,23 @@

   @Override
   public void close() throws IOException {
-    dir.maybeThrowDeterministicException();
-    delegate.close();
-    if (dir.trackDiskUsage) {
-      // Now compute actual disk usage & track the maxUsedSize
-      // in the MockDirectoryWrapper:
-      long size = dir.getRecomputedActualSizeInBytes();
-      if (size > dir.maxUsedSize) {
-        dir.maxUsedSize = size;
+    try {
+      dir.maybeThrowDeterministicException();
+    } finally {
+      delegate.close();
+      if (dir.trackDiskUsage) {
+        // Now compute actual disk usage & track the maxUsedSize
+        // in the MockDirectoryWrapper:
+        long size = dir.getRecomputedActualSizeInBytes();
+        if (size > dir.maxUsedSize) {
+          dir.maxUsedSize = size;
+        }
       }
+      synchronized(dir) {
+        dir.openFileHandles.remove(this);
+        dir.openFilesForWrite.remove(name);
+      }
     }
-    synchronized(dir) {
-      dir.openFileHandles.remove(this);
-      dir.openFilesForWrite.remove(name);
-    }
   }

   @Override


Maybe we solve it by moving delegate.close() before dir.maybeThrow, instead of the try-finally?","I think I've found the problem - MockIndexOutputWrapper did not close delegate if dir.maybeThrowEx actually threw an exception.","shaie","NULL","1","issue","1","0","0","0","0"
"1928","1928","43617","4306","I think I've found the problem - MockIndexOutputWrapper did not close delegate if dir.maybeThrowEx actually threw an exception. Here's a patch that fixes it:


Index: lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java
===================================================================
--- lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (revision 1127062)
+++ lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (working copy)
@@ -45,20 +45,23 @@

   @Override
   public void close() throws IOException {
-    dir.maybeThrowDeterministicException();
-    delegate.close();
-    if (dir.trackDiskUsage) {
-      // Now compute actual disk usage & track the maxUsedSize
-      // in the MockDirectoryWrapper:
-      long size = dir.getRecomputedActualSizeInBytes();
-      if (size > dir.maxUsedSize) {
-        dir.maxUsedSize = size;
+    try {
+      dir.maybeThrowDeterministicException();
+    } finally {
+      delegate.close();
+      if (dir.trackDiskUsage) {
+        // Now compute actual disk usage & track the maxUsedSize
+        // in the MockDirectoryWrapper:
+        long size = dir.getRecomputedActualSizeInBytes();
+        if (size > dir.maxUsedSize) {
+          dir.maxUsedSize = size;
+        }
       }
+      synchronized(dir) {
+        dir.openFileHandles.remove(this);
+        dir.openFilesForWrite.remove(name);
+      }
     }
-    synchronized(dir) {
-      dir.openFileHandles.remove(this);
-      dir.openFilesForWrite.remove(name);
-    }
   }

   @Override


Maybe we solve it by moving delegate.close() before dir.maybeThrow, instead of the try-finally?","Here's a patch that fixes it:


Index: lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java
===================================================================
--- lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (revision 1127062)
+++ lucene/src/test-framework/org/apache/lucene/store/MockIndexOutputWrapper.java       (working copy)
@@ -45,20 +45,23 @@

   @Override
   public void close() throws IOException {
-    dir.maybeThrowDeterministicException();
-    delegate.close();
-    if (dir.trackDiskUsage) {
-      // Now compute actual disk usage & track the maxUsedSize
-      // in the MockDirectoryWrapper:
-      long size = dir.getRecomputedActualSizeInBytes();
-      if (size > dir.maxUsedSize) {
-        dir.maxUsedSize = size;
+    try {
+      dir.maybeThrowDeterministicException();
+    } finally {
+      delegate.close();
+      if (dir.trackDiskUsage) {
+        // Now compute actual disk usage & track the maxUsedSize
+        // in the MockDirectoryWrapper:
+        long size = dir.getRecomputedActualSizeInBytes();
+        if (size > dir.maxUsedSize) {
+          dir.maxUsedSize = size;
+        }
       }
+      synchronized(dir) {
+        dir.openFileHandles.remove(this);
+        dir.openFilesForWrite.remove(name);
+      }
     }
-    synchronized(dir) {
-      dir.openFileHandles.remove(this);
-      dir.openFilesForWrite.remove(name);
-    }
   }

   @Override


Maybe we solve it by moving delegate.close() before dir.maybeThrow, instead of the try-finally?","shaie","NULL","1","alternative","0","1","0","0","0"
"1929","1929","43619","4306","Committed revision 1127470 (trunk).
Committed revision 1127471 (3x).
LTC now verbose whatever we need to pursue rmDir failures.","Committed revision 1127470 (trunk).","shaie","NULL","1","decision","0","0","0","0","1"
"1930","1930","43619","4306","Committed revision 1127470 (trunk).
Committed revision 1127471 (3x).
LTC now verbose whatever we need to pursue rmDir failures.","Committed revision 1127471 (3x).","shaie","NULL","1","decision","0","0","0","0","1"
"1931","1931","43619","4306","Committed revision 1127470 (trunk).
Committed revision 1127471 (3x).
LTC now verbose whatever we need to pursue rmDir failures.","LTC now verbose whatever we need to pursue rmDir failures.","shaie","NULL","1","decision","0","0","0","0","1"
"1932","1932","43620","4306","Bulk closing for 3.2","Bulk closing for 3.2","rcmuir","NULL","1","decision","0","0","0","0","1"
"1933","1933","43690","4318","This is on Ubuntu btw.
Run log:

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240
The following exceptions were thrown by threads:
*** Thread: Lucene Merge Thread #0 ***
org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)
	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
	at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69)
	at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90)
	at org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:56)
	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:337)
	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:402)
	at org.apache.lucene.index.codecs.mockrandom.MockRandomCodec.fieldsProducer(MockRandomCodec.java:236)
	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:113)
	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:210)
	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:131)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:495)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:635)
	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3260)
	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2930)
	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
NOTE: test params are: codec=RandomCodecProvider: {field=MockRandom}, locale=nl_NL, timezone=Turkey
NOTE: all tests run in this JVM:
[TestIndexWriter]
NOTE: Linux 2.6.32-31-generic i386/Sun Microsystems Inc. 1.6.0_20 (32-bit)/cpus=1,threads=2,free=26480072,total=33468416

","This is on Ubuntu btw.","doronc","NULL","0",NULL,"0","0","0","0","0"
"1934","1934","43690","4318","This is on Ubuntu btw.
Run log:

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240
The following exceptions were thrown by threads:
*** Thread: Lucene Merge Thread #0 ***
org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)
	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
	at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69)
	at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90)
	at org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:56)
	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:337)
	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:402)
	at org.apache.lucene.index.codecs.mockrandom.MockRandomCodec.fieldsProducer(MockRandomCodec.java:236)
	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:113)
	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:210)
	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:131)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:495)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:635)
	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3260)
	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2930)
	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
NOTE: test params are: codec=RandomCodecProvider: {field=MockRandom}, locale=nl_NL, timezone=Turkey
NOTE: all tests run in this JVM:
[TestIndexWriter]
NOTE: Linux 2.6.32-31-generic i386/Sun Microsystems Inc. 1.6.0_20 (32-bit)/cpus=1,threads=2,free=26480072,total=33468416

","Run log:

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240
The following exceptions were thrown by threads:
*** Thread: Lucene Merge Thread #0 ***
org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)
	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
	at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69)
	at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90)
	at org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:56)
	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:337)
	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:402)
	at org.apache.lucene.index.codecs.mockrandom.MockRandomCodec.fieldsProducer(MockRandomCodec.java:236)
	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:113)
	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:210)
	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:131)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:495)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:635)
	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3260)
	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2930)
	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
NOTE: test params are: codec=RandomCodecProvider: {field=MockRandom}, locale=nl_NL, timezone=Turkey
NOTE: all tests run in this JVM:
[TestIndexWriter]
NOTE: Linux 2.6.32-31-generic i386/Sun Microsystems Inc. 1.6.0_20 (32-bit)/cpus=1,threads=2,free=26480072,total=33468416","doronc","NULL","0",NULL,"0","0","0","0","0"
"1935","1935","43691","4318","Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...
The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields?","Does that repro line reproduce the failure for you Doron?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1936","1936","43691","4318","Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...
The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields?","It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index.","mikemccand","NULL","1","issue","1","0","0","0","0"
"1937","1937","43691","4318","Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...
The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields?","I'll drop that to 50...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1938","1938","43691","4318","Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...
The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields?","The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs...","mikemccand","NULL","1","issue","1","0","0","0","0"
"1939","1939","43691","4318","Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...
The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields?","I wonder if we should throttle it?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1940","1940","43691","4318","Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...
The nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields?","Ie if it accumulates too many codecs, to start sharing them b/w fields?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1941","1941","43692","4318","I dropped it from 100 to 50 segs.  Can you test if that works in your env Doron?","I dropped it from 100 to 50 segs.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1942","1942","43692","4318","I dropped it from 100 to 50 segs.  Can you test if that works in your env Doron?","Can you test if that works in your env Doron?","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1943","1943","43693","4318","Yes, thanks, now it passes (trunk) - with this seed as well quite a few times without specifying a seed. 
I'll now verify on 3x.","Yes, thanks, now it passes (trunk) - with this seed as well quite a few times without specifying a seed.","doronc","NULL","1","alternative, pro","0","1","1","0","0"
"1944","1944","43693","4318","Yes, thanks, now it passes (trunk) - with this seed as well quite a few times without specifying a seed. 
I'll now verify on 3x.","I'll now verify on 3x.","doronc","NULL","0",NULL,"0","0","0","0","0"
"1945","1945","43694","4318","I fact in 3x this is not reproducible with same seed (expected as Robert once explained) and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration? Need to verify this...)
Anyhow in 3x the test passes also after svn up with this fix.
So I think this can be resolved...","I fact in 3x this is not reproducible with same seed (expected as Robert once explained) and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration?","doronc","NULL","1","issue","1","0","0","0","0"
"1946","1946","43694","4318","I fact in 3x this is not reproducible with same seed (expected as Robert once explained) and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration? Need to verify this...)
Anyhow in 3x the test passes also after svn up with this fix.
So I think this can be resolved...","Need to verify this...)
Anyhow in 3x the test passes also after svn up with this fix.","doronc","NULL","0",NULL,"0","0","0","0","0"
"1947","1947","43694","4318","I fact in 3x this is not reproducible with same seed (expected as Robert once explained) and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration? Need to verify this...)
Anyhow in 3x the test passes also after svn up with this fix.
So I think this can be resolved...","So I think this can be resolved...","doronc","NULL","1","pro","0","0","1","0","0"
"1948","1948","43695","4318","Fixed by Mike, thanks Mike!","Fixed by Mike, thanks Mike!","doronc","NULL","1","decision","0","0","0","0","1"
"1949","1949","43696","4318","Thanks for raising it Doron!","Thanks for raising it Doron!","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"1950","1950","43697","4318","Bulk closing for 3.2","Bulk closing for 3.2","rcmuir","NULL","1","decision","0","0","0","0","1"
"1951","1951","43728","4325","bulk move 3.2 -> 3.3","bulk move 3.2 -> 3.3","rcmuir","NULL","1","decision","0","0","0","0","1"
"1952","1952","43729","4325","its easy to add the sleep, but we dont even have good multithreaded tests with rollback() [except testing how exceptions are handled and not really asserting anything?]
Can we push this out to 4.0?","its easy to add the sleep, but we dont even have good multithreaded tests with rollback() [except testing how exceptions are handled and not really asserting anything?]","rcmuir","NULL","1","alternative, pro, con","0","1","1","1","0"
"1953","1953","43729","4325","its easy to add the sleep, but we dont even have good multithreaded tests with rollback() [except testing how exceptions are handled and not really asserting anything?]
Can we push this out to 4.0?","Can we push this out to 4.0?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1954","1954","43730","4325","I think we can push to 4.0...","I think we can push to 4.0...","mikemccand","NULL","1","alternative","0","1","0","0","0"
"1955","1955","43731","4325","Bulk move 4.4 issues to 4.5 and 5.0","Bulk move 4.4 issues to 4.5 and 5.0","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1956","1956","43732","4325","Move issue to Lucene 4.9.","Move issue to Lucene 4.9.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1957","1957","44580","4400","Hi Mark, could you add an example algorithm with this behavior?
Also, this is from the package javadocs:


# multi val params are iterated by NewRound's, added to reports, start with column name.
merge.factor=mrg:10:20
max.buffered=buf:100:1000


Is it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g. 'mrg' or 'buf' in the above?","Hi Mark, could you add an example algorithm with this behavior?","doronc","NULL","0",NULL,"0","0","0","0","0"
"1958","1958","44580","4400","Hi Mark, could you add an example algorithm with this behavior?
Also, this is from the package javadocs:


# multi val params are iterated by NewRound's, added to reports, start with column name.
merge.factor=mrg:10:20
max.buffered=buf:100:1000


Is it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g. 'mrg' or 'buf' in the above?","Also, this is from the package javadocs:


# multi val params are iterated by NewRound's, added to reports, start with column name.","doronc","NULL","0",NULL,"0","0","0","0","0"
"1959","1959","44580","4400","Hi Mark, could you add an example algorithm with this behavior?
Also, this is from the package javadocs:


# multi val params are iterated by NewRound's, added to reports, start with column name.
merge.factor=mrg:10:20
max.buffered=buf:100:1000


Is it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g. 'mrg' or 'buf' in the above?","merge.factor=mrg:10:20
max.buffered=buf:100:1000


Is it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g.","doronc","NULL","1","alternative","0","1","0","0","0"
"1960","1960","44580","4400","Hi Mark, could you add an example algorithm with this behavior?
Also, this is from the package javadocs:


# multi val params are iterated by NewRound's, added to reports, start with column name.
merge.factor=mrg:10:20
max.buffered=buf:100:1000


Is it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g. 'mrg' or 'buf' in the above?","'mrg' or 'buf' in the above?","doronc","NULL","1","alternative","0","1","0","0","0"
"1961","1961","44581","4400","Hey Doron - I have a patch for this, I've just been too lazy to extract it. I'm not sure if there is anything built-in that is long enough to matter - it comes into play if, for example, if you want to alternate fully qualified class names per round.
My original workaround was to simply pad the column name - but it was ugly and had it's limitations, so I instead made some modifications to the formatting classes.","Hey Doron - I have a patch for this, I've just been too lazy to extract it.","markrmiller@gmail.com","NULL","1","alternative","0","1","0","0","0"
"1962","1962","44581","4400","Hey Doron - I have a patch for this, I've just been too lazy to extract it. I'm not sure if there is anything built-in that is long enough to matter - it comes into play if, for example, if you want to alternate fully qualified class names per round.
My original workaround was to simply pad the column name - but it was ugly and had it's limitations, so I instead made some modifications to the formatting classes.","I'm not sure if there is anything built-in that is long enough to matter - it comes into play if, for example, if you want to alternate fully qualified class names per round.","markrmiller@gmail.com","NULL","1","alternative, con","0","1","0","1","0"
"1963","1963","44581","4400","Hey Doron - I have a patch for this, I've just been too lazy to extract it. I'm not sure if there is anything built-in that is long enough to matter - it comes into play if, for example, if you want to alternate fully qualified class names per round.
My original workaround was to simply pad the column name - but it was ugly and had it's limitations, so I instead made some modifications to the formatting classes.","My original workaround was to simply pad the column name - but it was ugly and had it's limitations, so I instead made some modifications to the formatting classes.","markrmiller@gmail.com","NULL","1","alternative, con","0","1","0","1","0"
"1964","1964","44582","4400","My original workaround was to simply pad the column name
Yeah that's what I meant, so ok, better formatting will help.","My original workaround was to simply pad the column name
Yeah that's what I meant, so ok, better formatting will help.","doronc","NULL","1","alternative, pro","0","1","1","0","0"
"1965","1965","44583","4400","Mark, are you planning on working on this one? Is it ok to defer until 4.0?","Mark, are you planning on working on this one?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1966","1966","44583","4400","Mark, are you planning on working on this one? Is it ok to defer until 4.0?","Is it ok to defer until 4.0?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"1967","1967","44584","4400","yeah, this is def not that important. Lets move to 4.","yeah, this is def not that important.","markrmiller@gmail.com","NULL","1","con","0","0","0","1","0"
"1968","1968","44584","4400","yeah, this is def not that important. Lets move to 4.","Lets move to 4.","markrmiller@gmail.com","NULL","1","decision","0","0","0","0","1"
"1969","1969","44585","4400","Bulk move 4.4 issues to 4.5 and 5.0","Bulk move 4.4 issues to 4.5 and 5.0","steve_rowe","NULL","1","decision","0","0","0","0","1"
"1970","1970","44586","4400","Move issue to Lucene 4.9.","Move issue to Lucene 4.9.","thetaphi","NULL","1","decision","0","0","0","0","1"
"1971","1971","44706","4415","The patch. It fixes the problem when usePhraseHighlighter=true.
When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.
But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.","The patch.","koji","NULL","1","alternative","0","1","0","0","0"
"1972","1972","44706","4415","The patch. It fixes the problem when usePhraseHighlighter=true.
When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.
But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.","It fixes the problem when usePhraseHighlighter=true.","koji","NULL","1","alternative, pro","0","1","1","0","0"
"1973","1973","44706","4415","The patch. It fixes the problem when usePhraseHighlighter=true.
When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.
But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.","When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.","koji","NULL","1","alternative, con","0","1","0","1","0"
"1974","1974","44706","4415","The patch. It fixes the problem when usePhraseHighlighter=true.
When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.
But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.","But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.","koji","NULL","1","pro","0","0","1","0","0"
"1975","1975","44707","4415","bulk move 3.2 -> 3.3","bulk move 3.2 -> 3.3","rcmuir","NULL","1","decision","0","0","0","0","1"
"1976","1976","44708","4415","I'll commit soon.","I'll commit soon.","koji","NULL","1","decision","0","0","0","0","1"
"1977","1977","44709","4415","trunk: Committed revision 1170908.
3x: Committed revision 1170913.","trunk: Committed revision 1170908.","koji","NULL","1","decision","0","0","0","0","1"
"1978","1978","44709","4415","trunk: Committed revision 1170908.
3x: Committed revision 1170913.","3x: Committed revision 1170913.","koji","NULL","1","decision","0","0","0","0","1"
"1979","1979","44710","4415","Bulk close after release of 3.5","Bulk close after release of 3.5","thetaphi","NULL","1","decision","0","0","0","0","1"
"1980","1980","44711","4415","The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.","The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0.","imotov","NULL","1","alternative, con","0","1","0","1","0"
"1981","1981","44711","4415","The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.","Our queries sometime contain tens of thousands of terms.","imotov","NULL","1","con","0","0","0","1","0"
"1982","1982","44711","4415","The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.","As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.","imotov","NULL","1","con","0","0","0","1","0"
"1983","1983","44712","4415","Uh, that is not good news. Please don't hesitate to open a ticket. Patches would be welcome as I don't have time to look into it for a few weeks...","Uh, that is not good news.","koji","NULL","0",NULL,"0","0","0","0","0"
"1984","1984","44712","4415","Uh, that is not good news. Please don't hesitate to open a ticket. Patches would be welcome as I don't have time to look into it for a few weeks...","Please don't hesitate to open a ticket.","koji","NULL","0",NULL,"0","0","0","0","0"
"1985","1985","44712","4415","Uh, that is not good news. Please don't hesitate to open a ticket. Patches would be welcome as I don't have time to look into it for a few weeks...","Patches would be welcome as I don't have time to look into it for a few weeks...","koji","NULL","0",NULL,"0","0","0","0","0"
"1986","1986","44713","4415","Created LUCENE-3719 with a patch.","Created LUCENE-3719 with a patch.","imotov","NULL","1","alternative","0","1","0","0","0"
"1989","1987","45439","4489","Robert: In your patch: Why use exactly that float (looks arbitrary) and not e.g. Float.MIN_VALUE (of course not NEGATIVE_INFINITY!)?
Doesn't matter, just want to understand.","Robert: In your patch: Why use exactly that float (looks arbitrary) and not e.g.","thetaphi","NULL","1","con","0","0","0","1","0"
"1990","1988","45439","4489","Robert: In your patch: Why use exactly that float (looks arbitrary) and not e.g. Float.MIN_VALUE (of course not NEGATIVE_INFINITY!)?
Doesn't matter, just want to understand.","Float.MIN_VALUE (of course not NEGATIVE_INFINITY!)?","thetaphi","NULL","1","alternative","0","1","0","0","0"
"1991","1989","45439","4489","Robert: In your patch: Why use exactly that float (looks arbitrary) and not e.g. Float.MIN_VALUE (of course not NEGATIVE_INFINITY!)?
Doesn't matter, just want to understand.","Doesn't matter, just want to understand.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"1992","1990","45440","4489","Hi Uwe: you are right! obviously this one needs a comment, but this is the idea:

norm[0]=4.656613E-10
norm[1]=5.820766E-10
norm[2]=6.9849193E-10
norm[3]=8.1490725E-10

","Hi Uwe: you are right!","rcmuir","NULL","1","pro","0","0","1","0","0"
"1993","1991","45440","4489","Hi Uwe: you are right! obviously this one needs a comment, but this is the idea:

norm[0]=4.656613E-10
norm[1]=5.820766E-10
norm[2]=6.9849193E-10
norm[3]=8.1490725E-10

","obviously this one needs a comment, but this is the idea:

norm[0]=4.656613E-10
norm[1]=5.820766E-10
norm[2]=6.9849193E-10
norm[3]=8.1490725E-10","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1994","1992","45441","4489","Here's a new patch...
apparently there was code in SmallFloat to specifically do this. This simply removes the bug.
I would like to commit soon.","Here's a new patch...
apparently there was code in SmallFloat to specifically do this.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1995","1993","45441","4489","Here's a new patch...
apparently there was code in SmallFloat to specifically do this. This simply removes the bug.
I would like to commit soon.","This simply removes the bug.","rcmuir","NULL","1","pro","0","0","1","0","0"
"1996","1994","45441","4489","Here's a new patch...
apparently there was code in SmallFloat to specifically do this. This simply removes the bug.
I would like to commit soon.","I would like to commit soon.","rcmuir","NULL","1","decision","0","0","0","0","1"
"1997","1995","45442","4489","

  public boolean isMatch() {
    return (0.0f < getValue());
  }


Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?","

  public boolean isMatch() {
    return (0.0f < getValue());
  }


Isn't that a bug?","yseeley@gmail.com","NULL","1","issue","1","0","0","0","0"
"1998","1996","45442","4489","

  public boolean isMatch() {
    return (0.0f < getValue());
  }


Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?","We fixed our search code to not discard negative scores, so explain should also handle that?","yseeley@gmail.com","NULL","1","alternative","0","1","0","0","0"
"1999","1997","45443","4489","This line of code was also one of the first things I was thinking about.
I also think this is a bug in explain!","This line of code was also one of the first things I was thinking about.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2000","1998","45443","4489","This line of code was also one of the first things I was thinking about.
I also think this is a bug in explain!","I also think this is a bug in explain!","thetaphi","NULL","1","issue","1","0","0","0","0"
"2001","1999","45444","4489","
Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?
I agree this is a bad assumption really (although subclasses can override). I am just concerned what explains will 'break' if we fix this.
But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!","Isn't that a bug?","rcmuir","NULL","1","issue","1","0","0","0","0"
"2002","2000","45444","4489","
Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?
I agree this is a bad assumption really (although subclasses can override). I am just concerned what explains will 'break' if we fix this.
But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!","We fixed our search code to not discard negative scores, so explain should also handle that?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2003","2001","45444","4489","
Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?
I agree this is a bad assumption really (although subclasses can override). I am just concerned what explains will 'break' if we fix this.
But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!","I agree this is a bad assumption really (although subclasses can override).","rcmuir","NULL","1","pro","0","0","1","0","0"
"2004","2002","45444","4489","
Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?
I agree this is a bad assumption really (although subclasses can override). I am just concerned what explains will 'break' if we fix this.
But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!","I am just concerned what explains will 'break' if we fix this.","rcmuir","NULL","1","con","0","0","0","1","0"
"2005","2003","45444","4489","
Isn't that a bug?
We fixed our search code to not discard negative scores, so explain should also handle that?
I agree this is a bad assumption really (although subclasses can override). I am just concerned what explains will 'break' if we fix this.
But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!","But still i think the float quantization issue (the root cause of this problem really) should be fixed... ideally we fix both!","rcmuir","NULL","1","issue","1","0","0","0","0"
"2006","2004","45445","4489","I'm not sure why this is a quantization issue... SmallFloat handles underflow by mapping to the smallest number greater than 0, so the only way to get a 0 field norm is an explicit boost of 0.
Anyway, if we want to discard explicit representation for 0, some of the code that handled these edge cases should also be modified:


    if (smallfloat < fzero) {
      return (bits<=0) ?
        (byte)0   // negative numbers and zero both map to 0 byte
       :(byte)1;  // underflow is mapped to smallest non-zero number.

","I'm not sure why this is a quantization issue... SmallFloat handles underflow by mapping to the smallest number greater than 0, so the only way to get a 0 field norm is an explicit boost of 0.","yseeley@gmail.com","NULL","1","issue","1","0","0","0","0"
"2007","2005","45445","4489","I'm not sure why this is a quantization issue... SmallFloat handles underflow by mapping to the smallest number greater than 0, so the only way to get a 0 field norm is an explicit boost of 0.
Anyway, if we want to discard explicit representation for 0, some of the code that handled these edge cases should also be modified:


    if (smallfloat < fzero) {
      return (bits<=0) ?
        (byte)0   // negative numbers and zero both map to 0 byte
       :(byte)1;  // underflow is mapped to smallest non-zero number.

","Anyway, if we want to discard explicit representation for 0, some of the code that handled these edge cases should also be modified:


    if (smallfloat < fzero) {
      return (bits<=0) ?","yseeley@gmail.com","NULL","1","alternative","0","1","0","0","0"
"2008","2006","45445","4489","I'm not sure why this is a quantization issue... SmallFloat handles underflow by mapping to the smallest number greater than 0, so the only way to get a 0 field norm is an explicit boost of 0.
Anyway, if we want to discard explicit representation for 0, some of the code that handled these edge cases should also be modified:


    if (smallfloat < fzero) {
      return (bits<=0) ?
        (byte)0   // negative numbers and zero both map to 0 byte
       :(byte)1;  // underflow is mapped to smallest non-zero number.

","(byte)0   // negative numbers and zero both map to 0 byte
       :(byte)1;  // underflow is mapped to smallest non-zero number.","yseeley@gmail.com","NULL","1","alternative","0","1","0","0","0"
"2009","2007","45446","4489","OK, although underflow is generally handled, Robert found at least one case where it doesn't work.
System.out.println(SmallFloat.floatToByte315(5.8123817E-10f));
That prints 0 for some reason.  I'll see if I can figure out why.","OK, although underflow is generally handled, Robert found at least one case where it doesn't work.","yseeley@gmail.com","NULL","1","issue","1","0","0","0","0"
"2010","2008","45446","4489","OK, although underflow is generally handled, Robert found at least one case where it doesn't work.
System.out.println(SmallFloat.floatToByte315(5.8123817E-10f));
That prints 0 for some reason.  I'll see if I can figure out why.","System.out.println(SmallFloat.floatToByte315(5.8123817E-10f));
That prints 0 for some reason.","yseeley@gmail.com","NULL","1","issue","1","0","0","0","0"
"2011","2009","45446","4489","OK, although underflow is generally handled, Robert found at least one case where it doesn't work.
System.out.println(SmallFloat.floatToByte315(5.8123817E-10f));
That prints 0 for some reason.  I'll see if I can figure out why.","I'll see if I can figure out why.","yseeley@gmail.com","NULL","1","issue","1","0","0","0","0"
"2012","2010","45447","4489","Thanks, so the bug is really my imagination (as SmallFloat is designed to generally handle this, its just some corner case i produced in a test).
So, if we can fix smallfloat my gross hack is not necessary, as we would then only produce byte 0 norms in the case of a zero boost... we can discuss if anything needs to be done there (in my opinion, its not serious, i am only concerned about non-zero floats quantizing to a zero byte).
and then separately we gotta figure out about explains: in my opinion whether or not a document was matched by a query is unrelated to the score...","Thanks, so the bug is really my imagination (as SmallFloat is designed to generally handle this, its just some corner case i produced in a test).","rcmuir","NULL","1","issue","1","0","0","0","0"
"2013","2011","45447","4489","Thanks, so the bug is really my imagination (as SmallFloat is designed to generally handle this, its just some corner case i produced in a test).
So, if we can fix smallfloat my gross hack is not necessary, as we would then only produce byte 0 norms in the case of a zero boost... we can discuss if anything needs to be done there (in my opinion, its not serious, i am only concerned about non-zero floats quantizing to a zero byte).
and then separately we gotta figure out about explains: in my opinion whether or not a document was matched by a query is unrelated to the score...","So, if we can fix smallfloat my gross hack is not necessary, as we would then only produce byte 0 norms in the case of a zero boost... we can discuss if anything needs to be done there (in my opinion, its not serious, i am only concerned about non-zero floats quantizing to a zero byte).","rcmuir","NULL","1","issue, con","1","0","0","1","0"
"2014","2012","45447","4489","Thanks, so the bug is really my imagination (as SmallFloat is designed to generally handle this, its just some corner case i produced in a test).
So, if we can fix smallfloat my gross hack is not necessary, as we would then only produce byte 0 norms in the case of a zero boost... we can discuss if anything needs to be done there (in my opinion, its not serious, i am only concerned about non-zero floats quantizing to a zero byte).
and then separately we gotta figure out about explains: in my opinion whether or not a document was matched by a query is unrelated to the score...","and then separately we gotta figure out about explains: in my opinion whether or not a document was matched by a query is unrelated to the score...","rcmuir","NULL","1","issue","1","0","0","0","0"
"2015","2013","45450","4489","
I don't understand 95\% of what yonik & robert have been saying in this issue  but fortunately i don't think that matters  it sounds like they both agree htat what they are talking about is unrelated to this bug.


Sorry for the confusion, my issues were actually a separate (corner-case) bug, which yonik fixed already in LUCENE-2937. That bug would cause you to have a field boost of 0 in some very very rare/likely-to-have-never-happened-with-our-default-sim cases when you shouldn't.
So for this issue we can address the explains, for when you explicitly set boost of zero.","I don't understand 95\% of what yonik & robert have been saying in this issue  but fortunately i don't think that matters  it sounds like they both agree htat what they are talking about is unrelated to this bug.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2016","2014","45450","4489","
I don't understand 95\% of what yonik & robert have been saying in this issue  but fortunately i don't think that matters  it sounds like they both agree htat what they are talking about is unrelated to this bug.


Sorry for the confusion, my issues were actually a separate (corner-case) bug, which yonik fixed already in LUCENE-2937. That bug would cause you to have a field boost of 0 in some very very rare/likely-to-have-never-happened-with-our-default-sim cases when you shouldn't.
So for this issue we can address the explains, for when you explicitly set boost of zero.","Sorry for the confusion, my issues were actually a separate (corner-case) bug, which yonik fixed already in LUCENE-2937.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2017","2015","45450","4489","
I don't understand 95\% of what yonik & robert have been saying in this issue  but fortunately i don't think that matters  it sounds like they both agree htat what they are talking about is unrelated to this bug.


Sorry for the confusion, my issues were actually a separate (corner-case) bug, which yonik fixed already in LUCENE-2937. That bug would cause you to have a field boost of 0 in some very very rare/likely-to-have-never-happened-with-our-default-sim cases when you shouldn't.
So for this issue we can address the explains, for when you explicitly set boost of zero.","That bug would cause you to have a field boost of 0 in some very very rare/likely-to-have-never-happened-with-our-default-sim cases when you shouldn't.","rcmuir","NULL","1","issue","1","0","0","0","0"
"2018","2016","45450","4489","
I don't understand 95\% of what yonik & robert have been saying in this issue  but fortunately i don't think that matters  it sounds like they both agree htat what they are talking about is unrelated to this bug.


Sorry for the confusion, my issues were actually a separate (corner-case) bug, which yonik fixed already in LUCENE-2937. That bug would cause you to have a field boost of 0 in some very very rare/likely-to-have-never-happened-with-our-default-sim cases when you shouldn't.
So for this issue we can address the explains, for when you explicitly set boost of zero.","So for this issue we can address the explains, for when you explicitly set boost of zero.","rcmuir","NULL","1","issue, alternative","1","1","0","0","0"
"2019","2017","45452","4489","would appreciate a review so we can get this into 3.1
+1 to the patch.","would appreciate a review so we can get this into 3.1
+1 to the patch.","rcmuir","NULL","1","pro","0","0","1","0","0"
"2020","2018","45453","4489","Patch looks great.

So then i expanded the patch to also include BooleanQueries containing phrase queries on multiple fields, and then i was finally able to make TestSimpleExplanations in similar ways to what Koji is seeing.
Hoss's assumption is correct because the problem was found at customer site when they used a BooleanQuery containing PhraseQueries (term query on 1-gram fields).","Patch looks great.","koji","NULL","1","pro","0","0","1","0","0"
"2021","2019","45453","4489","Patch looks great.

So then i expanded the patch to also include BooleanQueries containing phrase queries on multiple fields, and then i was finally able to make TestSimpleExplanations in similar ways to what Koji is seeing.
Hoss's assumption is correct because the problem was found at customer site when they used a BooleanQuery containing PhraseQueries (term query on 1-gram fields).","So then i expanded the patch to also include BooleanQueries containing phrase queries on multiple fields, and then i was finally able to make TestSimpleExplanations in similar ways to what Koji is seeing.","koji","NULL","1","alternative, pro","0","1","1","0","0"
"2022","2020","45453","4489","Patch looks great.

So then i expanded the patch to also include BooleanQueries containing phrase queries on multiple fields, and then i was finally able to make TestSimpleExplanations in similar ways to what Koji is seeing.
Hoss's assumption is correct because the problem was found at customer site when they used a BooleanQuery containing PhraseQueries (term query on 1-gram fields).","Hoss's assumption is correct because the problem was found at customer site when they used a BooleanQuery containing PhraseQueries (term query on 1-gram fields).","koji","NULL","1","issue","1","0","0","0","0"
"2023","2021","45454","4489","fixed in trunk and 3x...
Committed revision 1074357.
Committed revision 1074363.","fixed in trunk and 3x...","hossman","NULL","1","decision","0","0","0","0","1"
"2024","2022","45454","4489","fixed in trunk and 3x...
Committed revision 1074357.
Committed revision 1074363.","Committed revision 1074357.","hossman","NULL","1","decision","0","0","0","0","1"
"2025","2023","45454","4489","fixed in trunk and 3x...
Committed revision 1074357.
Committed revision 1074363.","Committed revision 1074363.","hossman","NULL","1","decision","0","0","0","0","1"
"2026","2024","45456","4489","Bulk close for 3.1","Bulk close for 3.1","gsingers","NULL","1","decision","0","0","0","0","1"
"1987","2025","45437","4489","here's a patch along the lines i suggested.","here's a patch along the lines i suggested.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"1988","2026","45438","4489","sorry, here is correct patch (I had issue dyslexia)","sorry, here is correct patch (I had issue dyslexia)","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2027","2027","45499","4499","I think Steven addressed this issue already... I did some minor cleanups (typos, don't link to unversioned (trunk) resources since this is a versioned document, etc).","I think Steven addressed this issue already...","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2028","2028","45499","4499","I think Steven addressed this issue already... I did some minor cleanups (typos, don't link to unversioned (trunk) resources since this is a versioned document, etc).","I did some minor cleanups (typos, don't link to unversioned (trunk) resources since this is a versioned document, etc).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2029","2029","45500","4499","I think this one is still left to do?:

Demo sources are no longer shipped in a binary release

","I think this one is still left to do?","steve_rowe","NULL","0",NULL,"0","0","0","0","0"
"2030","2030","45500","4499","I think this one is still left to do?:

Demo sources are no longer shipped in a binary release

",":

Demo sources are no longer shipped in a binary release","steve_rowe","NULL","1","issue","1","0","0","0","0"
"2031","2031","45501","4499","Demo sources are no longer shipped in a binary release
Hmm i missed this: Can we simplify this and just tell users to download the source release?","Demo sources are no longer shipped in a binary release
Hmm i missed this: Can we simplify this and just tell users to download the source release?","rcmuir","NULL","1","issue","1","0","0","0","0"
"2032","2032","45502","4499","reopening to address the issue of demo source code being in the binary checkout... we should either include it, or change the demo instructions to tell people to get the demo sources from the source release","reopening to address the issue of demo source code being in the binary checkout... we should either include it, or change the demo instructions to tell people to get the demo sources from the source release","rcmuir","NULL","1","issue, alternative","1","1","0","0","0"
"2033","2033","45503","4499","I added to the demo-part-2 instructions that you should get a source checkout.","I added to the demo-part-2 instructions that you should get a source checkout.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2034","2034","45504","4499","Bulk close for 3.1","Bulk close for 3.1","gsingers","NULL","1","decision","0","0","0","0","1"
"2035","2035","45634","4517","here's a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.
i also added the 'dont skip when close' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).
I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising. I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its 'pendingPositions' consuming to balance it out.
So, I think we should apply this little patch as-is as a small step.","here's a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2036","2036","45634","4517","here's a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.
i also added the 'dont skip when close' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).
I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising. I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its 'pendingPositions' consuming to balance it out.
So, I think we should apply this little patch as-is as a small step.","i also added the 'dont skip when close' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2037","2037","45634","4517","here's a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.
i also added the 'dont skip when close' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).
I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising. I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its 'pendingPositions' consuming to balance it out.
So, I think we should apply this little patch as-is as a small step.","I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2038","2038","45634","4517","here's a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.
i also added the 'dont skip when close' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).
I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising. I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its 'pendingPositions' consuming to balance it out.
So, I think we should apply this little patch as-is as a small step.","I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its 'pendingPositions' consuming to balance it out.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2039","2039","45634","4517","here's a patch with what Mike suggested, moving the skipping stuff private to the codecs, and separating the interval from the minimum df necessary to index skip data.
i also added the 'dont skip when close' opto to Sep and Preflex codecs (since i neglected to do this and only did Standard).
I kept all parameters the same (I think we should benchmark etc before changing) but I did some experiments with a fairly large skipMinimum and it looked promising. I think we later (after we have good benchmarks) set this reasonable, and maybe bump skipInterval for Sep codec too, especially if we improve its 'pendingPositions' consuming to balance it out.
So, I think we should apply this little patch as-is as a small step.","So, I think we should apply this little patch as-is as a small step.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2040","2040","45635","4517","here's a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.
as discussed above, because its size on disk is fixed, we encode blockID and blockID deltas instead of file pointers. 
with the saved bits, we steal one for the case where the delta is within-block, in this case this delta is really the upto delta.
this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)","here's a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2041","2041","45635","4517","here's a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.
as discussed above, because its size on disk is fixed, we encode blockID and blockID deltas instead of file pointers. 
with the saved bits, we steal one for the case where the delta is within-block, in this case this delta is really the upto delta.
this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)","as discussed above, because its size on disk is fixed, we encode blockID and blockID deltas instead of file pointers.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2042","2042","45635","4517","here's a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.
as discussed above, because its size on disk is fixed, we encode blockID and blockID deltas instead of file pointers. 
with the saved bits, we steal one for the case where the delta is within-block, in this case this delta is really the upto delta.
this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)","with the saved bits, we steal one for the case where the delta is within-block, in this case this delta is really the upto delta.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2043","2043","45635","4517","here's a patch solving a lot of the issue for the skiplists and doc/freq/prox etc pointers for Simple64.
as discussed above, because its size on disk is fixed, we encode blockID and blockID deltas instead of file pointers. 
with the saved bits, we steal one for the case where the delta is within-block, in this case this delta is really the upto delta.
this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)","this puts my simple64 indexes smaller than standardcodec (and speeds up the queries too)","rcmuir","NULL","1","pro","0","0","1","0","0"
"2044","2044","45636","4517","I edited my comment above, because the new version of jira works differently somehow with newlines/pasting and it screwed up the quoting.
To add insult to injury, when you edit there is no longer a way to comment on what you changed... ","I edited my comment above, because the new version of jira works differently somehow with newlines/pasting and it screwed up the quoting.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2045","2045","45636","4517","I edited my comment above, because the new version of jira works differently somehow with newlines/pasting and it screwed up the quoting.
To add insult to injury, when you edit there is no longer a way to comment on what you changed... ","To add insult to injury, when you edit there is no longer a way to comment on what you changed...","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2046","2046","45637","4517","Both patches look great!  Bit by bit...","Both patches look great!","mikemccand","NULL","1","pro","0","0","1","0","0"
"2047","2047","45637","4517","Both patches look great!  Bit by bit...","Bit by bit...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"2048","2048","45638","4517","here's a patch for the regular FixedIntBlock and VariableIntBlock cases: they write upto first, if a bit is set then its within block and upto is the upto delta. otherwise they then read the vlong for the fp delta.
this saves about 5\% total bulkvint index size.","here's a patch for the regular FixedIntBlock and VariableIntBlock cases: they write upto first, if a bit is set then its within block and upto is the upto delta.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2049","2049","45638","4517","here's a patch for the regular FixedIntBlock and VariableIntBlock cases: they write upto first, if a bit is set then its within block and upto is the upto delta. otherwise they then read the vlong for the fp delta.
this saves about 5\% total bulkvint index size.","otherwise they then read the vlong for the fp delta.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2050","2050","45638","4517","here's a patch for the regular FixedIntBlock and VariableIntBlock cases: they write upto first, if a bit is set then its within block and upto is the upto delta. otherwise they then read the vlong for the fp delta.
this saves about 5\% total bulkvint index size.","this saves about 5\% total bulkvint index size.","rcmuir","NULL","1","pro","0","0","1","0","0"
"2051","2051","45641","4517","OK I committed this for now to the branch (r1070580), we can always revert it.
I cutover FOR, PFOR, and PFOR2 to use this, and all tests pass with all these codecs.","OK I committed this for now to the branch (r1070580), we can always revert it.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2052","2052","45641","4517","OK I committed this for now to the branch (r1070580), we can always revert it.
I cutover FOR, PFOR, and PFOR2 to use this, and all tests pass with all these codecs.","I cutover FOR, PFOR, and PFOR2 to use this, and all tests pass with all these codecs.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2053","2053","45865","4535","tarball containing a maven project with source code and unit tests for:

AFOR1
AFOR2
FOR
PFOR Non Compulsive
Simple64
a basic tool for debugging IntBlock codecs.

It includes also the lucene-1458 snapshot dependencies that are necessary to compile the code and run the tests.","tarball containing a maven project with source code and unit tests for:

AFOR1
AFOR2
FOR
PFOR Non Compulsive
Simple64
a basic tool for debugging IntBlock codecs.","renaud.delbru","NULL","1","alternative","0","1","0","0","0"
"2054","2054","45865","4535","tarball containing a maven project with source code and unit tests for:

AFOR1
AFOR2
FOR
PFOR Non Compulsive
Simple64
a basic tool for debugging IntBlock codecs.

It includes also the lucene-1458 snapshot dependencies that are necessary to compile the code and run the tests.","It includes also the lucene-1458 snapshot dependencies that are necessary to compile the code and run the tests.","renaud.delbru","NULL","1","alternative, pro","0","1","1","0","0"
"2055","2055","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2056","2056","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","Thanks for uploading this code Renaud, its great and the code is easy to work with.","rcmuir","NULL","1","pro","0","0","1","0","0"
"2057","2057","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","I hope to get some more of the codecs you wrote into the branch for testing.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2058","2058","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g.","rcmuir","NULL","1","alternative, pro, con","0","1","1","1","0"
"2059","2059","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","in the case of a block of 128 1's).","rcmuir","NULL","1","con","0","0","0","1","0"
"2060","2060","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","With variableintblock, we could do this differently, e.g.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2061","2061","45866","4535","I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.
Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.
I changed a few things that helped in benchmarking:

the decoder uses relative gets instead of absolute
we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)

But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).
With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2062","2062","45867","4535","New patch, including Robert's patch, and also adding Simple64 as a VarInt codec.  We badly need more testing of the VarInt cases, so it's great Simple64 came along (thanks Renaud!).
All tests pass w/ Simple64VarInt codec.","New patch, including Robert's patch, and also adding Simple64 as a VarInt codec.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2063","2063","45867","4535","New patch, including Robert's patch, and also adding Simple64 as a VarInt codec.  We badly need more testing of the VarInt cases, so it's great Simple64 came along (thanks Renaud!).
All tests pass w/ Simple64VarInt codec.","We badly need more testing of the VarInt cases, so it's great Simple64 came along (thanks Renaud!).","mikemccand","NULL","1","issue","1","0","0","0","0"
"2064","2064","45867","4535","New patch, including Robert's patch, and also adding Simple64 as a VarInt codec.  We badly need more testing of the VarInt cases, so it's great Simple64 came along (thanks Renaud!).
All tests pass w/ Simple64VarInt codec.","All tests pass w/ Simple64VarInt codec.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"2065","2065","45868","4535","OK I committed the two new Simple64 codecs (to bulk branch).","OK I committed the two new Simple64 codecs (to bulk branch).","mikemccand","NULL","1","decision","0","0","0","0","1"
"2066","2066","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2067","2067","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","our results were performed on the wikipedia dataset and blogs dataset.","renaud.delbru","NULL","1","alternative","0","1","0","0","0"
"2068","2068","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2069","2069","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?","renaud.delbru","NULL","1","alternative","0","1","0","0","0"
"2070","2070","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","Yes, this makes sense.","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2071","2071","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2072","2072","45872","4535","
In the case of 240 1's, i was surprised to see this selector was used over 2\% of the time
for the gov collection's doc file?
our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.

But still, for the all 1's case I'm not actually thinking about unstructured text so much...
in this case I am thinking about metadata fields and more structured data?
Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2073","2073","45873","4535","Just an additional comment on semi-structured data indexing. AFOR-2 and AFOR-3 (AFOR-3 refers to AFOR-2 with special code for allOnes frames), was able to beat Rice on two datasets, and S-64 on one (but it was very close to Rice on the others):
DBpedia dataset: (structured version of wikipedia)


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
0.246
0.043
0.141
0.065
0.180
0.816


AFOR-2
0.229
0.039
0.132
0.059
0.167
0.758


AFOR-3
0.229
0.031
0.131
0.054
0.159
0.736


FOR
0.315
0.061
0.170
0.117
0.216
1.049


PFOR
0.317
0.044
0.155
0.070
0.205
0.946


Rice
0.240
0.029
0.115
0.057
0.152
0.708


S-64
0.249
0.041
0.133
0.062
0.171
0.791


VByte
0.264
0.162
0.222
0.222
0.245
1.335


Geonames Dataset: 


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
0.129
0.023
0.058
0.025
0.025
0.318


AFOR-2
0.123
0.023
0.057
0.024
0.024
0.307


AFOR-3
0.114
0.006
0.056
0.016
0.008
0.256


FOR
0.150
0.021
0.065
0.025
0.023
0.349


PFOR
0.154
0.019
0.057
0.022
0.023
0.332


Rice
0.133
0.019
0.063
0.029
0.021
0.327


S-64
0.147
0.021
0.058
0.023
0.023
0.329


VByte
0.216
0.142
0.143
0.143
0.143
0.929


Sindice Dataset: Very heterogeneous dataset containing hundred of thousands of web dataset


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
2.578
0.395
0.942
0.665
1.014
6.537


AFOR-2
2.361
0.380
0.908
0.619
0.906
6.082


AFOR-3
2.297
0.176
0.876
0.530
0.722
5.475


FOR
3.506
0.506
1.121
0.916
1.440
8.611


PFOR
3.221
0.374
1.153
0.795
1.227
7.924


Rice
2.721
0.314
0.958
0.714
0.941
6.605


S-64
2.581
0.370
0.917
0.621
0.908
6.313


VByte
3.287
2.106
2.411
2.430
2.488
15.132


Here, Ent refers to entity id (similar to doc id), Att and Val are structural node ids.","Just an additional comment on semi-structured data indexing.","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2074","2074","45873","4535","Just an additional comment on semi-structured data indexing. AFOR-2 and AFOR-3 (AFOR-3 refers to AFOR-2 with special code for allOnes frames), was able to beat Rice on two datasets, and S-64 on one (but it was very close to Rice on the others):
DBpedia dataset: (structured version of wikipedia)


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
0.246
0.043
0.141
0.065
0.180
0.816


AFOR-2
0.229
0.039
0.132
0.059
0.167
0.758


AFOR-3
0.229
0.031
0.131
0.054
0.159
0.736


FOR
0.315
0.061
0.170
0.117
0.216
1.049


PFOR
0.317
0.044
0.155
0.070
0.205
0.946


Rice
0.240
0.029
0.115
0.057
0.152
0.708


S-64
0.249
0.041
0.133
0.062
0.171
0.791


VByte
0.264
0.162
0.222
0.222
0.245
1.335


Geonames Dataset: 


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
0.129
0.023
0.058
0.025
0.025
0.318


AFOR-2
0.123
0.023
0.057
0.024
0.024
0.307


AFOR-3
0.114
0.006
0.056
0.016
0.008
0.256


FOR
0.150
0.021
0.065
0.025
0.023
0.349


PFOR
0.154
0.019
0.057
0.022
0.023
0.332


Rice
0.133
0.019
0.063
0.029
0.021
0.327


S-64
0.147
0.021
0.058
0.023
0.023
0.329


VByte
0.216
0.142
0.143
0.143
0.143
0.929


Sindice Dataset: Very heterogeneous dataset containing hundred of thousands of web dataset


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
2.578
0.395
0.942
0.665
1.014
6.537


AFOR-2
2.361
0.380
0.908
0.619
0.906
6.082


AFOR-3
2.297
0.176
0.876
0.530
0.722
5.475


FOR
3.506
0.506
1.121
0.916
1.440
8.611


PFOR
3.221
0.374
1.153
0.795
1.227
7.924


Rice
2.721
0.314
0.958
0.714
0.941
6.605


S-64
2.581
0.370
0.917
0.621
0.908
6.313


VByte
3.287
2.106
2.411
2.430
2.488
15.132


Here, Ent refers to entity id (similar to doc id), Att and Val are structural node ids.","AFOR-2 and AFOR-3 (AFOR-3 refers to AFOR-2 with special code for allOnes frames), was able to beat Rice on two datasets, and S-64 on one (but it was very close to Rice on the others):
DBpedia dataset: (structured version of wikipedia)


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
0.246
0.043
0.141
0.065
0.180
0.816


AFOR-2
0.229
0.039
0.132
0.059
0.167
0.758


AFOR-3
0.229
0.031
0.131
0.054
0.159
0.736


FOR
0.315
0.061
0.170
0.117
0.216
1.049


PFOR
0.317
0.044
0.155
0.070
0.205
0.946


Rice
0.240
0.029
0.115
0.057
0.152
0.708


S-64
0.249
0.041
0.133
0.062
0.171
0.791


VByte
0.264
0.162
0.222
0.222
0.245
1.335


Geonames Dataset: 


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
0.129
0.023
0.058
0.025
0.025
0.318


AFOR-2
0.123
0.023
0.057
0.024
0.024
0.307


AFOR-3
0.114
0.006
0.056
0.016
0.008
0.256


FOR
0.150
0.021
0.065
0.025
0.023
0.349


PFOR
0.154
0.019
0.057
0.022
0.023
0.332


Rice
0.133
0.019
0.063
0.029
0.021
0.327


S-64
0.147
0.021
0.058
0.023
0.023
0.329


VByte
0.216
0.142
0.143
0.143
0.143
0.929


Sindice Dataset: Very heterogeneous dataset containing hundred of thousands of web dataset


Method
Ent
Frq
Att
Val
Pos
Total


AFOR-1
2.578
0.395
0.942
0.665
1.014
6.537


AFOR-2
2.361
0.380
0.908
0.619
0.906
6.082


AFOR-3
2.297
0.176
0.876
0.530
0.722
5.475


FOR
3.506
0.506
1.121
0.916
1.440
8.611


PFOR
3.221
0.374
1.153
0.795
1.227
7.924


Rice
2.721
0.314
0.958
0.714
0.941
6.605


S-64
2.581
0.370
0.917
0.621
0.908
6.313


VByte
3.287
2.106
2.411
2.430
2.488
15.132


Here, Ent refers to entity id (similar to doc id), Att and Val are structural node ids.","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2075","2075","45878","4535","Hi Michael, 
the first results are not that impressive. 

Could you tell me what is BulkVInt ? Is it the simple VInt codec implemented on top of the Bulk branch ?
What the difference between '+united +states' and '+nebraska +states' ? Is nebraska a low frequency term ?

","Hi Michael, 
the first results are not that impressive.","renaud.delbru","NULL","1","con","0","0","0","1","0"
"2076","2076","45878","4535","Hi Michael, 
the first results are not that impressive. 

Could you tell me what is BulkVInt ? Is it the simple VInt codec implemented on top of the Bulk branch ?
What the difference between '+united +states' and '+nebraska +states' ? Is nebraska a low frequency term ?

","Could you tell me what is BulkVInt ?","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2077","2077","45878","4535","Hi Michael, 
the first results are not that impressive. 

Could you tell me what is BulkVInt ? Is it the simple VInt codec implemented on top of the Bulk branch ?
What the difference between '+united +states' and '+nebraska +states' ? Is nebraska a low frequency term ?

","Is it the simple VInt codec implemented on top of the Bulk branch ?","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2078","2078","45878","4535","Hi Michael, 
the first results are not that impressive. 

Could you tell me what is BulkVInt ? Is it the simple VInt codec implemented on top of the Bulk branch ?
What the difference between '+united +states' and '+nebraska +states' ? Is nebraska a low frequency term ?

","What the difference between '+united +states' and '+nebraska +states' ?","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2079","2079","45878","4535","Hi Michael, 
the first results are not that impressive. 

Could you tell me what is BulkVInt ? Is it the simple VInt codec implemented on top of the Bulk branch ?
What the difference between '+united +states' and '+nebraska +states' ? Is nebraska a low frequency term ?

","Is nebraska a low frequency term ?","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2080","2080","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.","renaud.delbru","NULL","1","alternative","0","1","0","0","0"
"2081","2081","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.","renaud.delbru","NULL","0",NULL,"0","0","0","0","0"
"2082","2082","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2083","2083","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","This is odd, because we observed the contrary (on the lucene-1458 branch).","renaud.delbru","NULL","1","con","0","0","0","1","0"
"2084","2084","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","The standard codec was by an order of magnitude faster than any other codec.","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2085","2085","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).","renaud.delbru","NULL","1","issue, con","1","0","0","1","0"
"2086","2086","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).","renaud.delbru","NULL","1","con","0","0","0","1","0"
"2087","2087","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","But this might have been improved since then.","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2088","2088","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","Michael told me he worked on a new version of the IntBlock interface which was more performant.","renaud.delbru","NULL","1","alternative, pro","0","1","1","0","0"
"2089","2089","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","So, if we 'group' the long values so we are e.g.","renaud.delbru","NULL","1","alternative","0","1","0","0","0"
"2090","2090","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.","renaud.delbru","NULL","1","alternative, pro","0","1","1","0","0"
"2091","2091","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","If I understand, this is similar to increasing the boundaries of the variable block size.","renaud.delbru","NULL","1","alternative","0","1","0","0","0"
"2092","2092","45882","4535","
The BulkVInt codec is VInt implemented as a FixedIntBlock codec.
Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.

previously various codecs
looked much faster than Vint but a lot of the reason for this is due to the way Vint
was implemented...
This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:

was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).
had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.


So, if we 'group' the long values so we are e.g. reading say N long values
at once in a single internal 'block', I think we might get more efficiency
via the I/O system, and also less overhead from the bulkpostings apis.
If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.","renaud.delbru","NULL","1","pro","0","0","1","0","0"
"2093","2093","45883","4535","On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words. They do this for two reasons, index size and query time, both based on VByte. With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.
So the message is: please keep this in.","On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words.","paul.elschot@xs4all.nl","NULL","1","con","0","0","0","1","0"
"2094","2094","45883","4535","On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words. They do this for two reasons, index size and query time, both based on VByte. With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.
So the message is: please keep this in.","They do this for two reasons, index size and query time, both based on VByte.","paul.elschot@xs4all.nl","NULL","1","pro","0","0","1","0","0"
"2095","2095","45883","4535","On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words. They do this for two reasons, index size and query time, both based on VByte. With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.
So the message is: please keep this in.","With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.","paul.elschot@xs4all.nl","NULL","1","alternative, pro","0","1","1","0","0"
"2096","2096","45883","4535","On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words. They do this for two reasons, index size and query time, both based on VByte. With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.
So the message is: please keep this in.","So the message is: please keep this in.","paul.elschot@xs4all.nl","NULL","1","alternative","0","1","0","0","0"
"2097","2097","45884","4535","Attached patch, adding block multiplier to Simple64VarInt.
I haven't tested perf impact yet...","Attached patch, adding block multiplier to Simple64VarInt.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2098","2098","45884","4535","Attached patch, adding block multiplier to Simple64VarInt.
I haven't tested perf impact yet...","I haven't tested perf impact yet...","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"2099","2099","45885","4535","New patch, cuts over to bulk-reading the byte[] and then pulling a LongBuffer from that.","New patch, cuts over to bulk-reading the byte[] and then pulling a LongBuffer from that.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2100","2100","45889","4535","Bulk move 4.4 issues to 4.5 and 5.0","Bulk move 4.4 issues to 4.5 and 5.0","steve_rowe","NULL","1","decision","0","0","0","0","1"
"2101","2101","45890","4535","Move issue to Lucene 4.9.","Move issue to Lucene 4.9.","thetaphi","NULL","1","decision","0","0","0","0","1"
"2102","2102","46361","4565","I added a SegmentListener class which is set on IWC.  I still need to write unit tests and add an event for aborted merges.
Perhaps we want to enable a collection of segment listeners instead of only one?","I added a SegmentListener class which is set on IWC.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2103","2103","46361","4565","I added a SegmentListener class which is set on IWC.  I still need to write unit tests and add an event for aborted merges.
Perhaps we want to enable a collection of segment listeners instead of only one?","I still need to write unit tests and add an event for aborted merges.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2104","2104","46361","4565","I added a SegmentListener class which is set on IWC.  I still need to write unit tests and add an event for aborted merges.
Perhaps we want to enable a collection of segment listeners instead of only one?","Perhaps we want to enable a collection of segment listeners instead of only one?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2105","2105","46362","4565","A CompositeSegmentListener niftily removes the need for collection.","A CompositeSegmentListener niftily removes the need for collection.","earwin","NULL","1","alternative, pro","0","1","1","0","0"
"2106","2106","46363","4565","A CompositeSegmentListener niftily removes the need for collection
How would it look?","A CompositeSegmentListener niftily removes the need for collection
How would it look?","jasonrutherglen","NULL","0",NULL,"0","0","0","0","0"
"2107","2107","46364","4565","A SegmentListener that has a number of children SLs and delegates eventHappened() calls to them. ","A SegmentListener that has a number of children SLs and delegates eventHappened() calls to them.","earwin","NULL","1","alternative","0","1","0","0","0"
"2108","2108","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","I think start/endTime can be long and not Long?","shaie","NULL","1","alternative","0","1","0","0","0"
"2109","2109","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent.","shaie","NULL","1","alternative","0","1","0","0","0"
"2110","2110","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","So its signature will be MergeEvent(Type type, long time, OneMerge merge).","shaie","NULL","1","alternative","0","1","0","0","0"
"2111","2111","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","The 'time' parameter can then be interpreted according to Type.","shaie","NULL","1","alternative","0","1","0","0","0"
"2112","2112","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?","shaie","NULL","1","alternative","0","1","0","0","0"
"2113","2113","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?","shaie","NULL","1","alternative","0","1","0","0","0"
"2114","2114","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.","shaie","NULL","1","alternative","0","1","0","0","0"
"2115","2115","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'.","shaie","NULL","1","issue","1","0","0","0","0"
"2116","2116","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes.","shaie","NULL","1","issue","1","0","0","0","0"
"2117","2117","46365","4565","I think start/endTime can be long and not Long?
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
About this:


public long getMergeSegmentSize(boolean includeDocStores) throws IOException {
  // nocommit: cache this?
  return mergeInfo.sizeInBytes(includeDocStores);
}


SegmentInfo caches sizeInBytes, so I think the 'nocommit' can go away.
However, I did notice that SegmentInfo's cache is potentially buggy  it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' and then with 'true' (or vice versa), you won't get the right sizeInBytes. I'll open a separate issue to fix this.","I'll open a separate issue to fix this.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2118","2118","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","I think start/endTime can be long and not Long?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2119","2119","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","Long's used because it allows null.","jasonrutherglen","NULL","1","pro","0","0","1","0","0"
"2120","2120","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2121","2121","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","So its signature will be MergeEvent(Type type, long time, OneMerge merge).","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2122","2122","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","The 'time' parameter can then be interpreted according to Type.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2123","2123","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2124","2124","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","I think it's generally useful to keep track of the time(s) in the OneMerge object.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2125","2125","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","It's implemented this way so that the, likely a user interface does not need to store the various times itself.","jasonrutherglen","NULL","1","pro","0","0","1","0","0"
"2126","2126","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","Actually, logging applications also need to print the durations.","jasonrutherglen","NULL","1","pro","0","0","1","0","0"
"2127","2127","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2128","2128","46366","4565","I think start/endTime can be long and not Long?
Long's used because it allows null.
Maybe instead of adding init/start/endTime to OneMerge, you can pass a 'time' parameter to MergeEvent. So its signature will be MergeEvent(Type type, long time, OneMerge merge). The 'time' parameter can then be interpreted according to Type.
Even better, I think you can remove that parameter entirely, and have MergeEvent call System.currentTimeMillis() and set its internal member according to Type?
I think it's generally useful to keep track of the time(s) in the OneMerge object.  It's implemented this way so that the, likely a user interface does not need to store the various times itself.  Actually, logging applications also need to print the durations.
I did notice that SegmentInfo's cache is potentially buggy - it doesn't take into account 'includeDocStores'. 
Yes, that'd be good to fix.","Yes, that'd be good to fix.","jasonrutherglen","NULL","1","pro","0","0","1","0","0"
"2129","2129","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","Long's used because it allows null.","shaie","NULL","1","pro","0","0","1","0","0"
"2130","2130","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","I see.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2131","2131","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .","shaie","NULL","1","alternative, con","0","1","0","1","0"
"2132","2132","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","I think it's generally useful to keep track of the time(s) in the OneMerge object.","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"2133","2133","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it.","shaie","NULL","1","pro","0","0","1","0","0"
"2134","2134","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes.","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"2135","2135","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","But as I said, I don't mind about it too much.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2136","2136","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","BTW, I think SegmentListener is not the proper name?","shaie","NULL","1","con","0","0","0","1","0"
"2137","2137","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","I.e., it does not listen on Segments, right?","shaie","NULL","1","con","0","0","0","1","0"
"2138","2138","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","Maybe a SegmentMergeListener, or simple a MergeListener?","shaie","NULL","1","alternative","0","1","0","0","0"
"2139","2139","46367","4565","Long's used because it allows null.
I see. We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that .
I think it's generally useful to keep track of the time(s) in the OneMerge object.
I'm not going to argue too much about that point - was just thinking that OneMerge is already filled with members and it'd be nice if we can add more to it. As for the logging scenario, I think that with the CompositeSegmentsListener an application can easily plug in its SL for logging purposes. But as I said, I don't mind about it too much.
BTW, I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener? It anyway accepts only a MergeEvent ....","It anyway accepts only a MergeEvent ....","shaie","NULL","0",NULL,"0","0","0","0","0"
"2140","2140","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.","jasonrutherglen","NULL","1","alternative, con","0","1","0","1","0"
"2141","2141","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2142","2142","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","I think SegmentListener is not the proper name?","jasonrutherglen","NULL","1","con","0","0","0","1","0"
"2143","2143","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","I.e., it does not listen on Segments, right?","jasonrutherglen","NULL","1","con","0","0","0","1","0"
"2144","2144","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","Maybe a SegmentMergeListener, or simple a MergeListener?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2145","2145","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","I had MergeListener, then changed it to be more generic.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2146","2146","46368","4565","We could go with a -1 setting also - I see no reason to allocate an object, and use auto-boxing as we do that
That's a style thing.  I prefer null as it's conclusive and doesn't lead to incorrect calculations, just NPEs.
I think SegmentListener is not the proper name? I.e., it does not listen on Segments, right? Maybe a SegmentMergeListener, or simple a MergeListener?
I had MergeListener, then changed it to be more generic.  I think we should add other segment events such as flush, open, clone, close, etc?","I think we should add other segment events such as flush, open, clone, close, etc?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2147","2147","46369","4565","I see. I'm ok with both then.","I see.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2148","2148","46369","4565","I see. I'm ok with both then.","I'm ok with both then.","shaie","NULL","1","pro","0","0","1","0","0"
"2149","2149","46370","4565","I'll added events for flush, open, clone, close and the CompositeSegmentsListener.  ","I'll added events for flush, open, clone, close and the CompositeSegmentsListener.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2150","2150","46371","4565","Here's a first cut including workarounds to avoid NPEs and file not found exceptions in SegmentInfo (when calling size in bytes).  There's a test case for merge init, start, and complete.  I need to add one for abort.","Here's a first cut including workarounds to avoid NPEs and file not found exceptions in SegmentInfo (when calling size in bytes).","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2151","2151","46371","4565","Here's a first cut including workarounds to avoid NPEs and file not found exceptions in SegmentInfo (when calling size in bytes).  There's a test case for merge init, start, and complete.  I need to add one for abort.","There's a test case for merge init, start, and complete.","jasonrutherglen","NULL","0",NULL,"0","0","0","0","0"
"2152","2152","46371","4565","Here's a first cut including workarounds to avoid NPEs and file not found exceptions in SegmentInfo (when calling size in bytes).  There's a test case for merge init, start, and complete.  I need to add one for abort.","I need to add one for abort.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2153","2153","46372","4565","The aborted merge event is now generated and tested for.","The aborted merge event is now generated and tested for.","jasonrutherglen","NULL","1","decision","0","0","0","0","1"
"2154","2154","46373","4565","I separated out a ReaderListener because it's tied to the ReaderPool which eventually will exist external to IW.  ","I separated out a ReaderListener because it's tied to the ReaderPool which eventually will exist external to IW.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2155","2155","46375","4565","Here's another iteration.  Changed the name to IndexEventListener.  Added experimental to the Javadocs, and I probably need to add more.  There are some nocommits still, eg, for the reason a flush kicked off.  Reader events should be in a different issue as reader pool is moving out of IW soon?  All tests pass. ","Here's another iteration.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2156","2156","46375","4565","Here's another iteration.  Changed the name to IndexEventListener.  Added experimental to the Javadocs, and I probably need to add more.  There are some nocommits still, eg, for the reason a flush kicked off.  Reader events should be in a different issue as reader pool is moving out of IW soon?  All tests pass. ","Changed the name to IndexEventListener.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2157","2157","46375","4565","Here's another iteration.  Changed the name to IndexEventListener.  Added experimental to the Javadocs, and I probably need to add more.  There are some nocommits still, eg, for the reason a flush kicked off.  Reader events should be in a different issue as reader pool is moving out of IW soon?  All tests pass. ","Added experimental to the Javadocs, and I probably need to add more.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2158","2158","46375","4565","Here's another iteration.  Changed the name to IndexEventListener.  Added experimental to the Javadocs, and I probably need to add more.  There are some nocommits still, eg, for the reason a flush kicked off.  Reader events should be in a different issue as reader pool is moving out of IW soon?  All tests pass. ","There are some nocommits still, eg, for the reason a flush kicked off.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2159","2159","46375","4565","Here's another iteration.  Changed the name to IndexEventListener.  Added experimental to the Javadocs, and I probably need to add more.  There are some nocommits still, eg, for the reason a flush kicked off.  Reader events should be in a different issue as reader pool is moving out of IW soon?  All tests pass. ","Reader events should be in a different issue as reader pool is moving out of IW soon?","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2160","2160","46375","4565","Here's another iteration.  Changed the name to IndexEventListener.  Added experimental to the Javadocs, and I probably need to add more.  There are some nocommits still, eg, for the reason a flush kicked off.  Reader events should be in a different issue as reader pool is moving out of IW soon?  All tests pass. ","All tests pass.","jasonrutherglen","NULL","1","pro","0","0","1","0","0"
"2161","2161","46376","4565","Here's an update, there's one nocommit as I'm not sure how we want to capture and exception and rethrow (a Throwable).  Adding the reason a flush occurred requires quite a bit of refactoring that we can probably leave for later if it's needed.  Updated to trunk, and all tests pass.","Here's an update, there's one nocommit as I'm not sure how we want to capture and exception and rethrow (a Throwable).","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2162","2162","46376","4565","Here's an update, there's one nocommit as I'm not sure how we want to capture and exception and rethrow (a Throwable).  Adding the reason a flush occurred requires quite a bit of refactoring that we can probably leave for later if it's needed.  Updated to trunk, and all tests pass.","Adding the reason a flush occurred requires quite a bit of refactoring that we can probably leave for later if it's needed.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2163","2163","46376","4565","Here's an update, there's one nocommit as I'm not sure how we want to capture and exception and rethrow (a Throwable).  Adding the reason a flush occurred requires quite a bit of refactoring that we can probably leave for later if it's needed.  Updated to trunk, and all tests pass.","Updated to trunk, and all tests pass.","jasonrutherglen","NULL","1","pro, decision","0","0","1","0","1"
"2164","2164","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe?","shaie","NULL","1","alternative","0","1","0","0","0"
"2165","2165","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","If so, then it won't break anything.","shaie","NULL","1","pro","0","0","1","0","0"
"2166","2166","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?","shaie","NULL","1","alternative","0","1","0","0","0"
"2167","2167","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","What do you think?","shaie","NULL","0",NULL,"0","0","0","0","0"
"2168","2168","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","Would abort() on Directory fit better?","shaie","NULL","1","alternative","0","1","0","0","0"
"2169","2169","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput?","shaie","NULL","1","alternative","0","1","0","0","0"
"2170","2170","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code?","shaie","NULL","1","alternative","0","1","0","0","0"
"2171","2171","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","If so, would rollback() be a better name?","shaie","NULL","1","alternative","0","1","0","0","0"
"2172","2172","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it.","shaie","NULL","1","alternative","0","1","0","0","0"
"2173","2173","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).","shaie","NULL","1","alternative","0","1","0","0","0"
"2174","2174","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput.","shaie","NULL","1","alternative, pro, con","0","1","1","1","0"
"2175","2175","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","Maybe I don't understand the use case for it well though.","shaie","NULL","1","con","0","0","0","1","0"
"2176","2176","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?).","shaie","NULL","1","pro, con","0","0","1","1","0"
"2177","2177","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.","shaie","NULL","1","con","0","0","0","1","0"
"2178","2178","46701","4600","but constitutes an API backcompat break
Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
Anyway, I think we can make an exception in this case - only those who impl Directory and provide their own IndexOutput extension will be affected, which I think is a relatively low number of applications?
What do you think?
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
I always thought of IndexOutput as a means for writing bytes, no special semantic logic coded and executed by it. The management code IMO should be maintained by higher-level code, such as Directory or even higher (today IndexWriter, but that's what you're trying to remove ).
So on one hand, I'd like to see IndexWriter's code simplified (this class has become a monster), but on the other, it doesn't feel right to me to add this logic in IndexOutput. Maybe I don't understand the use case for it well though. I do think though, that abort() on IndexOutput has a specific, clearer, meaning, where on Directory it can be perceived as kinda vague (what exactly is it aborting, reading / writing?). And maybe aborting a Directory is not good, if say you want to abort/rollback the changes done to a particular file.
All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","All in all, I'm +1 for simplifying IW, but am still +0 on transferring the logic to IndexOutput, unless I misunderstand the use case.","shaie","NULL","1","alternative, pro, con","0","1","1","1","0"
"2179","2179","46702","4600","+1 I think this'd be a good simplification of IW/IR code.  I don't mind that IO would know how to delete the partial file it had created; that seems fair.
So eg CompoundFileWriter would abort its output file on hitting any exception.
I think we can make a default impl that simply closes & suppresses exceptions?  (We can't .deleteFile since an abstract IO doesn't know its Dir).  Our concrete impls can override w/ versions that do delete the file...","+1 I think this'd be a good simplification of IW/IR code.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2180","2180","46702","4600","+1 I think this'd be a good simplification of IW/IR code.  I don't mind that IO would know how to delete the partial file it had created; that seems fair.
So eg CompoundFileWriter would abort its output file on hitting any exception.
I think we can make a default impl that simply closes & suppresses exceptions?  (We can't .deleteFile since an abstract IO doesn't know its Dir).  Our concrete impls can override w/ versions that do delete the file...","I don't mind that IO would know how to delete the partial file it had created; that seems fair.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2181","2181","46702","4600","+1 I think this'd be a good simplification of IW/IR code.  I don't mind that IO would know how to delete the partial file it had created; that seems fair.
So eg CompoundFileWriter would abort its output file on hitting any exception.
I think we can make a default impl that simply closes & suppresses exceptions?  (We can't .deleteFile since an abstract IO doesn't know its Dir).  Our concrete impls can override w/ versions that do delete the file...","So eg CompoundFileWriter would abort its output file on hitting any exception.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2182","2182","46702","4600","+1 I think this'd be a good simplification of IW/IR code.  I don't mind that IO would know how to delete the partial file it had created; that seems fair.
So eg CompoundFileWriter would abort its output file on hitting any exception.
I think we can make a default impl that simply closes & suppresses exceptions?  (We can't .deleteFile since an abstract IO doesn't know its Dir).  Our concrete impls can override w/ versions that do delete the file...","I think we can make a default impl that simply closes & suppresses exceptions?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2183","2183","46702","4600","+1 I think this'd be a good simplification of IW/IR code.  I don't mind that IO would know how to delete the partial file it had created; that seems fair.
So eg CompoundFileWriter would abort its output file on hitting any exception.
I think we can make a default impl that simply closes & suppresses exceptions?  (We can't .deleteFile since an abstract IO doesn't know its Dir).  Our concrete impls can override w/ versions that do delete the file...","(We can't .deleteFile since an abstract IO doesn't know its Dir).","mikemccand","NULL","1","con","0","0","0","1","0"
"2184","2184","46702","4600","+1 I think this'd be a good simplification of IW/IR code.  I don't mind that IO would know how to delete the partial file it had created; that seems fair.
So eg CompoundFileWriter would abort its output file on hitting any exception.
I think we can make a default impl that simply closes & suppresses exceptions?  (We can't .deleteFile since an abstract IO doesn't know its Dir).  Our concrete impls can override w/ versions that do delete the file...","Our concrete impls can override w/ versions that do delete the file...","mikemccand","NULL","1","pro","0","0","1","0","0"
"2185","2185","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe?","earwin","NULL","1","alternative","0","1","0","0","0"
"2186","2186","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","If so, then it won't break anything.","earwin","NULL","1","pro","0","0","1","0","0"
"2187","2187","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","It can't.","earwin","NULL","1","con","0","0","0","1","0"
"2188","2188","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to.","earwin","NULL","1","alternative","0","1","0","0","0"
"2189","2189","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","Abstract IO class has neither.","earwin","NULL","1","con","0","0","0","1","0"
"2190","2190","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better?","earwin","NULL","1","alternative, con","0","1","0","1","0"
"2191","2191","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput?","earwin","NULL","1","alternative","0","1","0","0","0"
"2192","2192","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code?","earwin","NULL","1","alternative","0","1","0","0","0"
"2193","2193","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","If so, would rollback() be a better name?","earwin","NULL","1","alternative","0","1","0","0","0"
"2194","2194","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","Oh, no, no.","earwin","NULL","0",NULL,"0","0","0","0","0"
"2195","2195","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","No way.","earwin","NULL","1","con","0","0","0","1","0"
"2196","2196","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","I don't want to push someone else's responsibility on Directory.","earwin","NULL","1","con","0","0","0","1","0"
"2197","2197","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","This abort() is merely a shortcut.","earwin","NULL","1","alternative, con","0","1","0","1","0"
"2198","2198","46703","4600","Can abort() have a default impl in IndexOutput, such as close() followed by deleteFile() maybe? If so, then it won't break anything.
It can't. To call deleteFile you need both a reference to papa-Directory and a name of the file this IO writes to. Abstract IO class has neither. If we add them, they have to be passed to a new constructor, and that's an API break 
Would abort() on Directory fit better? E.g., it can abort all currently open and modified files, instead of the caller calling abort() on each IndexOutput? Are you thinking of a case where a write failed, and the caller would call abort() immediately, instead of some higher-level code? If so, would rollback() be a better name?
Oh, no, no. No way. I don't want to push someone else's responsibility on Directory. This abort() is merely a shortcut.
Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","Let's go with a usage example:
Here's FieldsWriter.java with LUCENE-2814 applied (skipping irrelevant parts) - https://gist.github.com/746358
Now, the same, with abort() - https://gist.github.com/746367","earwin","NULL","0",NULL,"0","0","0","0","0"
"2199","2199","46704","4600","I think we can make a default impl that simply closes & suppresses exceptions? (We can't .deleteFile since an abstract IO doesn't know its Dir). Our concrete impls can override w/ versions that do delete the file...
I don't think we need a default impl? For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can't forget this, so we're not gonna see broken directories that don't do abort() properly.","I think we can make a default impl that simply closes & suppresses exceptions?","earwin","NULL","1","alternative","0","1","0","0","0"
"2200","2200","46704","4600","I think we can make a default impl that simply closes & suppresses exceptions? (We can't .deleteFile since an abstract IO doesn't know its Dir). Our concrete impls can override w/ versions that do delete the file...
I don't think we need a default impl? For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can't forget this, so we're not gonna see broken directories that don't do abort() properly.","(We can't .deleteFile since an abstract IO doesn't know its Dir).","earwin","NULL","1","con","0","0","0","1","0"
"2201","2201","46704","4600","I think we can make a default impl that simply closes & suppresses exceptions? (We can't .deleteFile since an abstract IO doesn't know its Dir). Our concrete impls can override w/ versions that do delete the file...
I don't think we need a default impl? For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can't forget this, so we're not gonna see broken directories that don't do abort() properly.","Our concrete impls can override w/ versions that do delete the file...","earwin","NULL","1","alternative","0","1","0","0","0"
"2202","2202","46704","4600","I think we can make a default impl that simply closes & suppresses exceptions? (We can't .deleteFile since an abstract IO doesn't know its Dir). Our concrete impls can override w/ versions that do delete the file...
I don't think we need a default impl? For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can't forget this, so we're not gonna see broken directories that don't do abort() properly.","I don't think we need a default impl?","earwin","NULL","1","alternative","0","1","0","0","0"
"2203","2203","46704","4600","I think we can make a default impl that simply closes & suppresses exceptions? (We can't .deleteFile since an abstract IO doesn't know its Dir). Our concrete impls can override w/ versions that do delete the file...
I don't think we need a default impl? For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can't forget this, so we're not gonna see broken directories that don't do abort() properly.","For some directory impls close() is a noop + what is more important, having abstract method forces you to implement it, you can't forget this, so we're not gonna see broken directories that don't do abort() properly.","earwin","NULL","1","alternative, pro","0","1","1","0","0"
"2204","2204","46705","4600","This change is really minor, but I think, convinient.
You don't have to lug reference to Directory along, and recalculate the file name, if the only thing you want to say is that write was a failure and you no longer need this file.","This change is really minor, but I think, convinient.","earwin","NULL","1","pro","0","0","1","0","0"
"2205","2205","46705","4600","This change is really minor, but I think, convinient.
You don't have to lug reference to Directory along, and recalculate the file name, if the only thing you want to say is that write was a failure and you no longer need this file.","You don't have to lug reference to Directory along, and recalculate the file name, if the only thing you want to say is that write was a failure and you no longer need this file.","earwin","NULL","1","alternative","0","1","0","0","0"
"2206","2206","46706","4600","I offered a default impl just to not break the API. I don't think a default impl is a good option. If we're ok making an exception for 3x as well (I know I am), then I don't think we should have a default impl.","I offered a default impl just to not break the API.","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"2207","2207","46706","4600","I offered a default impl just to not break the API. I don't think a default impl is a good option. If we're ok making an exception for 3x as well (I know I am), then I don't think we should have a default impl.","I don't think a default impl is a good option.","shaie","NULL","1","con","0","0","0","1","0"
"2208","2208","46706","4600","I offered a default impl just to not break the API. I don't think a default impl is a good option. If we're ok making an exception for 3x as well (I know I am), then I don't think we should have a default impl.","If we're ok making an exception for 3x as well (I know I am), then I don't think we should have a default impl.","shaie","NULL","1","alternative, con","0","1","0","1","0"
"2209","2209","46707","4600","I think a bw compat exception is fine too!","I think a bw compat exception is fine too!","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"2210","2210","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.","shaie","NULL","1","alternative","0","1","0","0","0"
"2211","2211","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","Added a test case to TestIndexReader.","shaie","NULL","1","alternative","0","1","0","0","0"
"2212","2212","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","IndexCommit implements Comparable.","shaie","NULL","1","alternative","0","1","0","0","0"
"2213","2213","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).","shaie","NULL","1","alternative","0","1","0","0","0"
"2214","2214","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","I did not implement ReaderCommit to support deletes.","shaie","NULL","1","alternative","0","1","0","0","0"
"2215","2215","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use.","shaie","NULL","1","alternative, con","0","1","0","1","0"
"2216","2216","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions.","shaie","NULL","1","alternative, pro, con","0","1","1","1","0"
"2217","2217","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","Not sure it's worth the efforts.","shaie","NULL","1","con","0","0","0","1","0"
"2218","2218","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","I believe it is ready to commit.","shaie","NULL","1","pro","0","0","1","0","0"
"2219","2219","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","I'll wait a day or two until I commit it.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2220","2220","47446","4661","Patch against 3x:

Changes listCommits() signature to return a List<IndexCommit>
DirReader.listCommits() sorts the list in the end.
Added a test case to TestIndexReader.
IndexCommit implements Comparable. Removed impl from CommitPoint (which also removed a redundant duplicate 'gen' member).

I did not implement ReaderCommit to support deletes. Obtaining the lock for this purpose does not seem the right way to me ... IndexWriter has a deleteUnusedFiles which the application can use. If the app only does IR.listCommits, then being able to delete is an advantage, but otherwise it will need to mess with LockObtainFail exceptions. Not sure it's worth the efforts.
I believe it is ready to commit. I'll wait a day or two until I commit it. Your comments are welcome.","Your comments are welcome.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2221","2221","47447","4661","Shai, looks good to me! 
+1 to commit","Shai, looks good to me!","simonw","NULL","1","pro","0","0","1","0","0"
"2222","2222","47447","4661","Shai, looks good to me! 
+1 to commit","+1 to commit","simonw","NULL","1","pro","0","0","1","0","0"
"2223","2223","47448","4661","Committed revision 1034080 + 1034144 (3x). Due to backwards tests failure, I kept the method signature as returning Collection, and only documented the new behavior.
Committed revision 1034140 (trunk).","Committed revision 1034080 + 1034144 (3x).","shaie","NULL","1","decision","0","0","0","0","1"
"2224","2224","47448","4661","Committed revision 1034080 + 1034144 (3x). Due to backwards tests failure, I kept the method signature as returning Collection, and only documented the new behavior.
Committed revision 1034140 (trunk).","Due to backwards tests failure, I kept the method signature as returning Collection, and only documented the new behavior.","shaie","NULL","1","alternative","0","1","0","0","0"
"2225","2225","47448","4661","Committed revision 1034080 + 1034144 (3x). Due to backwards tests failure, I kept the method signature as returning Collection, and only documented the new behavior.
Committed revision 1034140 (trunk).","Committed revision 1034140 (trunk).","shaie","NULL","1","decision","0","0","0","0","1"
"2226","2226","47449","4661","Java 5 allows covariant return types. Could we not declare both methods in 3.x and deprecate the old one? In trunk we can remove it and only provide List.","Java 5 allows covariant return types.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2227","2227","47449","4661","Java 5 allows covariant return types. Could we not declare both methods in 3.x and deprecate the old one? In trunk we can remove it and only provide List.","Could we not declare both methods in 3.x and deprecate the old one?","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2228","2228","47449","4661","Java 5 allows covariant return types. Could we not declare both methods in 3.x and deprecate the old one? In trunk we can remove it and only provide List.","In trunk we can remove it and only provide List.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2229","2229","47450","4661","We cannot declare both methods  But backwards does not fail now","We cannot declare both methods  But backwards does not fail now","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"2230","2230","47451","4661","Bulk close for 3.1","Bulk close for 3.1","gsingers","NULL","1","decision","0","0","0","0","1"
"2231","2231","48092","4725","Clearing 3.1 fix version... it's not clear how we can fix this w/o drastic API changes...","Clearing 3.1 fix version... it's not clear how we can fix this w/o drastic API changes...","mikemccand","NULL","1","issue","1","0","0","0","0"
"2232","2232","48093","4725","
It is actually possible to force this, today, by having your collector return false from acceptDocsOutOfOrder...
Well you are using a custom collector anyway if you are doing this, so can't we just add a sentence to that
method's javadocs indicating that you should return false if you want to use the scorer navigation apis?","It is actually possible to force this, today, by having your collector return false from acceptDocsOutOfOrder...
Well you are using a custom collector anyway if you are doing this, so can't we just add a sentence to that
method's javadocs indicating that you should return false if you want to use the scorer navigation apis?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2233","2233","48094","4725","I think this issue is fixed already? VisitSubScorers works in 3.6.2 (if it gets released, Robert backported) and in 4.0 its working, too?
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?","I think this issue is fixed already?","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2234","2234","48094","4725","I think this issue is fixed already? VisitSubScorers works in 3.6.2 (if it gets released, Robert backported) and in 4.0 its working, too?
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?","VisitSubScorers works in 3.6.2 (if it gets released, Robert backported) and in 4.0 its working, too?","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2235","2235","48094","4725","I think this issue is fixed already? VisitSubScorers works in 3.6.2 (if it gets released, Robert backported) and in 4.0 its working, too?
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?","As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"2236","2236","48095","4725","
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?
+1, i think for freq() and getChildren() we should throw UOE with text like this. But we can also do the javadocs too.
Then i think there would be a lot less surprises.","As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2237","2237","48095","4725","
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?
+1, i think for freq() and getChildren() we should throw UOE with text like this. But we can also do the javadocs too.
Then i think there would be a lot less surprises.","+1, i think for freq() and getChildren() we should throw UOE with text like this.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2238","2238","48095","4725","
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?
+1, i think for freq() and getChildren() we should throw UOE with text like this. But we can also do the javadocs too.
Then i think there would be a lot less surprises.","But we can also do the javadocs too.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2239","2239","48095","4725","
As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?
+1, i think for freq() and getChildren() we should throw UOE with text like this. But we can also do the javadocs too.
Then i think there would be a lot less surprises.","Then i think there would be a lot less surprises.","rcmuir","NULL","1","pro","0","0","1","0","0"
"2240","2240","48099","4725","An idea (separate issue!) would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","An idea (separate issue!)","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2241","2241","48099","4725","An idea (separate issue!) would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2242","2242","48099","4725","An idea (separate issue!) would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","In my opinion, BS1 should not implement the Scorer interface, that the whole bug!","thetaphi","NULL","1","alternative, con","0","1","0","1","0"
"2243","2243","48099","4725","An idea (separate issue!) would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2244","2244","48099","4725","An idea (separate issue!) would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2245","2245","48099","4725","An idea (separate issue!) would be:
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2246","2246","48100","4725","Collectible... (not serious)","Collectible... (not serious)","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2247","2247","48102","4725","
If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation? (Or scoreDocAtOnce, scoreDocAtATime, something else...).
I actually just remembered the query-time join i think does this too?
But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than
scoreDocsInOrder (its standard IR terminology and less confusing).","
If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2248","2248","48102","4725","
If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation? (Or scoreDocAtOnce, scoreDocAtATime, something else...).
I actually just remembered the query-time join i think does this too?
But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than
scoreDocsInOrder (its standard IR terminology and less confusing).","(Or scoreDocAtOnce, scoreDocAtATime, something else...).","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2249","2249","48102","4725","
If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation? (Or scoreDocAtOnce, scoreDocAtATime, something else...).
I actually just remembered the query-time join i think does this too?
But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than
scoreDocsInOrder (its standard IR terminology and less confusing).","I actually just remembered the query-time join i think does this too?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2250","2250","48102","4725","
If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation? (Or scoreDocAtOnce, scoreDocAtATime, something else...).
I actually just remembered the query-time join i think does this too?
But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than
scoreDocsInOrder (its standard IR terminology and less confusing).","But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than
scoreDocsInOrder (its standard IR terminology and less confusing).","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2251","2251","48104","4725","Bulk move 4.4 issues to 4.5 and 5.0","Bulk move 4.4 issues to 4.5 and 5.0","steve_rowe","NULL","1","decision","0","0","0","0","1"
"2252","2252","48105","4725","Move issue to Lucene 4.9.","Move issue to Lucene 4.9.","thetaphi","NULL","1","decision","0","0","0","0","1"
"2253","2253","49665","4844","I tried to start on this however, nothing can be deleted without the terms dictionary and the terms docs working in order to obtain the doc ids to delete.","I tried to start on this however, nothing can be deleted without the terms dictionary and the terms docs working in order to obtain the doc ids to delete.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2254","2254","49666","4844","Resolving deleted terms -> doc IDs doesn't require a sorted terms dict right?  Ie a simple hash lookup suffices?","Resolving deleted terms -> doc IDs doesn't require a sorted terms dict right?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2255","2255","49666","4844","Resolving deleted terms -> doc IDs doesn't require a sorted terms dict right?  Ie a simple hash lookup suffices?","Ie a simple hash lookup suffices?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2256","2256","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2257","2257","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","Ie a simple hash lookup suffices?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2258","2258","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","True, however I figured it'd be best to try our own dog food, or
APIs.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2259","2259","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2260","2260","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","Then we should be able to
implement deleting, which doesn't require skip lists.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2261","2261","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2262","2262","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","getReader would block any writes that could
be altering the *BlockPools.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2263","2263","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","Maybe this is a good first step?","jasonrutherglen","NULL","1","pro","0","0","1","0","0"
"2264","2264","49667","4844","Resolving deleted terms -> doc IDs doesn't require a
sorted terms dict right? Ie a simple hash lookup suffices?
True, however I figured it'd be best to try our own dog food, or
APIs. I think the main issue right now is the concurrency of the
*BlockPools from LUCENE-2575. Then we should be able to
implement deleting, which doesn't require skip lists. I guess if
we really wanted to, we could simply buffer terms and only apply
them in getReader.  getReader would block any writes that could
be altering the *BlockPools. Maybe this is a good first step? Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?  ","Is there
any reason we need to apply deletes in the actual updateDoc and
deleteDoc methods?","jasonrutherglen","NULL","0",NULL,"0","0","0","0","0"
"2265","2265","49668","4844","I'm implementing a basic doc id iterator per DWPT which will allow us to implement delete by term, and the deleted docs sequence ids.  This is for merging of segments?  However we're using readers to do the merging so this really won't be useful?","I'm implementing a basic doc id iterator per DWPT which will allow us to implement delete by term, and the deleted docs sequence ids.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2266","2266","49668","4844","I'm implementing a basic doc id iterator per DWPT which will allow us to implement delete by term, and the deleted docs sequence ids.  This is for merging of segments?  However we're using readers to do the merging so this really won't be useful?","This is for merging of segments?","jasonrutherglen","NULL","0",NULL,"0","0","0","0","0"
"2267","2267","49668","4844","I'm implementing a basic doc id iterator per DWPT which will allow us to implement delete by term, and the deleted docs sequence ids.  This is for merging of segments?  However we're using readers to do the merging so this really won't be useful?","However we're using readers to do the merging so this really won't be useful?","jasonrutherglen","NULL","1","alternative, con","0","1","0","1","0"
"2268","2268","49669","4844","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted? I believe we need a secondary sequence id
array for deleted docs that is init'd to -1. When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr. When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted. 
I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM. Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted?","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2269","2269","49669","4844","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted? I believe we need a secondary sequence id
array for deleted docs that is init'd to -1. When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr. When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted. 
I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM. Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","I believe we need a secondary sequence id
array for deleted docs that is init'd to -1.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2270","2270","49669","4844","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted? I believe we need a secondary sequence id
array for deleted docs that is init'd to -1. When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr. When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted. 
I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM. Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2271","2271","49669","4844","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted? I believe we need a secondary sequence id
array for deleted docs that is init'd to -1. When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr. When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted. 
I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM. Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted.","jasonrutherglen","NULL","1","alternative, pro","0","1","1","0","0"
"2272","2272","49669","4844","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted? I believe we need a secondary sequence id
array for deleted docs that is init'd to -1. When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr. When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted. 
I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM. Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM.","jasonrutherglen","NULL","1","issue","1","0","0","0","0"
"2273","2273","49669","4844","For the deleted docs sequence id array, perhaps I'm a little bit
confused, but how will we signify in the sequence id array if a
document is deleted? I believe we need a secondary sequence id
array for deleted docs that is init'd to -1. When a document is
deleted, the sequence id is set for that doc in the
del-docs-seq-arr. When the deleted docs Bits is being accessed,
for a given doc, we'll compare the IRs seq-id-up-to with the
del-docs-seq-id, and if the IR seq-id is greater than or equal
to, the Bits.get method will return true, meaning the document
is deleted. 
I am forgetting how concurrency will work in this case, ie,
insuring multi-threaded visibility due to the JMM. Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","Actually,
because we're pausing the writes/deletes when get reader is
called on the DWPT, JMM concurrency should be OK.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2274","2274","49670","4844","If we implement deletes via sequence id across all segments, then the .del file should probably remain the same (a set of bits)?  Also, when we load up the BV on IW start, then I guess we'll need to init the array appropriately.","If we implement deletes via sequence id across all segments, then the .del file should probably remain the same (a set of bits)?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2275","2275","49670","4844","If we implement deletes via sequence id across all segments, then the .del file should probably remain the same (a set of bits)?  Also, when we load up the BV on IW start, then I guess we'll need to init the array appropriately.","Also, when we load up the BV on IW start, then I guess we'll need to init the array appropriately.","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2276","2276","49672","4844","In regards to the deltas, when they're in RAM (ie, for norm and DF updates), I'm guessing we'd need to place the updates into a hash map (that hopefully uses primitives instead of objects to save RAM)?  We could instantiate a new array when the map reached a certain size?","In regards to the deltas, when they're in RAM (ie, for norm and DF updates), I'm guessing we'd need to place the updates into a hash map (that hopefully uses primitives instead of objects to save RAM)?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2277","2277","49672","4844","In regards to the deltas, when they're in RAM (ie, for norm and DF updates), I'm guessing we'd need to place the updates into a hash map (that hopefully uses primitives instead of objects to save RAM)?  We could instantiate a new array when the map reached a certain size?","We could instantiate a new array when the map reached a certain size?","jasonrutherglen","NULL","1","alternative","0","1","0","0","0"
"2278","2278","49674","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks! ","Does anybody know where to checkout the realtime branch?","hyan","NULL","0",NULL,"0","0","0","0","0"
"2279","2279","49674","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks! ","I am very interested in it!","hyan","NULL","0",NULL,"0","0","0","0","0"
"2280","2280","49674","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks! ","Thanks!","hyan","NULL","0",NULL,"0","0","0","0","0"
"2281","2281","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","Does anybody know where to checkout the realtime branch?","simonw","NULL","0",NULL,"0","0","0","0","0"
"2282","2282","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","I am very interested in it!","simonw","NULL","0",NULL,"0","0","0","0","0"
"2283","2283","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","Thanks!","simonw","NULL","0",NULL,"0","0","0","0","0"
"2284","2284","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","there is no realtime branch open right now.","simonw","NULL","0",NULL,"0","0","0","0","0"
"2285","2285","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","We had to delete it since we re-integrated it for DocumentsWriterPerThread.","simonw","NULL","1","decision","0","0","0","0","1"
"2286","2286","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","(SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast.","simonw","NULL","0",NULL,"0","0","0","0","0"
"2287","2287","49675","4844","Does anybody know where to checkout the realtime branch? I am very interested in it! Thanks!
there is no realtime branch open right now. We had to delete it since we re-integrated it for DocumentsWriterPerThread. (SVN requires that once you have re-integrated) However, there is no development happening along those lines right now and we didn't decide if we move forward since for general purpose the NRT features we have is reasonably fast. Anyway, I think there is still a need for this if we can provide it as a non-default option?","Anyway, I think there is still a need for this if we can provide it as a non-default option?","simonw","NULL","1","alternative","0","1","0","0","0"
"2288","2288","50315","4920","Can you post here the full stacktrace?","Can you post here the full stacktrace?","shaie","NULL","0",NULL,"0","0","0","0","0"
"2289","2289","50316","4920","I agree, we should fix this.  I'll change to a try/finally w/ a success boolean.
You can use IndexWriter#unlock to forcefully remove the lock, as a workaround.","I agree, we should fix this.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2290","2290","50316","4920","I agree, we should fix this.  I'll change to a try/finally w/ a success boolean.
You can use IndexWriter#unlock to forcefully remove the lock, as a workaround.","I'll change to a try/finally w/ a success boolean.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2291","2291","50316","4920","I agree, we should fix this.  I'll change to a try/finally w/ a success boolean.
You can use IndexWriter#unlock to forcefully remove the lock, as a workaround.","You can use IndexWriter#unlock to forcefully remove the lock, as a workaround.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2292","2292","50318","4920","Patch.","Patch.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2293","2293","50319","4920","I tried both IndexWriter#unlock and Directory#cleanLock(IndexWriter.WRITE_LOCK_NAME) but non of those removed the entry from LOCK_HELD HashSet. It was unchanged.
Ahh, sorry, I think you are hitting LUCENE-2104.","I tried both IndexWriter#unlock and Directory#cleanLock(IndexWriter.WRITE_LOCK_NAME) but non of those removed the entry from LOCK_HELD HashSet.","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"2294","2294","50319","4920","I tried both IndexWriter#unlock and Directory#cleanLock(IndexWriter.WRITE_LOCK_NAME) but non of those removed the entry from LOCK_HELD HashSet. It was unchanged.
Ahh, sorry, I think you are hitting LUCENE-2104.","It was unchanged.","mikemccand","NULL","1","alternative, con","0","1","0","1","0"
"2295","2295","50319","4920","I tried both IndexWriter#unlock and Directory#cleanLock(IndexWriter.WRITE_LOCK_NAME) but non of those removed the entry from LOCK_HELD HashSet. It was unchanged.
Ahh, sorry, I think you are hitting LUCENE-2104.","Ahh, sorry, I think you are hitting LUCENE-2104.","mikemccand","NULL","1","issue","1","0","0","0","0"
"2296","2296","50320","4920","Just to confirm this patch as fix.
The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.","Just to confirm this patch as fix.","cstamas","NULL","1","alternative","0","1","0","0","0"
"2297","2297","50320","4920","Just to confirm this patch as fix.
The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem.","cstamas","NULL","1","alternative, pro","0","1","1","0","0"
"2298","2298","50320","4920","Just to confirm this patch as fix.
The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.","The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.","cstamas","NULL","1","pro","0","0","1","0","0"
"2299","2299","50321","4920","Yes, I do hit LUCENE-2104 at the same time... nice.","Yes, I do hit LUCENE-2104 at the same time... nice.","cstamas","NULL","1","issue","1","0","0","0","0"
"2300","2300","50322","4920","Out of curiosity - would you mind posting here the exception?","Out of curiosity - would you mind posting here the exception?","shaie","NULL","0",NULL,"0","0","0","0","0"
"2301","2301","50323","4920","This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it. Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again. After patch above applied to 3.0.1, the UT does pass okay.
This is the stack trace I have with vanilla 3.0.1:

org.sonatype.timeline.TimelineException: Fail to configure timeline index!
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)
	at org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)
	at org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)
	at org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock
	at org.apache.lucene.store.Lock.obtain(Lock.java:84)
	at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)
	... 19 more

","This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it.","cstamas","NULL","1","issue","1","0","0","0","0"
"2302","2302","50323","4920","This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it. Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again. After patch above applied to 3.0.1, the UT does pass okay.
This is the stack trace I have with vanilla 3.0.1:

org.sonatype.timeline.TimelineException: Fail to configure timeline index!
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)
	at org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)
	at org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)
	at org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock
	at org.apache.lucene.store.Lock.obtain(Lock.java:84)
	at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)
	... 19 more

","Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again.","cstamas","NULL","1","issue","1","0","0","0","0"
"2303","2303","50323","4920","This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it. Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again. After patch above applied to 3.0.1, the UT does pass okay.
This is the stack trace I have with vanilla 3.0.1:

org.sonatype.timeline.TimelineException: Fail to configure timeline index!
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)
	at org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)
	at org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)
	at org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock
	at org.apache.lucene.store.Lock.obtain(Lock.java:84)
	at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)
	... 19 more

","After patch above applied to 3.0.1, the UT does pass okay.","cstamas","NULL","1","alternative, pro","0","1","1","0","0"
"2304","2304","50323","4920","This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it. Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again. After patch above applied to 3.0.1, the UT does pass okay.
This is the stack trace I have with vanilla 3.0.1:

org.sonatype.timeline.TimelineException: Fail to configure timeline index!
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)
	at org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)
	at org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)
	at org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock
	at org.apache.lucene.store.Lock.obtain(Lock.java:84)
	at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)
	... 19 more

","This is the stack trace I have with vanilla 3.0.1:

org.sonatype.timeline.TimelineException: Fail to configure timeline index!","cstamas","NULL","0",NULL,"0","0","0","0","0"
"2305","2305","50323","4920","This is an UT, that 1st copies a known (broken) Index files to a place, and than tries to use it. Naturally, it fails (since the index files are corrupted), and then it tries to recreate the index files and recreate the index content, but it fails to obtain the write lock again. After patch above applied to 3.0.1, the UT does pass okay.
This is the stack trace I have with vanilla 3.0.1:

org.sonatype.timeline.TimelineException: Fail to configure timeline index!
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)
	at org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)
	at org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)
	at org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock
	at org.apache.lucene.store.Lock.obtain(Lock.java:84)
	at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)
	... 19 more

","at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:106)
	at org.sonatype.timeline.DefaultTimeline.repairTimelineIndexer(DefaultTimeline.java:79)
	at org.sonatype.timeline.DefaultTimeline.configure(DefaultTimeline.java:60)
	at org.sonatype.timeline.TimelineTest.testRepairIndexCouldNotRead(TimelineTest.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/cstamas/worx/sonatype/spice/trunk/spice-timeline/target/index/write.lock
	at org.apache.lucene.store.Lock.obtain(Lock.java:84)
	at org.apache.lucene.index.IndexWriter.init(IndexWriter.java:1045)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:868)
	at org.sonatype.timeline.DefaultTimelineIndexer.configure(DefaultTimelineIndexer.java:99)
	... 19 more","cstamas","NULL","0",NULL,"0","0","0","0","0"
"2306","2306","50324","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"2307","2307","50324","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2308","2308","50324","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","OK thanks for confirming  I'll backport to 3.0.x as well.","mikemccand","NULL","1","decision","0","0","0","0","1"
"2309","2309","50324","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","(Yes patch is against trunk).","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"2310","2310","50325","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem.","mikemccand","NULL","1","alternative, pro","0","1","1","0","0"
"2311","2311","50325","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2312","2312","50325","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","OK thanks for confirming  I'll backport to 3.0.x as well.","mikemccand","NULL","1","decision","0","0","0","0","1"
"2313","2313","50325","4920","The patch applied to 3.0.1 (I had to do it manually, since I believe this patch is against trunk, not 3.0.1) does fix my problem. The IndexWriter is now successfully recreated and my UT does recover just fine from corrupted indexes.
OK thanks for confirming  I'll backport to 3.0.x as well.
(Yes patch is against trunk).","(Yes patch is against trunk).","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"2314","2314","50326","4920","This exception shows a LockObtainFailed exception - can you post the one that resulted in NegativeArraySize  curious to know where you hit it, and what sort of corruption yields to that .","This exception shows a LockObtainFailed exception - can you post the one that resulted in NegativeArraySize  curious to know where you hit it, and what sort of corruption yields to that .","shaie","NULL","1","issue","1","0","0","0","0"
"2315","2315","50328","4920","Merged to 2.9 revision: 949507","Merged to 2.9 revision: 949507","thetaphi","NULL","1","decision","0","0","0","0","1"
"2318","2316","50331","4921","Adding a Google Map to help visualise the problem. 
The bounding box and my search point location (center) are shown as red dots
The blue dot is the location of the hit I am expecting to get but don't - In my real data there are many others around it.
The yellow dot is the location I added that is outside the box but inside the search circle. This lead me to conclude that the Bounding Box is not the issue.","The bounding box and my search point location (center) are shown as red dots
The blue dot is the location of the hit I am expecting to get but don't - In my real data there are many others around it.","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2319","2317","50331","4921","Adding a Google Map to help visualise the problem. 
The bounding box and my search point location (center) are shown as red dots
The blue dot is the location of the hit I am expecting to get but don't - In my real data there are many others around it.
The yellow dot is the location I added that is outside the box but inside the search circle. This lead me to conclude that the Bounding Box is not the issue.","The yellow dot is the location I added that is outside the box but inside the search circle.","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2320","2318","50331","4921","Adding a Google Map to help visualise the problem. 
The bounding box and my search point location (center) are shown as red dots
The blue dot is the location of the hit I am expecting to get but don't - In my real data there are many others around it.
The yellow dot is the location I added that is outside the box but inside the search circle. This lead me to conclude that the Bounding Box is not the issue.","This lead me to conclude that the Bounding Box is not the issue.","jpa458","NULL","1","pro","0","0","1","0","0"
"2321","2319","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter.","jpa458","NULL","1","issue","1","0","0","0","0"
"2322","2320","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.","jpa458","NULL","1","issue","1","0","0","0","0"
"2323","2321","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","This means to me that we end up searching in the wrong tier.","jpa458","NULL","1","issue","1","0","0","0","0"
"2324","2322","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2325","2323","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx",">>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.","jpa458","NULL","1","issue","1","0","0","0","0"
"2326","2324","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.","jpa458","NULL","1","issue","1","0","0","0","0"
"2327","2325","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","If I change this to not divide by 2 - my issue test case passes - as do all my other tests.","jpa458","NULL","1","alternative, pro","0","1","1","0","0"
"2328","2326","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2329","2327","50332","4921","OK, so ignoring the whole Boundary Box story and looking deeper into the code with my test case I noticed that a different bestFit value was being determined by the CartesianTierPlotter. 
I get a value of 13 for the test that passes (radius 32miles) and 14 when the test fails with a search radius of 31.  This means to me that we end up searching in the wrong tier. 
Looking at CartesianTierPlotter.bestFit() I see on the  line below the passed in value of miles is divided by 2.
>>> double r = miles / 2.0; 
I'm guessing r is meant to be a radius - but the miles parameter is already a radius  - of my search circle.
This has an effect on the calculation of the best fix box width - aka corner in the code - and the resulting bestFit or tierId.
If I change this to not divide by 2 - my issue test case passes - as do all my other tests.
Again I'd appreciate if someone who knows the code could comment and confirm my finding or tell my I'm crazy!
Thx","Thx","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2330","2328","50333","4921","I got pen and paper out and worked out  the calculation being done in  CartesianTierPlotter.bestFit().
>>>  double corner = r - Math.sqrt(Math.pow(r, 2) / 2.0d);
I ended up with the same formula and it is definitely expecting the radius of the search circle as param.
There is therefore no need to divide miles param by 2.
BTW the formula can be simplified to 
//corner is the width/height of the box that fits between the arc of the search circle 
//and a corner of the boundary box containing the search circle
double corner = r - r/Math.sqrt(2);
","I got pen and paper out and worked out  the calculation being done in  CartesianTierPlotter.bestFit().","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2331","2329","50333","4921","I got pen and paper out and worked out  the calculation being done in  CartesianTierPlotter.bestFit().
>>>  double corner = r - Math.sqrt(Math.pow(r, 2) / 2.0d);
I ended up with the same formula and it is definitely expecting the radius of the search circle as param.
There is therefore no need to divide miles param by 2.
BTW the formula can be simplified to 
//corner is the width/height of the box that fits between the arc of the search circle 
//and a corner of the boundary box containing the search circle
double corner = r - r/Math.sqrt(2);
",">>>  double corner = r - Math.sqrt(Math.pow(r, 2) / 2.0d);
I ended up with the same formula and it is definitely expecting the radius of the search circle as param.","jpa458","NULL","1","alternative","0","1","0","0","0"
"2332","2330","50333","4921","I got pen and paper out and worked out  the calculation being done in  CartesianTierPlotter.bestFit().
>>>  double corner = r - Math.sqrt(Math.pow(r, 2) / 2.0d);
I ended up with the same formula and it is definitely expecting the radius of the search circle as param.
There is therefore no need to divide miles param by 2.
BTW the formula can be simplified to 
//corner is the width/height of the box that fits between the arc of the search circle 
//and a corner of the boundary box containing the search circle
double corner = r - r/Math.sqrt(2);
","There is therefore no need to divide miles param by 2.","jpa458","NULL","1","alternative","0","1","0","0","0"
"2333","2331","50333","4921","I got pen and paper out and worked out  the calculation being done in  CartesianTierPlotter.bestFit().
>>>  double corner = r - Math.sqrt(Math.pow(r, 2) / 2.0d);
I ended up with the same formula and it is definitely expecting the radius of the search circle as param.
There is therefore no need to divide miles param by 2.
BTW the formula can be simplified to 
//corner is the width/height of the box that fits between the arc of the search circle 
//and a corner of the boundary box containing the search circle
double corner = r - r/Math.sqrt(2);
","BTW the formula can be simplified to 
//corner is the width/height of the box that fits between the arc of the search circle 
//and a corner of the boundary box containing the search circle
double corner = r - r/Math.sqrt(2);","jpa458","NULL","1","alternative, pro","0","1","1","0","0"
"2334","2332","50334","4921","Hi Julian
Your problem should be solved by work discussed here https://issues.apache.org/jira/browse/LUCENE-2359","Hi Julian
Your problem should be solved by work discussed here https://issues.apache.org/jira/browse/LUCENE-2359","nicolas.helleringer","NULL","1","alternative","0","1","0","0","0"
"2335","2333","50335","4921","Thanks Nicolas.  It took me a while but I finally got round to verifying your patch - I was using my fix in the meantime but your patch addresses other issues as well.  I've backported and all my tests pass. I'd mark this as resolved but your patch has been reverted I see.","Thanks Nicolas.","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2336","2334","50335","4921","Thanks Nicolas.  It took me a while but I finally got round to verifying your patch - I was using my fix in the meantime but your patch addresses other issues as well.  I've backported and all my tests pass. I'd mark this as resolved but your patch has been reverted I see.","It took me a while but I finally got round to verifying your patch - I was using my fix in the meantime but your patch addresses other issues as well.","jpa458","NULL","1","alternative, pro","0","1","1","0","0"
"2337","2335","50335","4921","Thanks Nicolas.  It took me a while but I finally got round to verifying your patch - I was using my fix in the meantime but your patch addresses other issues as well.  I've backported and all my tests pass. I'd mark this as resolved but your patch has been reverted I see.","I've backported and all my tests pass.","jpa458","NULL","1","decision","0","0","0","0","1"
"2338","2336","50335","4921","Thanks Nicolas.  It took me a while but I finally got round to verifying your patch - I was using my fix in the meantime but your patch addresses other issues as well.  I've backported and all my tests pass. I'd mark this as resolved but your patch has been reverted I see.","I'd mark this as resolved but your patch has been reverted I see.","jpa458","NULL","1","decision","0","0","0","0","1"
"2339","2337","50336","4921","I confirm that Nicolas' patch in LUCENE-2359 resolves this issue.","I confirm that Nicolas' patch in LUCENE-2359 resolves this issue.","jpa458","NULL","1","pro, decision","0","0","1","0","1"
"2340","2338","50337","4921","This implementation of geo in Lucene has been deprecated and will not be fixed any further nor backported. see LUCENE-1747","This implementation of geo in Lucene has been deprecated and will not be fixed any further nor backported.","nicolas.helleringer","NULL","1","decision","0","0","0","0","1"
"2341","2339","50337","4921","This implementation of geo in Lucene has been deprecated and will not be fixed any further nor backported. see LUCENE-1747","see LUCENE-1747","nicolas.helleringer","NULL","0",NULL,"0","0","0","0","0"
"2342","2340","50338","4921","Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.
By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.  The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.  So this issue is fixed, in a sense.  But the relevant code is gone.","Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.","dsmiley","NULL","1","con, decision","0","0","0","1","1"
"2343","2341","50338","4921","Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.
By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.  The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.  So this issue is fixed, in a sense.  But the relevant code is gone.","By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.","dsmiley","NULL","1","alternative, pro","0","1","1","0","0"
"2344","2342","50338","4921","Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.
By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.  The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.  So this issue is fixed, in a sense.  But the relevant code is gone.","The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.","dsmiley","NULL","1","decision","0","0","0","0","1"
"2345","2343","50338","4921","Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.
By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.  The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.  So this issue is fixed, in a sense.  But the relevant code is gone.","So this issue is fixed, in a sense.","dsmiley","NULL","1","decision","0","0","0","0","1"
"2346","2344","50338","4921","Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.
By the way, Spatial4j correctly computes the bounding lat-lon box of a circle no matter where the circle is.  The original code from the old Lucene spatial project (and old Solr code too) was incorrect, and is no longer used.  So this issue is fixed, in a sense.  But the relevant code is gone.","But the relevant code is gone.","dsmiley","NULL","0",NULL,"0","0","0","0","0"
"2316","2345","50329","4921","Attachment with a test case and proposed fix","Attachment with a test case and proposed fix","jpa458","NULL","1","alternative","0","1","0","0","0"
"2317","2346","50331","4921","Adding a Google Map to help visualise the problem. 
The bounding box and my search point location (center) are shown as red dots
The blue dot is the location of the hit I am expecting to get but don't - In my real data there are many others around it.
The yellow dot is the location I added that is outside the box but inside the search circle. This lead me to conclude that the Bounding Box is not the issue.","Adding a Google Map to help visualise the problem.","jpa458","NULL","0",NULL,"0","0","0","0","0"
"2347","2347","50780","4949","Here are the instructions:

mkdir -p modules/analysis
svn add modules
svn move lucene/contrib/analyzers/* modules/analysis
patch -p0 < ../LUCENE-2444.patch


All tests pass.","Here are the instructions:

mkdir -p modules/analysis
svn add modules
svn move lucene/contrib/analyzers/* modules/analysis
patch -p0 < ../LUCENE-2444.patch


All tests pass.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2348","2348","50781","4949","+1, looks good!","+1, looks good!","yseeley@gmail.com","NULL","1","pro","0","0","1","0","0"
"2349","2349","50782","4949","If no one objects, i'd like to commit this first patch today to move the code in SVN.
This is just a step. Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE? I know i have been polluting the lucene one heavily from analyzer poaching.
CHANGES? I think a module having its own would make sense
... other things?

","If no one objects, i'd like to commit this first patch today to move the code in SVN.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2350","2350","50782","4949","If no one objects, i'd like to commit this first patch today to move the code in SVN.
This is just a step. Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE? I know i have been polluting the lucene one heavily from analyzer poaching.
CHANGES? I think a module having its own would make sense
... other things?

","This is just a step.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2351","2351","50782","4949","If no one objects, i'd like to commit this first patch today to move the code in SVN.
This is just a step. Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE? I know i have been polluting the lucene one heavily from analyzer poaching.
CHANGES? I think a module having its own would make sense
... other things?

","Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE?","rcmuir","NULL","1","issue","1","0","0","0","0"
"2352","2352","50782","4949","If no one objects, i'd like to commit this first patch today to move the code in SVN.
This is just a step. Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE? I know i have been polluting the lucene one heavily from analyzer poaching.
CHANGES? I think a module having its own would make sense
... other things?

","I know i have been polluting the lucene one heavily from analyzer poaching.","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2353","2353","50782","4949","If no one objects, i'd like to commit this first patch today to move the code in SVN.
This is just a step. Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE? I know i have been polluting the lucene one heavily from analyzer poaching.
CHANGES? I think a module having its own would make sense
... other things?

","CHANGES?","rcmuir","NULL","0",NULL,"0","0","0","0","0"
"2354","2354","50782","4949","If no one objects, i'd like to commit this first patch today to move the code in SVN.
This is just a step. Then we can keep the issue open and discuss what/if-any additional
things should be moved to the module:

LICENSE/NOTICE? I know i have been polluting the lucene one heavily from analyzer poaching.
CHANGES? I think a module having its own would make sense
... other things?

","I think a module having its own would make sense
... other things?","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2355","2355","50783","4949","This looks great Robert!","This looks great Robert!","mikemccand","NULL","1","pro","0","0","1","0","0"
"2356","2356","50784","4949","i applied this patch to a checkout, then removed contrib/analyzers completely.
there was a problem, the contrib-uptodate macro assumes contrib/*
So this patch fixes the problem by adding a module-uptodate macro, you can test it
by following the same instructions, but additionally doing 'rm -rf contrib/analyzers'.","i applied this patch to a checkout, then removed contrib/analyzers completely.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2357","2357","50784","4949","i applied this patch to a checkout, then removed contrib/analyzers completely.
there was a problem, the contrib-uptodate macro assumes contrib/*
So this patch fixes the problem by adding a module-uptodate macro, you can test it
by following the same instructions, but additionally doing 'rm -rf contrib/analyzers'.","there was a problem, the contrib-uptodate macro assumes contrib/*
So this patch fixes the problem by adding a module-uptodate macro, you can test it
by following the same instructions, but additionally doing 'rm -rf contrib/analyzers'.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2358","2358","50785","4949","Committed revision 941308.","Committed revision 941308.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2359","2359","50787","4949","ok if no one objects i'll commit this boilerplate stuff soon.
we can try to improve the language etc later but its a start.","ok if no one objects i'll commit this boilerplate stuff soon.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2360","2360","50787","4949","ok if no one objects i'll commit this boilerplate stuff soon.
we can try to improve the language etc later but its a start.","we can try to improve the language etc later but its a start.","rcmuir","NULL","1","issue","1","0","0","0","0"
"2361","2361","50788","4949","Committed LUCENE-2444_boilerplate.patch revision 941369.","Committed LUCENE-2444_boilerplate.patch revision 941369.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2362","2362","51001","4973","attached is a patch, its a little ugly since CharTermAttribute doesn't implement Replaceable ","attached is a patch, its a little ugly since CharTermAttribute doesn't implement Replaceable","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"2363","2363","51002","4973","Go for it, its a private impl class, what should we do else. Speed, speed, speed. Its better than coping into a StringBuilder before and after. Even Java 6 has no Replaceable interface!","Go for it, its a private impl class, what should we do else.","thetaphi","NULL","1","pro","0","0","1","0","0"
"2364","2364","51002","4973","Go for it, its a private impl class, what should we do else. Speed, speed, speed. Its better than coping into a StringBuilder before and after. Even Java 6 has no Replaceable interface!","Speed, speed, speed.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2365","2365","51002","4973","Go for it, its a private impl class, what should we do else. Speed, speed, speed. Its better than coping into a StringBuilder before and after. Even Java 6 has no Replaceable interface!","Its better than coping into a StringBuilder before and after.","thetaphi","NULL","1","pro","0","0","1","0","0"
"2366","2366","51002","4973","Go for it, its a private impl class, what should we do else. Speed, speed, speed. Its better than coping into a StringBuilder before and after. Even Java 6 has no Replaceable interface!","Even Java 6 has no Replaceable interface!","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2367","2367","51004","4973","attached is an updated patch, with examples in the overview etc.
I would like to commit at the end of the day if no one objects.","attached is an updated patch, with examples in the overview etc.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2368","2368","51004","4973","attached is an updated patch, with examples in the overview etc.
I would like to commit at the end of the day if no one objects.","I would like to commit at the end of the day if no one objects.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2369","2369","51005","4973","Committed revision 937039.","Committed revision 937039.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2370","2370","51006","4973","backported to 3x, revision 941698","backported to 3x, revision 941698","rcmuir","NULL","1","decision","0","0","0","0","1"
"2371","2371","51007","4973","Bulk close for 3.1","Bulk close for 3.1","gsingers","NULL","1","decision","0","0","0","0","1"
"2372","2372","51029","4979","Lets reuse IW.deleteUnusedFiles() ?
No need to multiply confusion )","Lets reuse IW.deleteUnusedFiles() ?","earwin","NULL","1","alternative","0","1","0","0","0"
"2373","2373","51029","4979","Lets reuse IW.deleteUnusedFiles() ?
No need to multiply confusion )","No need to multiply confusion )","earwin","NULL","1","pro","0","0","1","0","0"
"2374","2374","51030","4979","Lets reuse IW.deleteUnusedFiles() ?
+1","Lets reuse IW.deleteUnusedFiles() ?","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2375","2375","51030","4979","Lets reuse IW.deleteUnusedFiles() ?
+1","+1","mikemccand","NULL","1","pro","0","0","1","0","0"
"2376","2376","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.","shaie","NULL","1","alternative","0","1","0","0","0"
"2377","2377","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory.","shaie","NULL","1","issue","1","0","0","0","0"
"2378","2378","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.","shaie","NULL","1","issue","1","0","0","0","0"
"2379","2379","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference.","shaie","NULL","1","alternative, pro, con","0","1","1","1","0"
"2380","2380","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other.","shaie","NULL","1","issue","1","0","0","0","0"
"2381","2381","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing.","shaie","NULL","1","alternative, pro, con","0","1","1","1","0"
"2382","2382","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","Even cloning SI did not work because it might have changed just before the clone.","shaie","NULL","1","issue","1","0","0","0","0"
"2383","2383","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","Only when passing rollbackSI to checkpoint does the test pass.","shaie","NULL","1","alternative, pro","0","1","1","0","0"
"2384","2384","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles.","shaie","NULL","1","con","0","0","0","1","0"
"2385","2385","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","So I guess it's a NRT problem only.","shaie","NULL","1","issue","1","0","0","0","0"
"2386","2386","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g.","shaie","NULL","1","con","0","0","0","1","0"
"2387","2387","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","if someone extends IW and adds some logic which requires reading SI ...","shaie","NULL","1","alternative","0","1","0","0","0"
"2388","2388","51031","4979","Patch changes deleteUnusedFiles to call IFD.checkpoint and also adds a testDeleteUnusedFiles2 to TestIndexWriter.
Currently, TestIndexWriterReader.testDuringAddIndexes fails, if deleteUnusedFiles is coded like this:


public synchronized void deleteUnusedFiles() throws IOException {
  deleter.checkpoint(segmentInfos, true);
}


The failure happens in CommitPoint's ctor in the assert statement which verifies the SegmentInos does not have external Directory. When I debug-traced the test, it passed and so I concluded it's a concurrency issue (and indeed testDuringAddIndexes spawns several threads.
addIndexesNoOptimize does change SegmentInfos as it adds indexes, however at the end it 'fixes' their Directory reference. I wondered how is regular commit() works when addIndexesNoOptimize is called, but couldn't find any synchronization block where one blocks the other. Eventually, I've changed deleteUnusedFiles to this:


public synchronized void deleteUnusedFiles() throws IOException {
  synchronized (commitLock) {
    deleter.checkpoint(rollbackSegmentInfos, true);
    // deleter.checkpoint((SegmentInfos) segmentInfos.clone(), true);
  }
}


I've tried to sync on commitLock (which seems good anyway), but the test kept failing. Even cloning SI did not work because it might have changed just before the clone. Only when passing rollbackSI to checkpoint does the test pass. But I'm not sure if that's the right solution, as when I debug-traced it and put a break point just before the call to checkpoint, SI included one segment w/ a different name than rollbackSI ...
BTW, the test fails on DirReader.doClose, where it checks if writer != null and then calls deleteUnusedFiles. So I guess it's a NRT problem only.
In general, that that addIndexesNoOptimize messes w/ SI seems dangerous to me, because that's undocumented and unprotected - e.g. if someone extends IW and adds some logic which requires reading SI ... I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","I'm not sure how to solve it, but that seems unrelated to that issue (probably much more complicated to solve).","shaie","NULL","1","issue","1","0","0","0","0"
"2389","2389","51035","4979","Ok I understand. About the name, revisitPolicy is not exactly accurate (I think?) because it also deletes the pending files (and not just revisit the policy). Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do


if (commits.size() > 0) {
  policy.onCommit(commits);
  deleteCommits();
}


What do you think?","Ok I understand.","shaie","NULL","0",NULL,"0","0","0","0","0"
"2390","2390","51035","4979","Ok I understand. About the name, revisitPolicy is not exactly accurate (I think?) because it also deletes the pending files (and not just revisit the policy). Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do


if (commits.size() > 0) {
  policy.onCommit(commits);
  deleteCommits();
}


What do you think?","About the name, revisitPolicy is not exactly accurate (I think?)","shaie","NULL","1","con","0","0","0","1","0"
"2391","2391","51035","4979","Ok I understand. About the name, revisitPolicy is not exactly accurate (I think?) because it also deletes the pending files (and not just revisit the policy). Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do


if (commits.size() > 0) {
  policy.onCommit(commits);
  deleteCommits();
}


What do you think?","because it also deletes the pending files (and not just revisit the policy).","shaie","NULL","1","con","0","0","0","1","0"
"2392","2392","51035","4979","Ok I understand. About the name, revisitPolicy is not exactly accurate (I think?) because it also deletes the pending files (and not just revisit the policy). Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do


if (commits.size() > 0) {
  policy.onCommit(commits);
  deleteCommits();
}


What do you think?","Unless IW.deleteUnusedFiles will invoke both deletePendingFiles and revisitPolicy ... the latter will just do


if (commits.size() > 0) {
  policy.onCommit(commits);
  deleteCommits();
}


What do you think?","shaie","NULL","1","alternative","0","1","0","0","0"
"2393","2393","51036","4979","+1, I think that's a good approach.","+1, I think that's a good approach.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2394","2394","51037","4979","Adds revisitPolicy to IFD (package-private) and also calls it from IW.deleteUnusedFiles. All tests pass","Adds revisitPolicy to IFD (package-private) and also calls it from IW.deleteUnusedFiles.","shaie","NULL","1","alternative","0","1","0","0","0"
"2395","2395","51037","4979","Adds revisitPolicy to IFD (package-private) and also calls it from IW.deleteUnusedFiles. All tests pass","All tests pass","shaie","NULL","1","pro","0","0","1","0","0"
"2396","2396","51039","4979","ok I'll remove them before commit. Will commit this later - giving chance for more people to review.","ok I'll remove them before commit.","shaie","NULL","1","decision","0","0","0","0","1"
"2397","2397","51039","4979","ok I'll remove them before commit. Will commit this later - giving chance for more people to review.","Will commit this later - giving chance for more people to review.","shaie","NULL","1","decision","0","0","0","0","1"
"2398","2398","51040","4979","Committed revision 936605.","Committed revision 936605.","shaie","NULL","1","decision","0","0","0","0","1"
"2399","2399","51041","4979","Backport to 3.1","Backport to 3.1","shaie","NULL","1","decision","0","0","0","0","1"
"2400","2400","51042","4979","Committed revision 941417.","Committed revision 941417.","shaie","NULL","1","decision","0","0","0","0","1"
"2401","2401","51043","4979","Bulk close for 3.1","Bulk close for 3.1","gsingers","NULL","1","decision","0","0","0","0","1"
"2402","2402","51080","4983","attached is a patch that makes CollationTestBase, BaseTestLRU, and BenchmarkTestCase abstract, as a start.","attached is a patch that makes CollationTestBase, BaseTestLRU, and BenchmarkTestCase abstract, as a start.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2403","2403","51082","4983","Attached is an updated patch, with a new test for MemoryIndex.
Instead of looking for stuff on your hard drive, it creates some randomish documents using a selection of strings that will match the test queries, combined with some random unicode strings ala TestStressIndexing2.
This removes the use of lucene.common.dir here, so now moving on to the benchmark tests.","Attached is an updated patch, with a new test for MemoryIndex.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2404","2404","51082","4983","Attached is an updated patch, with a new test for MemoryIndex.
Instead of looking for stuff on your hard drive, it creates some randomish documents using a selection of strings that will match the test queries, combined with some random unicode strings ala TestStressIndexing2.
This removes the use of lucene.common.dir here, so now moving on to the benchmark tests.","Instead of looking for stuff on your hard drive, it creates some randomish documents using a selection of strings that will match the test queries, combined with some random unicode strings ala TestStressIndexing2.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2405","2405","51082","4983","Attached is an updated patch, with a new test for MemoryIndex.
Instead of looking for stuff on your hard drive, it creates some randomish documents using a selection of strings that will match the test queries, combined with some random unicode strings ala TestStressIndexing2.
This removes the use of lucene.common.dir here, so now moving on to the benchmark tests.","This removes the use of lucene.common.dir here, so now moving on to the benchmark tests.","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2406","2406","51083","4983","the attached patch refactors the benchmark tests:

logic to run a benchmark test is moved to BenchmarkTestCase
this forces them all to respect LuceneTestCase.TEMP_DIR for all file operations
lucene.common.dir is removed

","the attached patch refactors the benchmark tests:

logic to run a benchmark test is moved to BenchmarkTestCase
this forces them all to respect LuceneTestCase.TEMP_DIR for all file operations
lucene.common.dir is removed","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2407","2407","51084","4983","All tests pass with the latest patch, additionally I tested that the benchmark tests work from Eclipse.
If no one objects I would like to commit in a bit.","All tests pass with the latest patch, additionally I tested that the benchmark tests work from Eclipse.","rcmuir","NULL","1","pro","0","0","1","0","0"
"2408","2408","51084","4983","All tests pass with the latest patch, additionally I tested that the benchmark tests work from Eclipse.
If no one objects I would like to commit in a bit.","If no one objects I would like to commit in a bit.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2409","2409","51085","4983","This is a great cleanup Robert!  I love the copyToWorkDir...","This is a great cleanup Robert!","mikemccand","NULL","1","pro","0","0","1","0","0"
"2410","2410","51085","4983","This is a great cleanup Robert!  I love the copyToWorkDir...","I love the copyToWorkDir...","mikemccand","NULL","1","pro","0","0","1","0","0"
"2411","2411","51086","4983","Committed revision 935014.","Committed revision 935014.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2412","2412","51087","4983","Committed an additional fix: 935048, this allows you to run the contrib/ant tests from eclipse too.","Committed an additional fix: 935048, this allows you to run the contrib/ant tests from eclipse too.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2413","2413","51088","4983","backported to 3.x: revision 941669","backported to 3.x: revision 941669","rcmuir","NULL","1","decision","0","0","0","0","1"
"2414","2414","51089","4983","I am reopening (not setting as blocker since its just a test issue, but it did cause tests to fail when reviewing the release).
worst case, after the release, i think it would be good to backport the new MemoryIndexTest to the 2.9.x and 3.0.x branches.","I am reopening (not setting as blocker since its just a test issue, but it did cause tests to fail when reviewing the release).","rcmuir","NULL","1","con, decision","0","0","0","1","1"
"2415","2415","51089","4983","I am reopening (not setting as blocker since its just a test issue, but it did cause tests to fail when reviewing the release).
worst case, after the release, i think it would be good to backport the new MemoryIndexTest to the 2.9.x and 3.0.x branches.","worst case, after the release, i think it would be good to backport the new MemoryIndexTest to the 2.9.x and 3.0.x branches.","rcmuir","NULL","1","alternative, pro","0","1","1","0","0"
"2416","2416","51090","4983","Here the patch that fixes the MemoryIndexTest bug in 3.0 / 2.9","Here the patch that fixes the MemoryIndexTest bug in 3.0 / 2.9","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2417","2417","51091","4983","Backported the MemoryIndexTest fixes to 2.9.4 and 3.0.3","Backported the MemoryIndexTest fixes to 2.9.4 and 3.0.3","thetaphi","NULL","1","decision","0","0","0","0","1"
"2418","2418","51198","4994","Attached patch nulls out the Fieldable reference.","Attached patch nulls out the Fieldable reference.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2419","2419","51199","4994","As Tokenizers are reused, the analyzer holds also a reference to the last used Reader. The easy fix would be to unset the Reader in Tokenizer.close(). If this is the case for you, that may be easy to do. So Tokenizer.close() looks like this:


/** By default, closes the input Reader. */
@Override
public void close() throws IOException {
    input.close();
    input = null; // <-- new!
}

","As Tokenizers are reused, the analyzer holds also a reference to the last used Reader.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2420","2420","51199","4994","As Tokenizers are reused, the analyzer holds also a reference to the last used Reader. The easy fix would be to unset the Reader in Tokenizer.close(). If this is the case for you, that may be easy to do. So Tokenizer.close() looks like this:


/** By default, closes the input Reader. */
@Override
public void close() throws IOException {
    input.close();
    input = null; // <-- new!
}

","The easy fix would be to unset the Reader in Tokenizer.close().","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"2421","2421","51199","4994","As Tokenizers are reused, the analyzer holds also a reference to the last used Reader. The easy fix would be to unset the Reader in Tokenizer.close(). If this is the case for you, that may be easy to do. So Tokenizer.close() looks like this:


/** By default, closes the input Reader. */
@Override
public void close() throws IOException {
    input.close();
    input = null; // <-- new!
}

","If this is the case for you, that may be easy to do.","thetaphi","NULL","1","pro","0","0","1","0","0"
"2422","2422","51199","4994","As Tokenizers are reused, the analyzer holds also a reference to the last used Reader. The easy fix would be to unset the Reader in Tokenizer.close(). If this is the case for you, that may be easy to do. So Tokenizer.close() looks like this:


/** By default, closes the input Reader. */
@Override
public void close() throws IOException {
    input.close();
    input = null; // <-- new!
}

","So Tokenizer.close() looks like this:


/** By default, closes the input Reader.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2423","2423","51199","4994","As Tokenizers are reused, the analyzer holds also a reference to the last used Reader. The easy fix would be to unset the Reader in Tokenizer.close(). If this is the case for you, that may be easy to do. So Tokenizer.close() looks like this:


/** By default, closes the input Reader. */
@Override
public void close() throws IOException {
    input.close();
    input = null; // <-- new!
}

","*/
@Override
public void close() throws IOException {
    input.close();
    input = null; // <-- new!
}","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2424","2424","51200","4994","I agree, Uwe  I'll fold that into the patch.  Thanks.","I agree, Uwe  I'll fold that into the patch.","mikemccand","NULL","1","pro","0","0","1","0","0"
"2425","2425","51200","4994","I agree, Uwe  I'll fold that into the patch.  Thanks.","Thanks.","mikemccand","NULL","0",NULL,"0","0","0","0","0"
"2426","2426","51201","4994","29x version of this patch.","29x version of this patch.","mikemccand","NULL","1","alternative","0","1","0","0","0"
"2427","2427","51202","4994","Is there a chance that this can also be applied to 3.0.2 / 3.1? It would be really helpful to get this as soon as possible in the next Lucene version.","Is there a chance that this can also be applied to 3.0.2 / 3.1?","kimchy","NULL","1","alternative","0","1","0","0","0"
"2428","2428","51202","4994","Is there a chance that this can also be applied to 3.0.2 / 3.1? It would be really helpful to get this as soon as possible in the next Lucene version.","It would be really helpful to get this as soon as possible in the next Lucene version.","kimchy","NULL","1","pro","0","0","1","0","0"
"2429","2429","51203","4994","OK I'll backport.","OK I'll backport.","mikemccand","NULL","1","decision","0","0","0","0","1"
"2430","2430","51204","4994","Thanks!","Thanks!","kimchy","NULL","0",NULL,"0","0","0","0","0"
"2431","2431","53711","5167","patch with mod to wordlistloader, test, and snowball stoplists for danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, russian, spanish, and swedish","patch with mod to wordlistloader, test, and snowball stoplists for danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, russian, spanish, and swedish","rcmuir","NULL","1","alternative","0","1","0","0","0"
"2432","2432","53712","5167","I will commit this in a few days if no one objects. Again i add the getSnowballWordSet to WordListLoader, but if this is inappropriate we could instead have a SnowballWordListLoader in our snowball package or something, doesn't matter to me.","I will commit this in a few days if no one objects.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2433","2433","53712","5167","I will commit this in a few days if no one objects. Again i add the getSnowballWordSet to WordListLoader, but if this is inappropriate we could instead have a SnowballWordListLoader in our snowball package or something, doesn't matter to me.","Again i add the getSnowballWordSet to WordListLoader, but if this is inappropriate we could instead have a SnowballWordListLoader in our snowball package or something, doesn't matter to me.","rcmuir","NULL","1","alternative, con","0","1","0","1","0"
"2434","2434","53713","5167","Robert, patch looks good except of one thing. 


  public static HashSet<String> getSnowballWordSet(Reader reader)


it returns a hashset but should really return a Set<String>. We plan to change all return types to the interface instead of the implementation.","Robert, patch looks good except of one thing.","simonw","NULL","1","pro, con","0","0","1","1","0"
"2435","2435","53713","5167","Robert, patch looks good except of one thing. 


  public static HashSet<String> getSnowballWordSet(Reader reader)


it returns a hashset but should really return a Set<String>. We plan to change all return types to the interface instead of the implementation.","public static HashSet<String> getSnowballWordSet(Reader reader)


it returns a hashset but should really return a Set<String>.","simonw","NULL","1","con","0","0","0","1","0"
"2436","2436","53713","5167","Robert, patch looks good except of one thing. 


  public static HashSet<String> getSnowballWordSet(Reader reader)


it returns a hashset but should really return a Set<String>. We plan to change all return types to the interface instead of the implementation.","We plan to change all return types to the interface instead of the implementation.","simonw","NULL","1","alternative","0","1","0","0","0"
"2437","2437","53714","5167","thanks Simon, I agree","thanks Simon, I agree","rcmuir","NULL","1","pro","0","0","1","0","0"
"2438","2438","53715","5167","Committed revision 899955.","Committed revision 899955.","rcmuir","NULL","1","decision","0","0","0","0","1"
"2439","2439","53716","5167","Hi Robert,
when i changed the backwards tests i added a new param to svn exec task. With this patch it now behaves equal to bw checkouts:

If you have no svn.exe available, it will ignore the checkout. If this is so, the test should pass, so I added an exit condition, if the data dir is not available.
If you have a svn.exe available, but the checkout fails, there is an network error or something else. The data dir now exists but the build should stop in this case.

","Hi Robert,
when i changed the backwards tests i added a new param to svn exec task.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2440","2440","53716","5167","Hi Robert,
when i changed the backwards tests i added a new param to svn exec task. With this patch it now behaves equal to bw checkouts:

If you have no svn.exe available, it will ignore the checkout. If this is so, the test should pass, so I added an exit condition, if the data dir is not available.
If you have a svn.exe available, but the checkout fails, there is an network error or something else. The data dir now exists but the build should stop in this case.

","With this patch it now behaves equal to bw checkouts:

If you have no svn.exe available, it will ignore the checkout.","thetaphi","NULL","1","alternative, pro","0","1","1","0","0"
"2441","2441","53716","5167","Hi Robert,
when i changed the backwards tests i added a new param to svn exec task. With this patch it now behaves equal to bw checkouts:

If you have no svn.exe available, it will ignore the checkout. If this is so, the test should pass, so I added an exit condition, if the data dir is not available.
If you have a svn.exe available, but the checkout fails, there is an network error or something else. The data dir now exists but the build should stop in this case.

","If this is so, the test should pass, so I added an exit condition, if the data dir is not available.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2442","2442","53716","5167","Hi Robert,
when i changed the backwards tests i added a new param to svn exec task. With this patch it now behaves equal to bw checkouts:

If you have no svn.exe available, it will ignore the checkout. If this is so, the test should pass, so I added an exit condition, if the data dir is not available.
If you have a svn.exe available, but the checkout fails, there is an network error or something else. The data dir now exists but the build should stop in this case.

","If you have a svn.exe available, but the checkout fails, there is an network error or something else.","thetaphi","NULL","1","alternative, con","0","1","0","1","0"
"2443","2443","53716","5167","Hi Robert,
when i changed the backwards tests i added a new param to svn exec task. With this patch it now behaves equal to bw checkouts:

If you have no svn.exe available, it will ignore the checkout. If this is so, the test should pass, so I added an exit condition, if the data dir is not available.
If you have a svn.exe available, but the checkout fails, there is an network error or something else. The data dir now exists but the build should stop in this case.

","The data dir now exists but the build should stop in this case.","thetaphi","NULL","1","alternative","0","1","0","0","0"
"2444","2444","53717","5167","Sorry some whitespace issues. Fixed here.","Sorry some whitespace issues.","thetaphi","NULL","0",NULL,"0","0","0","0","0"
"2445","2445","53717","5167","Sorry some whitespace issues. Fixed here.","Fixed here.","thetaphi","NULL","1","decision","0","0","0","0","1"
"2446","2446","53718","5167","Committed Revision: 900160","Committed Revision: 900160","thetaphi","NULL","1","decision","0","0","0","0","1"
