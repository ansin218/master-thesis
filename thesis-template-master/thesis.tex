\documentclass[a4paper,12pt,twoside]{report}

\usepackage{acronym}
\usepackage{url}
\usepackage{cite}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\usepackage[hang,small,bf]{caption}
\usepackage{styles/tum}
\usepackage{setspace}
\usepackage[german,english]{babel}
\usepackage{float}
\usepackage{floatflt}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{booktabs}
\usepackage[pdftex,bookmarks=true,plainpages=false,pdfpagelabels=true]{hyperref}
\usepackage{mdwlist}
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{array}
\usepackage{longtable}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[capitalize, noabbrev]{cleveref}

% Path for graphics
\graphicspath{{figures/}}

\begin{document}
\setlength{\evensidemargin}{22pt}
\setlength{\oddsidemargin}{22pt}

\def\doctype{Master's Thesis}
\def\faculty{Informatik}
\def\title{Mining Rationale from Issue Tracking Systems}		%TODO add title in German
\def\titleGer{Rekonstruktion von Begr{\"u}ndungsmodellen aus Aufgabenverwaltungssystemen}	%TODO add title in German
\def\supervisor{Prof. Bernd Br√ºgge, Ph.D.}
\def\advisor{Rana Alkadhi, M.Sc.}
\def\author{Ankur Sinha}			%TODO add author name
\def\date{15.02.2018}		%TODO add submission / handover date


\hypersetup{pdfborder={0 0 0},
                        pdfauthor={<author>},
                        pdftitle={<title english>},
                        }

\lstset{showspaces=false, numbers=left, frame=single, basicstyle=\small}

\pagenumbering{alph}

\include{tex/cover}
\include{tex/titlepage}
\newpage
\thispagestyle{empty}
\mbox{}
\include{tex/disclaimer}

\newpage
\thispagestyle{empty}
\mbox{}

\chapter*{Acknowledgements}


\pagenumbering{roman}

\selectlanguage{english}
\begin{abstract}

%abstract english

Rationale knowledge is particularly important during software maintenance and evolution
because it is valuable to understand the rationale behind the previous decisions. Rationale
knowledge, in the context of software engineering, corresponds to the reasoning and logic
behind the decision taken during all the phases of software development. Such decisive con-
versations are spread out across various mediums. Rationale may be discussed face-to-face
or over other modes of communication such as chats, emails, or issue trackers to mention a
few. It is thus important to extract and mine these messages that were exchanged over dif-
ferent modes of communication to manage the rationale. Open sourced projects comes with
licenses that provides the rights to study, develop and also contribute to the same. Therefore,
this research focuses on mining rationale from open sources projects such as Apache Lucene,
Mozilla Thunderbird and Ubuntu. These rationale captured can help developers analyze
decisions, improve understandability, and document the pertinent information better.

\end{abstract}

\clearpage

\selectlanguage{german}
\begin{abstract}

%abstract german
\textit{Note: Insert the German translation of the English abstract here.}

\end{abstract}

\clearpage

\selectlanguage{english}


\tableofcontents
\clearpage

\clearpage

\begin{acronym}
\acro{GUI}{Graphical User Interface}

\end{acronym}

\pagenumbering{arabic}

\fancyhead{}
\pagestyle{fancy}
\fancyhead[LE]{\slshape \leftmark}
\fancyhead[RO]{\slshape \rightmark}
\headheight=15pt




%------- chapter 1 -------

\chapter{Introduction}

\textit{Note: Introduce the topic of your thesis, e.g. with a little historical overview.}

\section{Problem}

\textit{Note: Describe the problem that you like to address in your thesis to show the importance of your work. Focus on the negative symptoms of the currently available solution.}

\section{Motivation}

\textit{Note: Motivate scientifically why solving this problem is necessary. What kind of benefits do we have by solving the problem?}

\section{Objectives}

\textit{Note: Describe the research goals and/or research questions and how you address them by summarizing what you want to achieve in your thesis, e.g. developing a system and then evaluating it.}

\section{Outline}

\textit{Note: Describe the outline of your thesis}




%------- chapter 2 -------

\chapter{Background}

\textit{Note: Describe each proven technology / concept shortly that is important to understand your thesis. Point out why it is interesting for your thesis. Make sure to incorporate references to important literature here.}

\section{e.g. User Feedback}

\textit{Note: This section would summarize the concept User Feedback using definitions, historical overviews and pointing out the most important aspects of User Feedback.}

\section{e.g. Representational State Transfer}

\textit{Note: This section would summarize the architectural style Representational State Transfer (REST) using definitions, historical overviews and pointing out the most important aspects of the architecture.}

\section{e.g. Scrum}

\textit{Note: This section would summarize the agile method Scrum using definitions, historical overviews and pointing out the most important aspects of Scrum.}



%------- chapter 3 -------

\chapter{Related Work}

\textit{Note: Describe related work regarding your topic and emphasize your (scientific) contribution in \textbf{contrast} to existing approaches / concepts / workflows. Related work is usually current research by others and you defend yourself against the statement: ``Why is your thesis relevant? The problem was already solved by XYZ.'' If you have multiple related works, use subsections to separate them.}




%------- chapter 4 -------

\chapter{Data Collection}

This chapter describes the datasets used to carry out the research and the collection of data from the three sources: Apache Lucene, Mozilla Thunderbird and Ubuntu. All three sources mentioned use different issue trackers. Apache Lucene uses JIRA owned by Atlassian Inc., Mozilla Thunderbird uses Bugzilla developed by Mozilla Foundation and Ubuntu uses Launchpad created by Canonical Ltd. 

\section{Data Scraping and Parsing}

In this section, we explore how the data was scraped for issues trackers and commit logs from each of the three sources: Apache Lucene, Mozilla Thunderbird and Ubuntu. 

\subsection{Issue Trackers Dataset}
This section investigates how data was obtained from issue trackers of Apache Lucene, Mozilla Thunderbird and Ubuntu. 

\subsubsection{Apache Lucene}
Apache Lucene is an open sourced high performance text search engine written in Java by Doug Cutting in 1999. The project was donated to Apache Foundation in the year 2001. To develop and maintain the software, the issue tracker called JIRA developed by Atlassian is used. At the time of conducting this research, the total number of issues were 7940. However, only 7932 issues were fetched since some issues could have been deleted by the moderators, contained non-English characters or had no comments.

In JIRA, each issue gives us an option to export all the data needed for the research in XML, JSON or Word. However, JIRA also provides a link to get all these information needed for upto 1000 issues in XML. This link was altered 8 times to cover 7932 issues. The XML data from the links were saved locally as XML files. The XML ElementTree API available for Python was used to parse the XML files and extract the necessary data. 

\subsubsection{Mozilla Thunderbird}
Mozilla Thunderbird is an open sourced cross platform email client developed by Mozilla Foundation with initial release dating back to 2003. The issue tracker used by Mozilla Thunderbird is Bugzilla which was developed by Mozilla Foundation itself. The oldest issue, however, dates back to June 2007 and the number of issues fetched were 4367. Like Apache Lucene, issues with no comments were ignored. 

As mentioned earlier, JIRA gave us the option to get all the data needed for upto 1000 issues on each request. However, Bugzilla did not have any option that was similar. In the case of Bugzilla, data for each issue had to be accessed individually. Another drawback of Bugzilla when compared to JIRA was that the data when exported did not have the comments for the relevant issues. Hence, all the necessary data, including the comments, for the issues in Bugzilla were scraped using BeautifulSoup, a library for Python to scrape data from the web. 

\subsubsection{Ubuntu}
Ubuntu is an open sourced operating system developed by Canonical Ltd. in the year 2004. The same team is also responsible for having their own issue tracking system known as Launchpad which also houses the issues of Ubuntu. The number of issues fetched since the beginning of the project are 28,429. Issues with no comments were not scraped, just like in the case of Apache Lucene and Mozilla Thunderbird. Additionally, a major problem faced while scraping data from Launchpad was that there were abundant number of downlinks. When these links were accessed manually, they kept loading until getting timed out and returned 404 error while scraping.

Launchpad did not have its export option to fetch mass number of issues and its comments. Hence, each and every issue had to be scraped one at a time. In addition, not all the issues had the same HTML tags and attributes for the comments section. Launchpad provides individual link for each of the comment of the pertinent issue and hence, the comments were scraped by obtaining the individual links for comments. All the links were requested and the necessary data was retrieved using BeautifulSoup, the library mentioned in the previous section.

\subsection{Commit Logs Dataset}
This section scrutinizes how the data of commit messages were obtained for Apache Lucene, Mozilla Thunderbird and Ubuntu.

\subsubsection{Apache Lucene}
Apache Lucene uses Git for its version control, which was developed by Linus Torvald and released in 2005. Apache Lucene along with SOLR is hosted together within the same repository on GitHub. The repository was cloned to local environment in the system to retrieve the commit logs. Git allows the logs of the repository to be exported in TXT format and this is the method that has been used to get all the logs for Apache Lucene. However, a parsing script was additionally written in Python to obtain all the commit id, author, date and commit message respectively. These information was later stored in a MySQL database. 

\subsubsection{Mozilla Thunderbird}
Mozilla Thunderbird uses Mercurial for its version control, which was developed by Matt Mackall and released in 2005. The Mozilla Thunderbird project in a repository owned by Mozilla. This repository was cloned to the system to extract the commit logs. Like Git, Mercurial also offers users an option to extract the entire commit log history in TXT format. This data was parsed using a script written in Python to retrieve all the commit id, author, date and commit message respectively. The retrieved information was eventually stored in a MySQL database. 

\subsubsection{Ubuntu}
Ubuntu makes use of Bazaar primarily for its version control, which was developed by Martin Pool in 2005 and maintained by Canonical Ltd. The Ubuntu project is hosted on Launchpad platform using Bazaar. Due to too numerous repositories, the entire Ubuntu project could not be cloned individually. However, a script in Python was written using the BeautifulSoup library; similar to the script used for fetching data from issue trackers for Ubuntu. Identical problems of many downlinks were encountered and hence, only those that were up were fetched automatically. A MySQL database was used to store all of these information. 

\section{IRC Messages Dataset}

This section explores how data was obtained from IRC messages for Apache Lucene, Mozilla Thunderbird and Ubuntu. 

%------- chapter 5 -------

\chapter{Manual Content Analysis}

This chapter dives into the procedure of manual content analysis and the formation of ground truth for machine learning experiments to be carried out later. In addition, this chapter also gives insights on how the sample data for the experiements to be performed were obtained from the initial raw dataset. 

\section{Ground Truth Dataset}
An imperative step after performing machine learning experiements is to measure characteristics such as accuracy, precision, recall and F1 score. This, in turn, means one needs to have a reliable dataset for ground truth. Messages from the issue trackers have no pre-defined structure as to whether they contain any kind of rationale or not. To have similar understanding, be on the same page and reduce the number of disagreements later, both the annotators randomly went through a few sentences. The annotators marked rationale for each of them if applicable, classified them appropriately, and shared their perspectives before concluding on mutual agreements as to how to go about in the main manual content analysis experiment.  Some of the examples are mentioned in detail in the coding guide for manual content analysis. There were two kinds of classification performed in this stage: binary and multi-class classification. Binary classification was done to check if the sentence contains any kind of rationale or not. Multi-class classification was done to classify the sentence, if rationale is found, into five categories: issue, alternatives, pro-arguments, con-arguments and decision, which are explained as follows.

\begin{itemize}
\item \textbf{Issue:} the problem to be solved, or a feature to be implemented
\item \textbf{Alternative:} the feasible solutions that could address the issue
\item \textbf{Pro-Argument:} the argument put across in favor of a particular alternative
\item \textbf{Con-Argument:} the argument put across against a particular alternative
\item \textbf{Decision:} the final conclusions made to resolve the issue
\end{itemize}

\section{Random Stratified Sampling}
For the main content analysis, two annotators individually read through XXXX number of individual sentences from XXXX comments spanning 100 issues from each of the three projects. Although the number of issues fetched for each of them were much higher, only 100 of them were attained for each project using random stratified sampling due to time constraints of the research. Random stratified sampling can be of two types: proportionate random stratified sampling and disproportionate random stratified sampling. In proportionate random stratified sampling, the parent dataset is divided into multiple strata and the child dataset sampled randomly must have the strata in the same proportion as the parent dataset. Contrary to proportionate random stratified sampling, the case of disproportionate random stratified sampling has the parent dataset divided into multiple strate but the child dataset sampled randomly may not have its strata in the same proportion as the parent dataset. There are, however, three steps carried out sequentially to obtain the sample which are as follows.

\begin{itemize}
\item {Step 1:} Filter the dataset by discarding issues that do not fall under the required date range. This is to make sure the comments from issue trackers are from the same duration as messages from IRC. 
\item {Step 2:} In the next step, count the number of comments for each issue that are under the required date range but only consider issues that have between 5 and 30 comments, and divide them into 5 groups: 6-10 comments, 11-15 comments, 16-20 comments, 21-25 comments and 26-30 comments. We wish to avoid issues with too few comments (less than 5) or way too many comments (greater than 30). 
\item {Step 3:} Perform proportionate random stratified sampling to obtain 100 random issues from the filtered dataset in step 2 for each of the three projects.
\end{itemize}


%------- chapter 6 -------

\chapter{Natural Language Processing}

\textit{Note: If you did an evaluation / case study, describe it here.}

\section{Topic Modeling}

\textit{Note: Describe the design / methodology of the evaluation and why you did it like that. E.g. what kind of evaluation have you done (e.g. questionnaire, personal interviews, simulation, quantitative analysis of metrics, what kind of participants, what kind of questions, what was the procedure?}

\section{Parts of Speech Tagging}

\textit{Note: Derive concrete objectives / hypotheses for this evaluation from the general ones in the introduction.}

%------- chapter 7 -------

\chapter{Summary}

\textit{Note: This chapter includes the status of your thesis, a conclusion and an outlook about future work.}

\section{Status}

\textit{Note: Describe honestly the achieved goals (e.g. the well implemented and tested use cases) and the open goals here. if you only have achieved goals, you did something wrong in your analysis.}

\subsection{Realized Goals}

\textit{Note: Summarize the achieved goals by repeating the realized requirements or use cases stating how you realized them.}

\subsection{Open Goals}

\textit{Note: Summarize the open goals by repeating the open requirements or use cases and explaining why you were not able to achieve them. \textbf{Important:} It might be suspicious, if you do not have open goals. This usually indicates that you did not thoroughly analyze your problems.}

\section{Conclusion}

\textit{Note: Recap shortly which problem you solved in your thesis and discuss your \textbf{contributions} here.}

\section{Future Work}

\textit{Note: Tell us the next steps  (that you would do if you have more time. be creative, visionary and open-minded here.}



\appendix

\chapter{e.g. Questionnaire}

\textit{Note: If you have large models, additional evaluation data like questionnaires or non summarized results, put them into the appendix.}


\clearpage

\listoffigures
\clearpage

\listoftables
\clearpage

\bibliography{thesis}
\bibliographystyle{alpha}

\end{document}
