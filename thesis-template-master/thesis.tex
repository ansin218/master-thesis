\documentclass[a4paper,12pt,twoside]{report}

\usepackage{acronym}
\usepackage{url}
\usepackage{cite}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\usepackage[hang,small,bf]{caption}
\usepackage{styles/tum}
\usepackage{setspace}
\usepackage[german,english]{babel}
\usepackage{float}
\usepackage{floatflt}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{booktabs}
\usepackage[pdftex,bookmarks=true,plainpages=false,pdfpagelabels=true]{hyperref}
\usepackage{mdwlist}
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{array}
\usepackage{longtable}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[capitalize, noabbrev]{cleveref}

% Path for graphics
\graphicspath{{figures/}}

\begin{document}
\setlength{\evensidemargin}{22pt}
\setlength{\oddsidemargin}{22pt}

\def\doctype{Master's Thesis}
\def\faculty{Informatik}
\def\title{Mining Rationale from Issue Tracking Systems}		%TODO add title in German
\def\titleGer{Rekonstruktion von Begr{\"u}ndungsmodellen aus Aufgabenverwaltungssystemen}	%TODO add title in German
\def\supervisor{Prof. Bernd Brügge, Ph.D.}
\def\advisor{Rana Alkadhi, M.Sc.}
\def\author{Ankur Sinha}			%TODO add author name
\def\date{15.02.2018}		%TODO add submission / handover date


\hypersetup{pdfborder={0 0 0},
                        pdfauthor={<author>},
                        pdftitle={<title english>},
                        }

\lstset{showspaces=false, numbers=left, frame=single, basicstyle=\small}

\pagenumbering{alph}

\include{tex/cover}
\include{tex/titlepage}
\newpage
\thispagestyle{empty}
\mbox{}
\include{tex/disclaimer}

\newpage
\thispagestyle{empty}
\mbox{}

\chapter*{Acknowledgements}
To write.

\pagenumbering{roman}

\selectlanguage{english}
\begin{abstract}

%abstract english

Rationale knowledge is particularly important during software maintenance and evolution
because it is valuable to understand the rationale behind the previous decisions. Rationale
knowledge, in the context of software engineering, corresponds to the reasoning and logic
behind the decision taken during all the phases of software development. Such decisive con-
versations are spread out across various mediums. Rationale may be discussed face-to-face
or over other modes of communication such as chats, emails, or issue trackers to mention a
few. It is thus important to extract and mine these messages that were exchanged over dif-
ferent modes of communication to manage the rationale. Open sourced projects comes with
licenses that provides the rights to study, develop and also contribute to the same. Therefore,
this research focuses on mining rationale from open sources projects such as Apache Lucene,
Mozilla Thunderbird and Ubuntu. These rationale captured can help developers analyze
decisions, improve understandability, and document the pertinent information better.

\end{abstract}

\clearpage

\selectlanguage{german}
\begin{abstract}

%abstract german
Rationales Wissen ist besonders wichtig während der Softwarewartung und evolution, da es wertvoll ist, die Gründe hinter den vorherigen Entscheidungen zu verstehen. Rationales Wissen, im Kontext von Software-Engineering, entspricht der Argumentation und Logik hinter der Entscheidung in allen Phasen der Softwareentwicklung. Solche entscheidenden Gespräche sind auf verschiedene Medien verteilt. Begründungen können von Angesicht zu Angesicht oder über andere Arten der Kommunikation wie Chats, E-Mails oder Problem-Tracker diskutiert werden, um nur einige zu nennen. Es ist daher wichtig, diese Nachrichten, die über verschiedene Kommunikationsarten ausgetauscht wurden, zu extrahieren und abzubauen, um die Logik zu verwalten. Open-Source-Projekte werden mit Lizenzen geliefert, die das Recht zum Studium, zur Entwicklung und zum Beitrag dazu bieten. Daher konzentriert sich diese Forschung auf Bergbauprinzipien aus Open-Source-Projekten wie Apache Lucene, Mozilla Thunderbird und Ubuntu. Diese erfasste Begründung kann Entwicklern helfen, Entscheidungen zu analysieren, die Verständlichkeit zu verbessern und die relevanten Informationen besser zu dokumentieren.

\end{abstract}

\clearpage

\selectlanguage{english}


\tableofcontents
\clearpage

\clearpage

\begin{acronym}

\acro{MNB}{Multinomial Naive Bayes}
\acro{DT}{Decision Tree}
\acro{RF}{Random Forest}
\acro{LR}{Logistic Regression}
\acro{SVM}{Support Vector Machines}
\acro{HTML}{Hypertext Markup Language}
\acro{TXT}{Text}
\acro{SQL}{Structured Query Language}
\acro{NLP}{Natural Language Processing}
\acro{POS}{Parts-of-Speech}
\acro{SOLR}{Need to look up}
\acro{SGD}{Stochastic Gradient Descent}
\acro{GATE}{Need to look up}
\acro{WEKA}{Need to look up}
\acro{MEKA}{Need to look up}
\acro{NER}{Named Entity Recognition}

\end{acronym}

\pagenumbering{arabic}

\fancyhead{}
\pagestyle{fancy}
\fancyhead[LE]{\slshape \leftmark}
\fancyhead[RO]{\slshape \rightmark}
\headheight=15pt




%------- chapter 1 -------

\chapter{Introduction}

\textit{Note: Introduce the topic of your thesis, e.g. with a little historical overview.}

\section{Problem}

\textit{Note: Describe the problem that you like to address in your thesis to show the importance of your work. Focus on the negative symptoms of the currently available solution.}

\section{Objectives}

\textit{Note: Describe the research goals and/or research questions and how you address them by summarizing what you want to achieve in your thesis, e.g. developing a system and then evaluating it.}

\section{Methodology}

\textit{Note: Motivate scientifically why solving this problem is necessary. What kind of benefits do we have by solving the problem?}

\section{Outline}

\textit{Note: Describe the outline of your thesis}




%------- chapter 2 -------
\chapter{Foundation}

\section{Rationale Management}

During different stages such as planning, designing, developing, testing and maintenance, individuals responsible come together to tackle any particular issue. Such a discussion includes the numerous ways in which the problem can be solved. A problem, however, cannot be solved just by proposing a possible solution. Different team members come together to show their support or go against it by pointing the demerits of a suggested implementation. Finally, they come together towards a mutual conclusion and decide to go ahead with a particular suggestion. Rationale can thus be defined as the logical reasoning behind a particular act or statement. In the context of software engineering, rationale is the reasoning behind the decision taken with respect to some task in hand during various phases of software development. 

Conversations between developers can take place in person or across digital medium. Digital medium may include emails, chat messengers and issue trackers to name a few. While discussions taking place in person cannot be captured, technological advancements have made it possible to extract rationale from the aforementioned digital medium. Capturing these vital information from different artifcats and managing these rationale can therefore be of great benefit for other developers. This helps them in a way such that one does not have to go through the entire conversation history which would include a lot of irrelevant, petty, or off-topic comments; concentrating and focusing only on the pertinent messages can be made possible through rationale management. 

The problems or issues to be tackled, more often than not, do not have path towards a solution or a concrete end point. Such problems are popularly described as wicked problems. With respect to computer science, wicked problems do not have any concrete algorithm or predefined point to solve a problem. The term wicked problem was introduced first by Professor Horst Rittel and Webber from the Architecture Department at the University of California, Berkely. He described such problems as those which are not formulated properly. In addition, he stated that such problems are solved by multiple decision makers. This is very synonymous to the fact that, in software engineering, such a situation has become a commonplace. C. West Churchman of the same university mentioned in a couple of sentences earlier had written a guest editorial echoing similar views. In the year 2014, Marcelo, George and Tero, proposed the Massive Open Online Research, which is claimed to be a framework to deal with wicked problems. 

Taking a leaf out of Bruegge and Dutoit, five types of rationale elements are taken into consideration for this research. They are: issues, alternatives, pro-arguments, con-arguments and decision.
 
\subsubsection{Issues}
Issues are tasks that need to be solved. They can be a new feature, a major or minor bug, or a problem that could have stemmed out of an existing implementation. Issues can also be termed as wicked problems since they may neither have any predefined path to reach the goal nor a proper end point to the goal itself. Some examples of issues from the data obtained are, "How do I go about limiting the search window as well as the number of matches?" and "It does seem odd that all failures we've seen have been for very complex polygons".  

\subsubsection{Alternatives}
Alternatives are the possible solutions to solve the issue in hand. Developers come up with different strategies to solve the issue. However, there are cases when an individual or a group of indviduals may not come up with a concrete solution. In such a scenario, temporary fixes are suggested by members of the group. Statements like, "I made a change just to see what would happen and x-sender started changing" and "I found the way to remove a news server account" are some examples of alternatives. 

\subsubsection{Pro-arguments}
Pro-arguments are made to lend support to a particular alternative or decision. Additionally, they may also state extra advantages and merits as to why a particular approach must be taken. From the dataset, examples such as, "It is well possible that that will work" and "From how I read the code, it does reject them, so it should work", for pro-arguments were encountered. 

\subsubsection{Con-arguments}
Con-arguments are the opposite of pro-arguments. They refute claims and also state the disadvantages or demerits of choosing a particular solution. Examples of con-arguments from the dataset includes, "One principle problem is that it's duplicated functionality for no good reason" and "This is inconsistent and unneccesarily difficult to use".

\subsubsection{Decisions}
Decisions are final conclusions made by the team members to solve an issue. While decisions can be made by alternatives suggested, they can also be made directly. Alternatively, decisions can also be made by without discussing or dismissing an issue. "This bug was fixed in the package snapcraft - 2.8.8+16.10" and "Though Banshee won't be in 12.04 anymore, this is still a valid papercut" are a couple of examples of decision from the dataset used in this thesis.


\section{Related Work}

In this section, we look at all the works relevant to this thesis done by others. 

\subsection{Mining Sentiments from Development Artifacts}
In the past, research has been carried out to retrieve different kinds of elements from issue tracking systems. Marco et al. published their work on extracting emotions and sentiments of developers from JIRA issue trackers of four different open source projects.  Sentiments such as love, joy, surprise, anger, sadness and fear were mined from 2,000 issue comments spanning 4,000 sentences from the JIRA issue tracking system of open source communities namely, Apache, Spring, CodeHaus and JBoss. This was a smaller version of a project where Marco et. al. carried out similar research with a dataset that spanned 700,000 reports of issues and 2 million comments. In another work of Marco et al., it has been concluded that being polite helped in fixing the issue in lesser amount of time. 

Marshall et al. demonstrated the outcomes of emotions from the forum posts of agile teams. Their dataset contained more than 1,300 forum posts from five teams. The posts from the forums were classified into unemotional, positive, negative and neutral. It was discovered that members of the team showing less emotions performed better than others. In addition, such individuals were evaluated more positively by their cohorts. 

A study was conducted by Souza and Silva where they performed sentiment analysis on Travis Continuous Integration builds. They carried out empirical assessment of 1,262 projects hosted on GitHub having more than 609,000 builds using Travis CI. It was found out that negative sentiments from developers leads to build breakage in continuous integration processes, although the influence is small. 

Sentiments about software development can also be mined from social media. One such research was conducted by Alkadhi et al. where they extracted more than 10 million tweets of 30 different mobile and desktop software applications from Twitter. Their study demonstrated the type of content present in tweets with respect to the concerned software application. The tweets were categorized into 25 groups; some of which are praises, complaints, service, shortcomings and others, from general public. In addition to manual content analysis, supervised machine learning techniques were used to classify the tweets into different categories. 

\subsection{Automatic Classification of Rationale Elements}
Like mining and classifying sentiments, rationale elements were also extracted in some works. Kurtanovic et al. conducted research similar to that of Alkadhi et al. mentioned in the previous section. They performed manual content analysis and automatic classification of rationale from more than 52,000 user reviews of 52 software applications from the Amazon Store. Reviews and its pertinent sentences were classified using SVM, MNB and LR into the following: issue, alternative, criteria, decision and justification.  

Fan et al. performed a study on a dataset obtained from GitHub spanning 80 projects and more than 252,000 issue reports. The goal was to classify them into bug or non-bug related sentences by running algorithms such as SVM, LR, MNB and RF using a two-fold approach. The first step included the computing probability to find out whether a sentence was bug-prone related or not. This was later used as an input in the second step coupled with other factors such as characteristics of the person who made the comment, their contributions, and others, to finally classify the sentence as bug-prone or not. 

Rogers et al. carried out manual and automatic classification of rationale on 200 Chrome bug reports using GATE and WEKA. While the binary classification was conducted to see if a sentence contained rationale or not, fine-grained classification was also performed to check if a sentence had the following kinds of rationale: requirements, decisions, alternatives, arguments, assumptions, questions, answers, and procedures. The automatic extraction of rationale elements was carried out using machine learning techniques such as SGD, RF and MNB. 

Synonymous to the classification mentioned earlier, Alkadhi et al. performed classification of rationale elements where the dataset included more than 8,700 chat messages from 3 software development teams. Apart from the binary classification of extracting if there was any rationale in a sentence or not, fine-grained classification was also performed to extract issues, alternatives, pro-arguments, con-arguments and decisions using MNB and SVM machine learning algorithms. 

Bachelli et al. used a dataset of around 1,500 emails from mailing lists. They then tried to classify the contents of the emails into 5 categories. The categories into which they were classified automatically were natural language, junk, patch, stack trace and source code, using Naive Bayes classifier. 

\subsection{Generating Categories using Topic Modeling}
In the past, research has been carried out to generate categories using Topic Modeling techniques such as LDA. 

\section{Natural Language Processing}

Natural Language Processing (NLP) is a popular branch of computer science using which one can perform text classification or categorization. In addition, some of the methodologies under NLP can also be used for data preprocessing while performing machine learning tasks. Some of the most popular methods and techniques from NLP and their pertinent works are discussed below. 

\subsection{Preprocessing Steps}
The following section explores different preprocessing steps that can be applied for text classification. 

\subsubsection{Stopwords Removal}
A sentence usually is comprised of different parts-of-speech. However, there may be some words that are very common across all sentences and thereby does not contribute to learning about specific features or keywords. In such a scenario, researchers deploy stopword removal mechanism to filter out words that are of less or no interest to them. An exhaustive list of stopwords has been published by the Information Retrieval Group, School of Computing, University of Glasgow. In addition, one can also create their own stopwords list if specific information or keywords are being looked for. 

Given a sentence, 
Munich is a really beautiful city with a cosmopolitan crowd. 

After applying stopwords removal, the sentence would then look like,
Munich really beautiful city cosmopolitan crowd.

Rogers et al. carried out an experiment to identify rationale from documents using linguistic features such as stopwords removal to conclude that it resulted in better precision but lower recall. Kurtanovic et al. also performed stopwords removal in their data preprocessing step while retrieving rationale from software reviews. Guzman et al. used stopwords removal in all of her works which included obtaining information from tweets as well as chat messages.

\subsubsection{Punctuations Removal}
Similar to words, punctuations are different parts of speech that can be removed from text after which the filtered data can be fed to the classifier. Some commonly witnessed punctuations include fullstop (.), comma (,), colon (:), semi-colon (;) amongst others. 

Given a sentence, 
Munich has many kinds of people living here such as: Indians, Chinese, Americans. 

After applying punctuation removal, the sentence would then look like,
Munich has many kinds of people living here such as Indians Chinese Americans

Bacchelli et al. got rid of punctuations in the data preprocessing step for classifying contents of emails. Like stopwords removal, Kurtanovic et al. also performed punctuation removal as well for retrieving rationale from software reviews. Guzman et al. used removed punctuations from the dataset to carry out sentiment analysis on app reviews. 

\subsubsection{Stemming}
Stemming is a popular preprocessing technique of mapping similar words with different variations to one single word by truncation. One of the most popular stemmers out there in the field of research is the Porter Stemmer. IR Book Stanford NLP states that stemming increases recall but harms precision. An example of stemming would be that words like operational, operative, operations would all get mapped to operate. However, there are cases were words also get mapped to a single word which may not have an exact dictionary meaning. An example of such a case can be calculation, calculative getting mapped to calculat instead of calculate. Stemming has been used as a preprocessing step in the following works. 

\subsubsection{Lemmatization}
Another preprocessing technique, but rather a better replacement of stemming, is lemmatization. In this technique, words of different variations are not truncated but rather are mapped to a root word. This root word is identical to a word found in the dictionary. Toman et al. state that lemmatization is a difficult and expensive process but is more beneficial. An example of lemmatization would be that words like is and are would be mapped to be, which is an actual word found in the dictionary. Some other works that were mentioned earlier also have lemmatization done for preprocessing the data. 

\subsubsection{Bag-of-Words}
Given a corpus of N documents, each document may contain many words. A bag of words can thus be formed by counting the frequency of every unique word occuring across N documents. However, to form a vocabulary pertaining to the text dataset, techniques such as stopwords removal, punctuations removal, stemming and lemmatization, can be applied to prune on commonly occuring words and different variations of the same word.

Sentence 1: I went to Munich

Sentence 2: Munich is a very beautiful place compared to Berlin

Sentence 3: Berlin has more startups than Munich

Applying the preprocessing techniques such as stopwords removal, punctuations removal (Although no punctuations occur in the three given sentences), stemming and lemmatization, the bag-of-words would look like:

Munich: 3
Berlin: 2
beautiful: 1
compared: 1
startups: 1
place: 1

NOTE: To write about previous works. 

\subsubsection{N-Gram Tokenization}
Bag-of-words takes one word at a time and counts them. However, n-gram tokenization is a method were multiple words appearing continuously can be grouped as one term. This approach helps in extracting better meaning if multiple words occuring together make better sense than their individual counterparts. These terms of n-grams or n-tokens groupped together can then be used as a feature while running the classifier. 

An example sentence without any kind of preprocessing applied: Linux is a robust operating system

Result of 1-gram (unigram) tokenization: Linux, is, a, robust, operating, system 

Result of 2-gram (bigram) tokenization: Linux is, is a, a robust, robust operating, operating system

Result of 3-gram (trigram) tokenization: Linux is a, is a robust, a robust operating, robust operating system

It can be seen clearly that the results of 2-gram or 3-gram tokenization add more value in terms of using it as features when compared to 1-gram tokenization.

NOTE: To write about previous works. 

\subsubsection{Term Frequency - Inverse Document Frequency}
abc

\subsection{Topic Modeling}
Topic modeling is an approach in the domain of NLP to extract a specific number of topics from a large text corpus. The topics are retrieved by computing the probabilities of frequently occurring words that constitute the text corpus. Preprocessing techniques mentioned earlier can be applied before generating all the topics depending on the insights being looked for to make the final results more meaningful. It is to be noted that the name of the topics are not given by the algorithm but the end users themselves; the algorithm will only give the frequently occurred words for every given topic.

Mathematically, topic modeling is a kind of a Bayesian inference model where every document in the text corpus is associated with a probability distribution over topics and these topics are nothing but probability distribution over words. Many topic modeling algorithms exists such as LSA, p-LSA, LDA, and HDP to mention a few. This thesis performs topic modeling using LDA as it is widely used in the academia domain for working with text corpora. NOTE: Mention some works using which rationale data was worked with.

\subsection{Parts-of-Speech Tagging}
abc

\subsection{Named Entity Recognition}
abc


\section{Machine Learning}

\textit{Note: This section would summarize the agile method Scrum using definitions, historical overviews and pointing out the most important aspects of Scrum.}

\subsection{Techniques}

\subsubsection{Supervised Learning}
Supervised learning is a technique where the algorithm learns from a given set of examples. Given a dataset with many entries, the algorithm gets hold of both, all the inputs and the pertinent output. This helps the classifier to come up with patterns to map the given set of inputs to the output. Supervised learning, thus, can classify if a sentence contains rationale or not based on certain inputs and its features. This technique would however require sufficient amount of data to learn or train and hence improve the performance of the classifier. Some of the popular supervised learning algorithms are LR, MNB, SVM, DT and RF, all of which have been used in this thesis.

\subsubsection{Unsupervised Learning}  
Contrary to supervised learning, unsupervised learning technique has no labeled outputs for the given set of inputs. The unsupervised learning algorithm applied has to come up with patterns and relations from the data supplied. The discovered patterns thus help in categorizing or clustering the data which can be studied by researchers later for better understanding of what constitutes the data. Clustering algorithms such as K-NN and K-Means are popular clustering algorithms belong to the class of unsupervised learning techniques. 

\subsubsection{Reinforcement Learning} 
There exists another technique, which neither completely relies on output like supervised learning nor is completely like the approach of unsupervised learning, known as reinforcement learning. In reinforcement learning, the machine or agent tries to learn the behavior of something specific from a given environment based on feedback or reward. The goal of the algorithm is to maximize the reward. These behaviors, depending on the nature of the problem, can be learnt all at once or can be adaptive in nature. NOTE: Give some references for reinforcement learning like DeepMind, Go, etc.

\subsection{Pipeline}

\subsubsection{Training} 
For an algorithm to make predictions, it must first learn and train itself using some data. A part of the overall dataset, known as training dataset, is thus used for training the algorithm by working with some parameters. It is to be noted that more the quality and quantity of the training dataset, better the training. In other words, there must be enough examples from the different classes in good proportion for the algorithm to learn.  If the dataset is highly imbalanced or skewed, balancing techniques can be applied. In the case of text classification, the training data can be a group of sentences with some preprocessing steps mentioned in Section X.Y.Z along with the concerned output. The output can be something like, whether the sentence contains rationale or not, or whether the sentence has a positive tone or not, to mention a few.

\subsubsection{Testing} 

\subsubsection{Cross-Validation} 

\subsection{Transformation Methods}

\subsubsection{Binary Relevance} 
Binary relevance is a kind of problem transformation method applied to multi-class or multi-label classification problems. Using this approach, a multi-class problem is decomposed into multiple binary classification problems. For example, if there are five classes, the problem would be decomposed into five binary classification problem where the classifier runs independently on each of the class using the input features. An example of binary relevance is given below. 

\subsubsection{Label Powerset} 
Label powerset is another form of problem transformation method applied to multi-class or multi-label classification problems. In the case of label powerset, one single multi-class classifier is used to train all unique combinations of label from the training dataset. An example of label powerset is given below.

\subsection{Algorithms}
Many algorithms exist in the domain of machine learning to perform different kinds of classification. The scope of this thesis, however, is text classification using supervised learning techniques. The algorithms that will be looked into are mentioned in this section.

\subsubsection{Logistic Regression}
Given a set of independent variables, x1, x2, .. , xn, logistic regression is an algorithm that predicts the outcome, y, that is a binary or categorical variable. It is a special form of linear regression except that probability is calculated by using the ratio of logarithm of odds. The data is fit into a logit function for the prediction of outcomes. Binary logistic model predicts the outcomes of two classes, Class A (0) and Class B (1), whereas multinomial logistic regression model is capable of predicting more than two classes, for example, Class A, Class B and Class C. 

Log of odds ratio is given as:

log(Y/(1-Y)) = a + bx1

where b is the co-efficient of x1 and a is the bias term.

Transforming the above log of odds ratio to get the value of Y, 

Y = exp(a + bx1) / 1 + exp(a + bx1) 

NOTE: Mention previous works where this was applied

\subsubsection{Multinomial Naive Bayes}
Naive Bayes belongs to the class of algorithms that are predominantly based on the Bayes Theorem. Algorithms of this group have a common notion that features that contribute to the predicition of an outcome are all independent of each other. However, in real world, this assumption contradict the hypothesis based on which these algorithms function. Naive Bayes, though, is considered to be easy, fast and efficient. 

Given two events A and B, the Bayes Theorem formula, for computing the probability of event A given that B has occured, can be defined as: 

P(A|B) = P(B|A) x P(A) / P(B)

MacCullum and Nigam published their research on a variant of Naive Bayes, popularly known as the Multinomial Naive Bayes (MNB) model. Unlike the simple Naive Bayes model, MNB uses multinomial distribution for each of the attribute features. In addition, for tasks like text classification, MNB takes the frequency of word counts, or sequences of word counts, or n-gram tokens independently into account when compared to NB model which only tries to predict the presence or absense of individual words. 

Formulate MNB here

NOTE: Mention previous works where this was applied

\subsubsection{Support Vector Machines}
Support Vector Machine (SVM) is a popular supervised machine learning algorithm that can be used for regression as well as classification. SVM has been used rigorously in the past for various text classification projects. In SVM, items from a dataset are plotted as points in an n-dimensional space where n equals the number of features in consideration to perform the classification. The value of each feature corresponds to the value of coordinate. A hyperplane is then used to segregate the classes into two. To increase the confidence of correctly classifying a data point to the right class, the margin between the hyperplane and the data point must not be close; more the distance between them better the likelihood of proper classification. 

NOTE: Add picture here! 

In real world, datasets can rarely be linearly separable. In such a scenario, kernels come to rescue when more than two dimensions are involved. To separate the classes, the low dimensional input dimensional space is therefore mapped to a higher dimensional space. The demerit of this approach, although inevitable, is that is results in longer training time while running the classifier. 

NOTE: Mention previous works where this was applied

\subsubsection{Decision Trees}
Write about Decision Trees here

\subsubsection{Random Forest}
Write about Random Forest here

\subsection{Performance Metrics}
Performance metrics of a classifier is imperative to know how accurate the model is. A confusion matrix can hence be plotted which comprises of True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN) as depicted in Figure (insert figure).

Given a positive class, CP, and a negative class, CN, the following can be defined:

True Positive Rate is the proportion of correctly classified positive class, CP

True Negative Rate is the proportion of correctly classified negative class, CN

False Positive Rate is the proportion of incorrectly classified negative class, CN, classified as CP

False Negative Rate is the proportion of incorrectly classified positive class, CP, classified as CN

The four components of the confusion matrix defined earlier can now be used to define the following subsections.

\subsubsection{Accuracy} 
Accuracy is the ratio of sum of TP and TN to the entire number of samples in the dataset

Accuracy = TP + TN / TP + TN + FP + FN

\subsubsection{Precision} 
Precision is the ratio of TP to the sum of TP and FP

Precision = TP / TP + FP

\subsubsection{Recall} 
Recall is the ratio of TP to the sum of TP and FN. Recall is also known as hit rate, True Positive Rate (TPR) or sensitivity

Recall = TP / TP +FN

\subsubsection{F1} 
F1 is the harmonic mean of recall and precision

F1 = 2 TP / 2 TP + FP + FN

\section{Tools}

%------- chapter 3 -------

\chapter{Data Collection}

This chapter describes the datasets used to carry out the research and the collection of data from the three sources: Apache Lucene, Mozilla Thunderbird and Ubuntu. All three sources mentioned use different issue trackers. Apache Lucene uses JIRA owned by Atlassian Inc., Mozilla Thunderbird uses Bugzilla developed by Mozilla Foundation and Ubuntu uses Launchpad created by Canonical Ltd. 

\section{Data Scraping and Parsing}

In this section, we explore how the data was scraped for issues trackers and commit logs from each of the three sources: Apache Lucene, Mozilla Thunderbird and Ubuntu. 

\subsection{Issue Trackers Dataset}
This section investigates how data was obtained from issue trackers of Apache Lucene, Mozilla Thunderbird and Ubuntu. 

\subsubsection{Apache Lucene}
Apache Lucene is an open sourced high performance text search engine written in Java by Doug Cutting in 1999. The project was donated to Apache Foundation in the year 2001. To develop and maintain the software, the issue tracker called JIRA developed by Atlassian is used. At the time of conducting this research, the total number of issues were 7940. However, only 7932 issues were fetched since some issues could have been deleted by the moderators, contained non-English characters or had no comments.

In JIRA, each issue gives us an option to export all the data needed for the research in XML, JSON or Word. However, JIRA also provides a link to get all these information needed for upto 1000 issues in XML. This link was altered 8 times to cover 7932 issues. The XML data from the links were saved locally as XML files. The XML ElementTree API available for Python was used to parse the XML files and extract the necessary data. 

\subsubsection{Mozilla Thunderbird}
Mozilla Thunderbird is an open sourced cross platform email client developed by Mozilla Foundation with initial release dating back to 2003. The issue tracker used by Mozilla Thunderbird is Bugzilla which was developed by Mozilla Foundation itself. The oldest issue, however, dates back to June 2007 and the number of issues fetched were 4367. Like Apache Lucene, issues with no comments were ignored. 

As mentioned earlier, JIRA gave us the option to get all the data needed for upto 1000 issues on each request. However, Bugzilla did not have any option that was similar. In the case of Bugzilla, data for each issue had to be accessed individually. Another drawback of Bugzilla when compared to JIRA was that the data when exported did not have the comments for the relevant issues. Hence, all the necessary data, including the comments, for the issues in Bugzilla were scraped using BeautifulSoup, a library for Python to scrape data from the web. 

\subsubsection{Ubuntu}
Ubuntu is an open sourced operating system developed by Canonical Ltd. in the year 2004. The same team is also responsible for having their own issue tracking system known as Launchpad which also houses the issues of Ubuntu. The number of issues fetched since the beginning of the project are 28,429. Issues with no comments were not scraped, just like in the case of Apache Lucene and Mozilla Thunderbird. Additionally, a major problem faced while scraping data from Launchpad was that there were abundant number of downlinks. When these links were accessed manually, they kept loading until getting timed out and returned 404 error while scraping.

Launchpad did not have its export option to fetch mass number of issues and its comments. Hence, each and every issue had to be scraped one at a time. In addition, not all the issues had the same HTML tags and attributes for the comments section. Launchpad provides individual link for each of the comment of the pertinent issue and hence, the comments were scraped by obtaining the individual links for comments. All the links were requested and the necessary data was retrieved using BeautifulSoup, the library mentioned in the previous section.

\subsection{Commit Logs Dataset}
This section scrutinizes how the data of commit messages were obtained for Apache Lucene, Mozilla Thunderbird and Ubuntu.

\subsubsection{Apache Lucene}
Apache Lucene uses Git for its version control, which was developed by Linus Torvald and released in 2005. Apache Lucene along with SOLR is hosted together within the same repository on GitHub. The repository was cloned to local environment in the system to retrieve the commit logs. Git allows the logs of the repository to be exported in TXT format and this is the method that has been used to get all the logs for Apache Lucene. However, a parsing script was additionally written in Python to obtain all the commit id, author, date and commit message respectively. These information was later stored in a MySQL database. 

\subsubsection{Mozilla Thunderbird}
Mozilla Thunderbird uses Mercurial for its version control, which was developed by Matt Mackall and released in 2005. The Mozilla Thunderbird project in a repository owned by Mozilla. This repository was cloned to the system to extract the commit logs. Like Git, Mercurial also offers users an option to extract the entire commit log history in TXT format. This data was parsed using a script written in Python to retrieve all the commit id, author, date and commit message respectively. The retrieved information was eventually stored in a MySQL database. 

\subsubsection{Ubuntu}
Ubuntu makes use of Bazaar primarily for its version control, which was developed by Martin Pool in 2005 and maintained by Canonical Ltd. The Ubuntu project is hosted on Launchpad platform using Bazaar. Due to too numerous repositories, the entire Ubuntu project could not be cloned individually. However, a script in Python was written using the BeautifulSoup library; similar to the script used for fetching data from issue trackers for Ubuntu. Identical problems of many downlinks were encountered and hence, only those that were up were fetched automatically. A MySQL database was used to store all of these information. 

\section{IRC Messages Dataset}

This section explores how data was obtained from IRC messages for Apache Lucene, Mozilla Thunderbird and Ubuntu. 

%------- chapter 4 -------

\chapter{Manual Content Analysis}

This chapter dives into the procedure of manual content analysis and the formation of ground truth for machine learning experiments to be carried out later. In addition, this chapter also gives insights on how the sample data for the experiements to be performed were obtained from the initial raw dataset. 

\section{Ground Truth Dataset}
An imperative step after performing machine learning experiements is to measure characteristics such as accuracy, precision, recall and F1 score. This, in turn, means one needs to have a reliable dataset for ground truth. Messages from the issue trackers have no pre-defined structure as to whether they contain any kind of rationale or not. To have similar understanding, be on the same page and reduce the number of disagreements later, both the annotators randomly went through a few sentences. The annotators marked rationale for each of them if applicable, classified them appropriately, and shared their perspectives before concluding on mutual agreements as to how to go about in the main manual content analysis experiment.  Some of the examples are mentioned in detail in the coding guide for manual content analysis. There were two kinds of classification performed in this stage: binary and multi-class classification. Binary classification was done to check if the sentence contains any kind of rationale or not. Multi-class classification was done to classify the sentence, if rationale is found, into five categories: issue, alternatives, pro-arguments, con-arguments and decision, which are explained as follows.

\begin{itemize}
\item \textbf{Issue:} the problem to be solved, or a feature to be implemented
\item \textbf{Alternative:} the feasible solutions that could address the issue
\item \textbf{Pro-Argument:} the argument put across in favor of a particular alternative
\item \textbf{Con-Argument:} the argument put across against a particular alternative
\item \textbf{Decision:} the final conclusions made to resolve the issue
\end{itemize}

\section{Random Stratified Sampling}
For the main content analysis, two annotators individually read through XXXX number of individual sentences from XXXX comments spanning 100 issues from each of the three projects. Although the number of issues fetched for each of them were much higher, only 100 of them were attained for each project using random stratified sampling due to time constraints of the research. Random stratified sampling can be of two types: proportionate random stratified sampling and disproportionate random stratified sampling. In proportionate random stratified sampling, the parent dataset is divided into multiple strata and the child dataset sampled randomly must have the strata in the same proportion as the parent dataset. Contrary to proportionate random stratified sampling, the case of disproportionate random stratified sampling has the parent dataset divided into multiple strate but the child dataset sampled randomly may not have its strata in the same proportion as the parent dataset. There are, however, three steps carried out sequentially to obtain the sample which are as follows.

\begin{itemize}
\item {Step 1:} Filter the dataset by discarding issues that do not fall under the required date range. This is to make sure the comments from issue trackers are from the same duration as messages from IRC. 
\item {Step 2:} In the next step, count the number of comments for each issue that are under the required date range but only consider issues that have between 5 and 30 comments, and divide them into 5 groups: 6-10 comments, 11-15 comments, 16-20 comments, 21-25 comments and 26-30 comments. We wish to avoid issues with too few comments (less than 5) or way too many comments (greater than 30). 
\item {Step 3:} Perform proportionate random stratified sampling to obtain 100 random issues from the filtered dataset in step 2 for each of the three projects.
\end{itemize}


%------- chapter 5 -------

\chapter{Natural Language Processing}

\textit{Note: If you did an evaluation / case study, describe it here.}

\section{Topic Modeling}

\textit{Note: Describe the design / methodology of the evaluation and why you did it like that. E.g. what kind of evaluation have you done (e.g. questionnaire, personal interviews, simulation, quantitative analysis of metrics, what kind of participants, what kind of questions, what was the procedure?}

\section{Parts of Speech Tagging}

\textit{Note: Derive concrete objectives / hypotheses for this evaluation from the general ones in the introduction.}

\section{Named Entity Recognition}

\textit{Note: Derive concrete objectives / hypotheses for this evaluation from the general ones in the introduction.}

%------- chapter 7 -------

\chapter{Machine Learning}

\textit{Note: This chapter includes the status of your thesis, a conclusion and an outlook about future work.}

\section{Binary Classification}

\textit{Note: Describe honestly the achieved goals (e.g. the well implemented and tested use cases) and the open goals here. if you only have achieved goals, you did something wrong in your analysis.}

\section{Fine Grained Classification}

\textit{Note: Describe honestly the achieved goals (e.g. the well implemented and tested use cases) and the open goals here. if you only have achieved goals, you did something wrong in your analysis.}

%------- chapter 6 -------

\chapter{Summary}

\textit{Note: This chapter includes the status of your thesis, a conclusion and an outlook about future work.}

\section{Status}

\textit{Note: Describe honestly the achieved goals (e.g. the well implemented and tested use cases) and the open goals here. if you only have achieved goals, you did something wrong in your analysis.}

\subsection{Realized Goals}

\textit{Note: Summarize the achieved goals by repeating the realized requirements or use cases stating how you realized them.}

\subsection{Open Goals}

\textit{Note: Summarize the open goals by repeating the open requirements or use cases and explaining why you were not able to achieve them. \textbf{Important:} It might be suspicious, if you do not have open goals. This usually indicates that you did not thoroughly analyze your problems.}

\section{Conclusion}

\textit{Note: Recap shortly which problem you solved in your thesis and discuss your \textbf{contributions} here.}

\section{Future Work}

\textit{Note: Tell us the next steps  (that you would do if you have more time. be creative, visionary and open-minded here.}



\appendix

\chapter{e.g. Questionnaire}

\textit{Note: If you have large models, additional evaluation data like questionnaires or non summarized results, put them into the appendix.}


\clearpage

\listoffigures
\clearpage

\listoftables
\clearpage

\bibliography{thesis}
\bibliographystyle{alpha}

\end{document}
