id,sentence,isIssue,isAlternative,isPro,isCon,isDecision,,
1,"Another non-reproducing failure, from my Jenkins:

Checking out Revision 85a27a231fdddb118ee178baac170da0097a02c0 (refs/remotes/origin/master)
[...]
   [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(7.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=61A674A498110EC0 -Dtests.slow=true -Dtests.locale=ja-JP -Dtests.timezone=Greenwich -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] FAILURE 0.07s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f0, reader=_4(7.0.0):c19 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([61A674A498110EC0:575A168B19E46DDC]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=false): {}, locale=ja-JP, timezone=Greenwich
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=220134656,total=306184192
   [junit4]   2> NOTE: All tests run in this JVM: [TestStringMSBRadixSorter, TestSpanTermQuery, TestOmitPositions, TestIndexableField, TestHighCompressionMode, TestDeterminizeLexicon, TestPackedTokenAttributeImpl, TestTopDocsCollector, TestIndexOrDocValuesQuery, TestDocValuesRewriteMethod, TestDocument, TestCrash, TestWildcardRandom, TestDocIdSetBuilder, TestFilterLeafReader, TestMergedIterator, TestMultiThreadTermVectors, TestAtomicUpdate, TestNorms, Test4GBStoredFields, TestFixedLengthBytesRefArray, TestFieldInvertState, TestBoolean2ScorerSupplier, TestLevenshteinAutomata, TestGraphTokenStreamFiniteStrings, TestStandardAnalyzer, TestSegmentReader, TestScorerPerf, TestBoostQuery, TestMergePolicyWrapper, TestComplexExplanations, TestPointQueries, TestMixedCodecs, TestPointValues, TestMultiMMap, TestLazyProxSkipping, TestTerms, TestIndexWriterThreadsToSegments, TestFilterWeight, TestDocumentsWriterDeleteQueue, TestCharFilter, TestDocInverterPerFieldErrorInfo, TestSimilarityProvider, LimitedFiniteStringsIteratorTest, TestNewestSegment, TestFSTs, TestClassicSimilarity, TestUnicodeUtil, TestQueryBuilder, TestSwappedIndexFiles, TestTimSorterWorstCase, TestBinaryDocValuesUpdates]",1,0,0,0,0,,
2,"Another non-reproducing failure, from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/19961/ (log - and commit sha - no longer available; the notification email arrived on June 24 at 10:37PM):

[...]
  [junit4] Suite: org.apache.lucene.index.TestBinaryDocValuesUpdates
  [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(7.0.0):C2)
  [junit4]   1> TEST: now reopen
  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=3A4BC284D906CE1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=hr-HR -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8
  [junit4] FAILURE 0.65s J2 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
  [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f4, reader=_28(7.0.0):C936 expected:<12> but was:<11>
  [junit4]    > 	at __randomizedtesting.SeedInfo.seed([3A4BC284D906CE1A:CB7A0AB58F3AD06]:0)
  [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
  [junit4]    > 	at java.lang.Thread.run(Thread.java:748)
  [junit4] IGNOR/A 0.00s J2 | TestBinaryDocValuesUpdates.testTonsOfUpdates
  [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
  [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, maxDocsPerChunk=1, blockSize=26), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=12304, blockSize=26)), sim=RandomSimilarity(queryNorm=false): {}, locale=hr-HR, timezone=Pacific/Pitcairn
  [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 1.8.0_131 (64-bit)/cpus=8,threads=1,free=302943312,total=518979584
  [junit4]   2> NOTE: All tests run in this JVM: [TestDirectPacked, TestFieldCacheRewriteMethod, TestBagOfPostings, TestEarlyTermination, TestReaderWrapperDVTypeCheck, TestNeedsScores, TestRoaringDocIdSet, TestShardSearching, TestSpansEnum, TestSegmentTermEnum, TestLongPostings, TestIndexReaderClose, TestLucene70NormsFormat, TestReqExclBulkScorer, TestField, TestSegmentTermDocs, TestSimilarityBase, TestGeoEncodingUtils, TestPayloadsOnVectors, TestCharTermAttributeImpl, TestDisjunctionMaxQuery, TestTermRangeQuery, TestLongValuesSource, TestCachingTokenFilter, TestOfflineSorter, TestTopDocsCollector, TestBufferedIndexInput, TestTermScorer, TestPerFieldPostingsFormat2, TestConsistentFieldNumbers, TestFieldsReader, TestConjunctions, TestSloppyPhraseQuery2, TestSetOnce, TestRollingUpdates, TestIndexWriterLockRelease, TestIndexWriterMergePolicy, TestRollingBuffer, TestBinaryDocument, TestSimpleFSLockFactory, TestIndexingSequenceNumbers, FiniteStringsIteratorTest, TestGraphTokenStreamFiniteStrings, TestSentinelIntSet, TestHugeRamFile, TestSortedNumericSortField, TestMultiCollector, TestSpanNotQuery, TestAllFilesHaveCodecHeader, TestTrackingDirectoryWrapper, TestControlledRealTimeReopenThread, TestDirectoryReader, TestDocValues, TestDoubleRangeFieldQueries, TestSpanCollection, TestDemoParallelLeafReader, TestSpans, TestTerms, Test2BBinaryDocValues, TestParallelCompositeReader, TestArrayUtil, TestPrefixQuery, TestAttributeSource, TestByteBlockPool, TestCompiledAutomaton, TestSimpleExplanationsOfNonMatches, TestDocValuesScoring, TestExceedMaxTermLength, TestNRTThreads, TestLazyProxSkipping, TestSimilarity2, TestSearchWithThreads, TestPolygon2D, TestGrowableByteArrayDataOutput, TestIndexCommit, TestBasics, TestSearcherManager, TestNorms, TestStandardAnalyzer, TestTopDocsMerge, TestMinimize, TestNRTReaderWithThreads, TestIndexWriterForceMerge, TestPerFieldPostingsFormat, TestCollectionUtil, TestFastDecompressionMode, TestSort, TestMultiDocValues, TestCustomSearcherSort, TestTermsEnum2, Test2BDocs, TestMixedCodecs, TestSpanExplanations, TestFastCompressionMode, TestStressIndexing2, TestMultiPhraseQuery, TestDeterminism, TestMergeSchedulerExternal, TestForceMergeForever, TestSameScoresWithThreads, TestMultiFields, TestLiveFieldValues, TestSpanSearchEquivalence, TestPayloads, TestDoc, TestFieldMaskingSpanQuery, TestExternalCodecs, TestRegexpQuery, TestIntBlockPool, TestComplexExplanationsOfNonMatches, TestParallelReaderEmptyIndex, TestDocument, TestFileSwitchDirectory, TestDirectory, TestRegexpRandom, TestMultiLevelSkipList, TestCheckIndex, TestBooleanQueryVisitSubscorers, TestMatchAllDocsQuery, TestSubScorerFreqs, TestIndexWriterConfig, TestPositionIncrement, TestSpanExplanationsOfNonMatches, TestFilterLeafReader, TestSameTokenSamePosition, TestAutomatonQueryUnicode, TestRamUsageEstimator, TestSpanFirstQuery, TestIsCurrent, TestNoMergePolicy, TestNoMergeScheduler, TestNamedSPILoader, TestBytesRef, TestCharFilter, TestTwoPhaseCommitTool, TestCloseableThreadLocal, TestVersion, TestReaderClosed, TestNGramPhraseQuery, TestIntsRef, Test2BPositions, Test2BPostingsBytes, Test2BTerms, TestByteArrayDataInput, Test2BPagedBytes, TestCharArraySet, TestDelegatingAnalyzerWrapper, TestStopFilter, TestBlockPostingsFormat, TestLucene50TermVectorsFormat, Test2BSortedDocValuesOrds, TestAllFilesCheckIndexHeader, TestAllFilesHaveChecksumFooter, TestBinaryDocValuesUpdates]
  [junit4] Completed [362/453 (1!)]",1,0,0,0,0,,
3,"on J2 in 6.22s, 29 tests, 1 failure, 1 skipped <<< FAILURES!",0,0,0,0,0,,
4,I'll tackle this.,0,0,0,0,0,,
5,"Commit eaf1d45a1cad74a1037c7c4178fd2379a903f8cc in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45 ]
LUCENE-7888: fix concurrency hazards between merge completing and DV updates applying",0,0,0,0,0,,
6,I think these should be fixed now.,0,0,0,0,0,,
7,"It was a tricky concurrency hazard, where an indexing thread that's resolving DV updates thinks it's done just as a merge is wrapping up and in that case there was a window between the two threads where DV updates could be lost.",1,0,0,0,0,,
8,Thanks Steve Rowe.,0,0,0,0,0,,
9,"Mike, any reason not to backport to branch_7x and branch_7_0?",0,0,0,0,0,,
10,"There was a recent failure on a Jenkins branch_7x job https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-Linux/9/:

Checking out Revision 758cbd98a7aa020ad67aea775028badf0be6418c (refs/remotes/origin/branch_7x)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D57106AE532F4164 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=qu-PE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.44s J2 | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f2, reader=_y(7.1.0):C435:fieldInfosGen=2:dvGen=2 expected:<7> but was:<6>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D57106AE532F4164:E38D6481D2DA2278]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   [junit4]    > 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   [junit4]    > 	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
   [junit4]    > 	at java.base/java.lang.Thread.run(Thread.java:844)
   [junit4] IGNOR/A 0.00s J2 | TestMixedDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: leaving temporary files on disk at: /home/jenkins/workspace/Lucene-Solr-7.x-Linux/lucene/build/core/test/J2/temp/lucene.index.TestMixedDocValuesUpdates_D57106AE532F4164-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=RandomSimilarity(queryNorm=true): {}, locale=qu-PE, timezone=America/Yakutat
   [junit4]   2> NOTE: Linux 4.10.0-21-generic amd64/Oracle Corporation 9 (64-bit)/cpus=8,threads=1,free=165322832,total=342360064",1,0,0,0,0,,
11,"More non-reproducing master failures from my Jenkins, commit shas are all after Mike's commit on this issue:

Checking out Revision 48b4960e0c093b480b8328f324992a7006054f17 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=6A2FDBE9B9C2F59C -Dtests.slow=true -Dtests.locale=ar-YE -Dtests.timezone=Asia/Beirut -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.31s J1 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=63, field=f1, reader=_l(8.0.0):c88:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([6A2FDBE9B9C2F59C:5CD3B9C638379680]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   1> TEST: isNRT=false reader1=StandardDirectoryReader(segments_1:4 _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4] IGNOR/A 0.00s J1 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, f=PostingsFormat(name=LuceneVarGapDocFreqInterval), k1=PostingsFormat(name=LuceneVarGapDocFreqInterval), dvUpdateKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), k2=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), foo=PostingsFormat(name=LuceneVarGapDocFreqInterval), upd=PostingsFormat(name=Direct), updKey=FSTOrd50, id=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), key=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128)))}, docValues:{val=DocValuesFormat(name=Lucene70), ndv=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Asserting), sdv=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Asserting), control=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), sort=DocValuesFormat(name=Asserting), f2=DocValuesFormat(name=Direct), cf0=DocValuesFormat(name=Lucene70), f3=DocValuesFormat(name=Lucene70), f4=DocValuesFormat(name=Asserting), f5=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Asserting), bdv2=DocValuesFormat(name=Asserting), number=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Lucene70), bdv=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Lucene70), key=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1142, maxMBSortInHeap=7.285443710546513, sim=RandomSimilarity(queryNorm=false): {}, locale=ar-YE, timezone=Asia/Beirut
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=296202568,total=395313152



Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=en-ZA -Dtests.timezone=America/Inuvik -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.89s J6  | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f3, reader=_o(8.0.0):c417:fieldInfosGen=2:dvGen=2 expected:<5> but was:<4>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestBinaryDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {bdv=FSTOrd50, k1=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), f=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), dvUpdateKey=FSTOrd50, foo=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), k2=PostingsFormat(name=LuceneFixedGap), upd=Lucene50(blocksize=128), updKey=TestBloomFilteredLucenePostings(BloomFilteringPostingsFormat(Lucene50(blocksize=128))), id=PostingsFormat(name=LuceneFixedGap), key=PostingsFormat(name=LuceneFixedGap)}, docValues:{ndv=DocValuesFormat(name=Memory), f10=DocValuesFormat(name=Asserting), f12=DocValuesFormat(name=Direct), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Asserting), f13=DocValuesFormat(name=Memory), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Direct), f1=DocValuesFormat(name=Direct), f15=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Memory), f18=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Asserting), f17=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Direct), bdv2=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Memory), number=DocValuesFormat(name=Direct), f7=DocValuesFormat(name=Asserting), f8=DocValuesFormat(name=Lucene70), bdv1=DocValuesFormat(name=Asserting), f9=DocValuesFormat(name=Direct), id=DocValuesFormat(name=Direct), val=DocValuesFormat(name=Asserting), f21=DocValuesFormat(name=Asserting), f20=DocValuesFormat(name=Memory), f23=DocValuesFormat(name=Direct), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Asserting), f24=DocValuesFormat(name=Memory), sort=DocValuesFormat(name=Lucene70), cf0=DocValuesFormat(name=Asserting), cf2=DocValuesFormat(name=Direct), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Asserting), cf3=DocValuesFormat(name=Memory), cf6=DocValuesFormat(name=Direct), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Asserting), cf7=DocValuesFormat(name=Memory), cf9=DocValuesFormat(name=Lucene70), ssdv=DocValuesFormat(name=Lucene70), sdv=DocValuesFormat(name=Asserting), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Memory), cf24=DocValuesFormat(name=Asserting), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Direct), cf20=DocValuesFormat(name=Asserting), key=DocValuesFormat(name=Direct), cf=DocValuesFormat(name=Direct), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Direct), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Memory), cf17=DocValuesFormat(name=Asserting), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Direct), control=DocValuesFormat(name=Asserting), cf12=DocValuesFormat(name=Memory), cf13=DocValuesFormat(name=Asserting), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Direct), bdv=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=74, maxMBSortInHeap=7.963102974639169, sim=RandomSimilarity(queryNorm=true): {}, locale=en-ZA, timezone=America/Inuvik
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=73914040,total=520617984



Checking out Revision cb23fa9b4efa5fc7c17f215f507901d459e9aa6f (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestMixedDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=D695B86B920AF645 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=pt-BR -Dtests.timezone=Africa/Porto-Novo -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1
   [junit4] FAILURE 0.17s J6  | TestMixedDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid binary value for doc=0, field=f1, reader=_7(8.0.0):c118:fieldInfosGen=2:dvGen=2 expected:<3> but was:<2>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([D695B86B920AF645:E069DA4413FF9559]:0)
   [junit4]    > 	at org.apache.lucene.index.TestMixedDocValuesUpdates.testManyReopensAndFields(TestMixedDocValuesUpdates.java:141)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4]   2> NOTE: leaving temporary files on disk at: /var/lib/jenkins/jobs/Lucene-Solr-Nightly-master/workspace/lucene/build/core/test/J6/temp/lucene.index.TestMixedDocValuesUpdates_D695B86B920AF645-001
   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {upd=Lucene50(blocksize=128), updKey=PostingsFormat(name=LuceneVarGapDocFreqInterval), id=PostingsFormat(name=LuceneVarGapFixedInterval), key=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{f10=DocValuesFormat(name=Memory), f12=DocValuesFormat(name=Lucene70), f11=DocValuesFormat(name=Lucene70), f14=DocValuesFormat(name=Memory), f13=DocValuesFormat(name=Asserting), f0=DocValuesFormat(name=Lucene70), f16=DocValuesFormat(name=Lucene70), f15=DocValuesFormat(name=Lucene70), f1=DocValuesFormat(name=Lucene70), f2=DocValuesFormat(name=Asserting), f18=DocValuesFormat(name=Memory), f17=DocValuesFormat(name=Asserting), f3=DocValuesFormat(name=Memory), f4=DocValuesFormat(name=Lucene70), f19=DocValuesFormat(name=Lucene70), f5=DocValuesFormat(name=Lucene70), f6=DocValuesFormat(name=Asserting), f7=DocValuesFormat(name=Memory), f8=DocValuesFormat(name=Lucene70), f9=DocValuesFormat(name=Lucene70), id=DocValuesFormat(name=Lucene70), f21=DocValuesFormat(name=Memory), f20=DocValuesFormat(name=Asserting), f23=DocValuesFormat(name=Lucene70), f22=DocValuesFormat(name=Lucene70), f25=DocValuesFormat(name=Memory), f24=DocValuesFormat(name=Asserting), cf0=DocValuesFormat(name=Memory), cf2=DocValuesFormat(name=Lucene70), cf1=DocValuesFormat(name=Lucene70), cf4=DocValuesFormat(name=Memory), cf3=DocValuesFormat(name=Asserting), cf6=DocValuesFormat(name=Lucene70), cf5=DocValuesFormat(name=Lucene70), cf8=DocValuesFormat(name=Memory), cf7=DocValuesFormat(name=Asserting), cf9=DocValuesFormat(name=Lucene70), cf25=DocValuesFormat(name=Lucene70), cf23=DocValuesFormat(name=Asserting), cf24=DocValuesFormat(name=Memory), cf21=DocValuesFormat(name=Lucene70), cf22=DocValuesFormat(name=Lucene70), cf20=DocValuesFormat(name=Memory), key=DocValuesFormat(name=Lucene70), cf=DocValuesFormat(name=Lucene70), cf18=DocValuesFormat(name=Lucene70), cf19=DocValuesFormat(name=Lucene70), f=DocValuesFormat(name=Lucene70), cf16=DocValuesFormat(name=Asserting), cf17=DocValuesFormat(name=Memory), cf14=DocValuesFormat(name=Lucene70), cf15=DocValuesFormat(name=Lucene70), cf12=DocValuesFormat(name=Asserting), cf13=DocValuesFormat(name=Memory), cf10=DocValuesFormat(name=Lucene70), cf11=DocValuesFormat(name=Lucene70)}, maxPointsInLeafNode=1397, maxMBSortInHeap=7.760564666966891, sim=RandomSimilarity(queryNorm=false): {}, locale=pt-BR, timezone=Africa/Porto-Novo
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=283409888,total=478150656



Checking out Revision 6c163658bbca15b1e4ff81d16b25e07df78468e8 (refs/remotes/origin/master)
[...]
   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBinaryDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=85C41F1E0BBEB082 -Dtests.slow=true -Dtests.locale=ca-ES -Dtests.timezone=Kwajalein -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [junit4] FAILURE 0.71s J0 | TestBinaryDocValuesUpdates.testManyReopensAndFields <<<
   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=0, field=f2, reader=_d(8.0.0):c55:fieldInfosGen=2:dvGen=2 expected:<4> but was:<3>
   [junit4]    > 	at __randomizedtesting.SeedInfo.seed([85C41F1E0BBEB082:B3387D318A4BD39E]:0)
   [junit4]    > 	at org.apache.lucene.index.TestBinaryDocValuesUpdates.testManyReopensAndFields(TestBinaryDocValuesUpdates.java:844)
   [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [junit4] IGNOR/A 0.00s J0 | TestBinaryDocValuesUpdates.testTonsOfUpdates
   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   1> TEST: isNRT=true reader1=StandardDirectoryReader(segments:3:nrt _0(8.0.0):c2)
   [junit4]   1> TEST: now reopen
   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=RandomSimilarity(queryNorm=true): {}, locale=ca-ES, timezone=Kwajalein
   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=312487000,total=502792192",1,0,0,0,0,,
12,"Mike, any reason not to backport to branch_7x and branch_7_0?",0,0,0,0,0,,
13,"Ugh, I thought my commit went in before 7.x/7.0 branched; I'll back port tomorrow, and look into the new test failures!",0,0,0,0,0,,
14,"Mike, any reason not to backport to branch_7x and branch_7_0?",0,0,0,0,0,,
15,"OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.",0,0,0,0,0,,
16,I'll dig on the new failures.,0,0,0,0,0,,
17,"
Mike, any reason not to backport to branch_7x and branch_7_0?",0,0,0,0,0,,
18,"OK original commit here (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eaf1d45) was in fact before 7.0/7.x branched off, so I'm not losing my mind as much as I previously thought.",0,0,0,0,0,,
19,"Crap, sorry for wasting your time.",0,0,0,0,0,,
20,"I think I looked at git log for one of those branches and didn't see your commit, and then assumed, given the close timing of the branches' being cut, that the commit didn't make it.",0,0,0,0,0,,
21,But looking now I see it on both branches' logs.,0,0,0,0,0,,
22,No worries Steve Rowe!,0,0,0,0,0,,
23,"Also, I think the 4 non-reproducing seeds above (https://issues.apache.org/jira/browse/LUCENE-7888?focusedCommentId=16077449&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16077449) were before my last commit (7c704d5258b3be8c383ccb96bf4a30be441f091c) fixing a race ... so I'm hoping there are no more failures in this challenging test",0,0,0,0,1,,
24,I can look into the clustering plugin's use of it.,1,0,0,0,0,,
25,"I recall it was unfortunately required, but will have to go into this again to remind myself why.",1,0,0,0,0,,
26,Patch removing context classloader usage.,0,1,0,0,0,,
27,"Tests seem to pass, unfortunately Solr trunk is very unstable.",1,0,0,0,0,,
28,"Some unrelated tests also fail on Jenkins, so I cannot be sure all is fine.",1,0,0,0,0,,
29,This patch also adds context class loaders on te forbidden api list.,0,1,0,0,0,,
30,"Because of that I used the withContextClassLoader(ClassLoader, () -> { ... }) lambda method.",0,1,0,0,0,,
31,Looks good to me.,0,0,1,0,0,,
32,I'll check why the context classloader is required in clustering later on.,0,1,0,0,0,,
33,I think the case was that clustering was under shared libraries and resources loaded per-core couldn't figure where to load classes from.,1,0,0,0,0,,
34,I did some live test with the standalone techproducts example.,0,1,0,0,0,,
35,"I have seen no issues, so I think this should be fine to commit.",0,0,1,0,0,,
36,"I will add a CHANGES entry in both Lucene and Solr, because this affects both projects.",0,0,0,0,0,,
37,Unfortunately the latest test failures on master make it hard to differentiate between failures caused by my changes and ones already there.,1,0,0,0,0,,
38,"But all failures in tests that I see here, look like the ones Jenkins is drinking with his beers!",0,0,0,0,0,,
39,"This is not a good state, the test suite should pass for a clean checkout.",0,0,0,0,0,,
40,"Commit 5de15ff403fbf4afe68718151617e6104f7e3888 in lucene-solr's branch refs/heads/master from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5de15ff ]
LUCENE-7883: Lucene/Solr no longer uses the context class loader when resolving resources",0,0,0,0,0,,
41,I added changes and migrate entries and committed to master (7.0).,0,0,0,0,1,,
42,Here is a patch.,0,1,0,0,0,,
43,+1 good catch!,0,0,1,0,0,,
44,1,0,0,1,0,0,,
45,"Commit 3a0c2a691d4fea1670b3d071032fc54c716b5d1a in lucene-solr's branch refs/heads/branch_6_5 from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a0c2a6 ]
LUCENE-7755: Join queries should not reference IndexReaders.",0,0,0,0,0,,
46,"Commit bd2ec8e40e83e4712062c37ed121132054409918 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bd2ec8e ]
LUCENE-7755: Join queries should not reference IndexReaders.",0,0,0,0,0,,
47,"Commit edafcbad14482f3cd2f072fdca0c89600e72885d in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=edafcba ]
LUCENE-7755: Join queries should not reference IndexReaders.",0,0,0,0,0,,
48,Patch that ensures we won't add any new compiler warnings in several categories (that we're pretty good on already) in the future.,0,1,1,0,0,,
49,We can deal with fixing existing rawtypes or a few other classes of warning in the future.,1,0,0,0,0,,
50,David Smiley - you seemed interested in this conversation last time it came up.,0,0,0,0,0,,
51,Thanks for filing this issue!,0,0,0,0,0,,
52,Are all the changes in this patch necessary to get the build to pass?,1,0,0,0,0,,
53,So to clarify... no code (outside what the patch touches) needs adjustments?,1,0,0,0,0,,
54,"Yes, the build passes for me with only the two additional changes in WordDictionary and SimpleServer.",0,1,1,0,0,,
55,Warnings for -Xlint:-auxiliaryclass -Xlint:-deprecation -Xlint:-rawtypes -Xlint:-serial -Xlint:-unchecked are all disabled.,1,0,0,0,0,,
56,Each of those causes a lot of errors that I'd like to see eventually followed up on.,1,0,0,0,0,,
57,"The auxiliary class warnings are the easiest of those, but still enough work that I felt like it should be a separate task.",1,0,0,0,0,,
58,"I also have a sneaking suspicion that this only affects lucene and solr is somehow ignoring it, but couldn't find anything to confirm that.",1,0,0,0,0,,
59,"Yes on Solr your change is not enabled: https://github.com/apache/lucene-solr/blob/master/solr/common-build.xml#L30
We should also review Solr (maybe in a separate issue).",1,0,0,0,0,,
60,"I am not sure if the warning exclusions are really needed, because we no longer have the general -Xlint.",0,1,0,1,0,,
61,But it's good to have them listed!,0,0,1,0,0,,
62,"Yea, I like having them listed because it makes it easier to go back and look at them and decide which ones to add.",0,1,1,0,0,,
63,I don't have access to an IBM jdk to check if that produces different output or not.,0,0,0,0,0,,
64,Uwe Schindler - do you think this is fine to commit or we should tackle more work in this issue?,0,0,0,0,0,,
65,"Hi,
IBM JDK8+ should also use OpenJDK internally, so I dont't think hit has much different options.",0,1,0,0,0,,
66,"I can try later, I have one installed.",0,0,0,0,0,,
67,What do we do with Solr?,1,0,0,0,0,,
68,Keep it as it is (it overrides to do no Xlint warnings at all and don't fail on warning).,0,0,0,0,1,,
69,Otherwise I am fine with committing this.,0,0,1,0,0,,
70,But we should really work on removing unsafe and rawtypes warnings from functions module.,1,0,0,0,0,,
71,"Now those are completely undetected (no warning, no error).",1,0,0,0,0,,
72,Patch of 2 Jan 2017.,0,1,0,0,0,,
73,"This can be used as proximity subquery whenever SynonymQuery is used now, i.e.",0,1,0,0,0,,
74,for synonym terms.,0,1,0,0,0,,
75,"I think this improves span scoring somewhat, see the tests and the test output when uncommenting showQueryResults for the test cases with two terms.",0,0,1,0,0,,
76,"Implementation:
SynonymQuery exposes new methods getField() and SynonymWeight.getSimScorer() for use in SpanSynonymQuery.",0,1,0,0,0,,
77,"Improved use of o.a.l.index.Terms and TermsEnum in SynonymQuery, at most a single TermsEnum will be used.",0,0,1,0,0,,
78,Aside: how about renaming Terms to FieldTerms?,0,1,0,0,0,,
79,This takes DisjunctionSpans out of SpanOrQuery.,0,0,1,0,0,,
80,"This adds SynonymSpans as (an almost empty) subclass of DisjunctionSpans, for later further scoring improvement.",0,0,1,0,0,,
81,PHRASE_TO_SPAN_TERM_POSITIONS_COST is used from SpanTermQuery and made package private there.,0,1,1,0,0,,
82,"Some plans for using this:
In LUCENE-7580 to get real synonym scoring behaviour.",0,0,1,0,0,,
83,In Surround to score truncations.,0,0,1,0,0,,
84,"In the patch of 2 Jan 2017 the term contexts are extracted twice, once in SynonymWeight and once to create the SpanSynonymWeight.",1,0,0,0,0,,
85,I'll post a fix later.,0,0,0,0,0,,
86,Patch of 3 Jan 2017.,0,1,0,0,0,,
87,"Compared to yesterday, this adds getTermContexts() in SynonymWeight for use in SpanSynonymQuery.",0,1,1,0,0,,
88,"In SpanSynonymQuery.java here, this is not used:


import org.apache.lucene.search.similarities.Similarity;",1,0,0,0,0,,
89,"GitHub user PaulElschot opened a pull request:
 https://github.com/apache/lucene-solr/pull/165
 LUCENE-7615 of 8 March 2017.",0,0,0,0,0,,
90,Adds support for SpanSynonymQuery in xml queryparser.,0,0,0,0,0,,
91,"You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/PaulElschot/lucene-solr lucene7615-20170308
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/165.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #165

commit 676c13c0c70e3f344ad6fb430eb5868270be83aa
Author: Paul Elschot <paul.j.elschot@gmail.com>
Date:   2017-03-08T22:10:40Z
 LUCENE-7615 of 8 March 2017.",0,0,0,0,0,,
92,Adds support for SpanSynonymQuery in xml queryparser.,0,0,0,0,0,,
93,"GitHub user tballison opened a pull request:
 https://github.com/apache/lucene-solr/pull/75
 LUCENE-7434, first draft
 LUCENE-7434, first draft
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/tballison/lucene-solr master
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/75.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #75

commit c37f1e0d66f1f28a5c83033d9496cc33c55f265e
Author: tballison <tallison@mitre.org>
Date:   2016-09-01T19:33:55Z
 LUCENE-7434, first draft",0,0,0,0,0,,
94,"But this allow to create Span Disjunction Query, which is considered as a black sheep in Lucene herd.",0,0,0,1,0,,
95,"I don't know why exactly, but have an idea.",0,0,0,0,0,,
96,"Sorry, I've been away from Lucene for too long.",0,0,0,0,0,,
97,Can you explain a bit more?,0,0,0,0,0,,
98,Thanks Tim Allison for creating this issue.,0,0,0,0,0,,
99,"In order to merge threads, I want to clarify that my original question was about limiting the search window as well as the number of matches.",1,0,0,0,0,,
100,"The slop  parameter sets the maximum distance allowed between each of the subspans and I was looking to add another parameter for the maximum window in which multiple the sub spans should appear together - between the beginning of the first, to the beginning/end of the last one.",0,1,0,0,0,,
101,"As it happens I am in slow progress making something for the case minNumberShouldMatch=2, all pairs.",0,1,0,0,0,,
102,"In case there is interest in an early version of that, please let me know.",0,0,0,0,0,,
103,For the maximum window there will be similar restrictions as for LUCENE-7398.,0,1,0,1,0,,
104,"This all pairs thing is useful here anyway, so here it is.",0,0,1,0,0,,
105,There is a nocommit for an unfinished corner.,0,0,0,0,0,,
106,It splits off DisjunctionSpans from SpanOr and uses that to determine the matching pairs.,0,1,0,0,0,,
107,Interesting.,0,0,1,0,0,,
108,Whoa on LUCENE-7398!,0,0,0,0,0,,
109,"What's the use case for minNumberShouldMatch=2, all pairs?",0,0,0,0,0,,
110,Apologies for my daftness...,0,0,0,0,0,,
111,"What's the use case for minNumberShouldMatch=2, all pairs?",0,0,0,0,0,,
112,"This might generalize to higher values of minNumberShouldMatch, and replacing the slop by a window is easy.",0,0,1,0,0,,
113,For higher values of minNumberShouldMatch it would probably be good to reuse the implementation from boolean queries.,0,1,1,0,0,,
114,"Ah, ok.",0,0,0,0,0,,
115,Thank you.,0,0,0,0,0,,
116,Is my proposed approach flawed for the minNumberShouldMatch component of Saar Carmi's request?,0,0,0,0,0,,
117,Is my proposed approach flawed for the minNumberShouldMatch component ... ?,0,0,0,0,0,,
118,"Looking at the code on github here https://github.com/apache/lucene-solr/pull/75/commits/c37f1e0d66f1f28a5c83033d9496cc33c55f265e
it uses NearSpansOrdered and NearSpansUnOrdered with all subSpans, as usual, see lines 277/278.",0,0,0,0,0,,
119,I think that is too strict when more than the required number of subSpans are actually present in the segment.,0,0,0,1,0,,
120,"The check for presence of subSpans should be at document level, and even then fewer than present might match for the given slop/window.",0,1,0,1,0,,
121,"The (untested) all pairs code above tries to do that, but only for pairs of subSpans.",0,0,1,0,0,,
122,"If I get it right, this is a minor difference.",0,0,1,0,0,,
123,For my case it should be fine.,0,0,1,0,0,,
124,Dupe of LUCENE-3369?,0,0,0,0,0,,
125,Y.,0,0,0,0,0,,
126,Thank you.,0,0,0,0,0,,
127,Thanks to Trejkaz for pointing out that LUCENE-7434 is a duplicate of LUCENE-3369.,0,0,0,0,0,,
128,Very cool.,0,0,1,0,0,,
129,I've been meaning to look into TermAutomatonQueries for a while.,1,0,0,0,0,,
130,"My two concerns: I'm not sure how this could play nicely with the other SpanQueries, and we'd want to integrate multiterm handling.",1,0,0,0,0,,
131,"I think so, if we blow up the seed sequence to a a a B B B c c c d d d.",1,0,0,0,0,,
132,attaching a proof for x a a a and a terrific pic for it,0,0,0,0,0,,
133,The most intriguing question about TermAutomatonQuery is its' efficiency.,1,0,0,0,0,,
134,Can it load term positions only for those docs which passed term level minShouldMatch condition?,1,0,0,0,0,,
135,"eg if someone can experiment, it would be great to search with this query on a huge index and then compare it to to the same query intersected with minShouldMatch disjunction with plain term queries.",0,0,1,0,0,,
136,Will the later speedups TAQ by faster dragging?,1,0,0,0,0,,
137,The most intriguing question about TermAutomatonQuery is its' efficiency.,1,0,0,0,0,,
138,I think there's plenty of work to be done here.,0,0,0,0,0,,
139,E.g.,0,0,0,0,0,,
140,LUCENE-6824 would at least rewrite TAQ to simpler queries when possible.,0,0,1,0,0,,
141,"Commit 0c1cab71920a540807555501f7198ca402e16740 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0c1cab7 ]
LUCENE-7401: Make sure BKD trees index all dimensions.",0,0,0,0,0,,
142,"Commit ba47f530d1165d4518569422472bc9e4f1c04b26 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ba47f53 ]
LUCENE-7401: Make sure BKD trees index all dimensions.",0,0,0,0,0,,
143,Thanks Mike for having a look.,0,0,0,0,0,,
144,I am looking into it.,0,0,0,0,0,,
145,It does not seem to be related to BS1 this time since the test still fails when I disable BS1.,0,0,0,0,0,,
146,This is a test bug.,0,0,0,0,0,,
147,"CheckHits assumes that if there is a single sub explanation, then its value is necessarily the same as the parent explanation.",1,0,0,0,0,,
148,This fails with dismax when there is a single sub that produces a negative score since in that case it uses 0 as a max score and multiplies the score with the tie breaker factor.,0,0,0,1,0,,
149,"+1
Thank you for digging Adrien Grand!",0,0,0,0,0,,
150,"Commit c5defadd70a9f91bc31012b7c31c39f16d883849 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c5defad ]
LUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.",0,0,0,0,0,,
151,"Commit 1e4d51f4085664ef073ecac18dd572b0a9a02757 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1e4d51f ]
LUCENE-7352: Fix CheckHits for DisjunctionMax queries that generate negative scores.",0,0,0,0,0,,
152,Bulk close resolved issues after 6.2.0 release.,0,0,0,0,1,,
153,"I have been experimenting with the attached patch, which compresses doc ids based on the number of required bytes to store them (it only specializes 8, 16, 24 and 32 bits per doc id) and also adds delta-compression for blocks whose values are all the same.",0,1,0,0,0,,
154,The IndexAndSearchOpenStreetMaps reported a slow down of 1.7\% for the box benchmark (72.3 QPS -> 71.1 QPS) but storage requirements decreased by 9.1\% (635MB -> 577MB).,0,0,1,1,0,,
155,The storage requirements improve even more with types that require fewer bytes (LatLonPoint requires 8 bytes per value).,0,0,1,0,0,,
156,For instance indexing 10M random half floats with the patch requires 28MB vs 43MB on master (-35\%).,0,0,1,0,0,,
157,Updated patch.,0,1,0,0,0,,
158,"It now specializes both reading doc ids into an array and feeding a visitor, which seems to help get the performance back to what it is on master, or at least less than 1\% slower (not easy to distinguish minor slowdowns to noise at this stage).",0,0,1,0,0,,
159,"It has 3 cases:

increasing doc ids, which is expected to happen for either sorted segments or when all docs in a block have the same value.",0,1,0,0,0,,
160,"In that case, we delta-encode using vints.",0,1,0,0,0,,
161,"doc ids requiring less than 24 bits, which are encoded on 3 bytes.",0,1,0,0,0,,
162,"doc ids requiring less than 32 bits, which are encoded on 4 bytes like on master today.",0,1,0,0,0,,
163,I think it's ready to go?,0,0,0,0,0,,
164,"I like this better than the last patch, I think the optimization is more general.",0,0,1,0,0,,
165,"I think in the base test class, tesMostEqual() is a mistake?",0,0,0,1,0,,
166,"Hmm I can remove both actually, they do not bring value now that the detection of whether doc ids are sorted is based on the doc ids themselves rather than the fact that there is a single value in a block.",0,1,0,1,0,,
167,"Commit 01de73bc0a1b315bbbe4df046b5c0661cec8de2e in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=01de73b ]
LUCENE-7351: Doc id compression for points.",0,0,0,0,0,,
168,"Commit d66e9935c39ed859659de46d3d5cfb66f2279bd4 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d66e993 ]
LUCENE-7351: Doc id compression for points.",0,0,0,0,0,,
169,Bulk close resolved issues after 6.2.0 release.,0,0,0,0,1,,
170,"Simple patch:

removes GeoPointTestUtil from TestGeoPointQuery
fixes a range corner case in GeoPointPrefixTermsEnum
adds an explicit test for the corner case",0,1,0,0,0,,
171,1,0,0,1,0,0,,
172,"Commit 1bbac6bbd896c110d08656f79fef3ce6d7828d6b in lucene-solr's branch refs/heads/branch_6_1 from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1bbac6b ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.",0,0,0,0,0,,
173,"Commit f767855da30e8d8b070b7566cb6eebb29af63334 in lucene-solr's branch refs/heads/master from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f767855 ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.",0,0,0,0,0,,
174,Nicholas Knize Did you forget to cherry-pick to branch_6x?,0,0,0,0,0,,
175,"Commit 7448abb3bca7b8204e56a52fc115f7a2d813884d in lucene-solr's branch refs/heads/branch_6x from Nicholas Knize
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7448abb ]
LUCENE-7331: Remove GeoPointTestUtil from TestGeoPointQuery.",0,0,0,0,0,,
176,David Smiley Pinging you in case you want to have a chance to look into it before we release 6.1.,0,0,0,0,0,,
177,FYI the seed still reproduces for me on master.,0,0,0,0,0,,
178, I already caught that and will commit that unchanged.,0,NULL,0,0,0,0,0
179,"Commit b33d7176aa3624df2de1708b17919f20d034872f in lucene-solr's branch refs/heads/master from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b33d717 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes",0,0,0,0,0,,
180,"Commit 7520d79e040c16c5ab666f1ad28c8665fb0ceb40 in lucene-solr's branch refs/heads/branch_6x from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7520d79 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit b33d717)",0,0,0,0,0,,
181,"Commit 6372c0b4042ec2c8d94e50e5c2b9c1df469414e2 in lucene-solr's branch refs/heads/branch_6_1 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6372c0b ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)",0,0,0,0,0,,
182,"Reopening to backport to 6.0.2, 5.6 and 5.5.2",0,0,0,0,1,,
183,"Commit 5c546537d7b8130c05263832baff4946260f6a31 in lucene-solr's branch refs/heads/branch_5_5 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5c54653 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)",0,0,0,0,0,,
184,"Commit 1d7ad90947699e103de39fded5b78f76a30e449b in lucene-solr's branch refs/heads/branch_5_5 from Steve Rowe
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1d7ad90 ]
LUCENE-7291: Add 5.5.2 CHANGES entry",0,0,0,0,0,,
185,"Commit f6b0fb95dea43f9f508b613cf32f489aaa263c4e in lucene-solr's branch refs/heads/branch_5x from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6b0fb9 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)",0,0,0,0,0,,
186,"Commit a7f2876ec5ce9ca5ef271cad97027a5cb5e43619 in lucene-solr's branch refs/heads/branch_6_0 from David Smiley
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a7f2876 ]
LUCENE-7291: Fix spatial HeatmapFacetCounter bug with dateline and large non-point shapes
(cherry picked from commit 7520d79)",0,0,0,0,0,,
187,This patch changes the DocIdSetBuilder API.,0,1,0,0,0,,
188,add() is gone.,0,1,0,0,0,,
189,"Instead, grow() returns a new BulkAdder object that can be used to add up to the number of documents that have been passed to the grow() method.",0,1,0,0,0,,
190,This helps save conditionals since the add method never needs to care about whether the buffer is large enough or whether to upgrade to a bitset since everything is done up-front in the grow() call.,0,0,1,0,0,,
191,One possible downside to this change is that it changes a predictable branch (that is handled at the CPU level) into a method call... which if it's not monomorphic can be un-inlined at the point of the call and thus end up slower (method call vs predictable branch).,0,0,0,1,0,,
192,Will be interesting to see the benchmark results.,0,0,0,0,0,,
193,I benchmarked it using IndexAndSearchOpenStreetMaps by temporarily using DocIdSetBuilder instead of MatchingPoints (I did not use luceneutil since its numeric range queries match too many docs).,0,1,0,1,0,,
194,The QPS went from 33.4 (old DocIdSetBuilder.add) to 35.0 with this patch.,0,0,1,0,0,,
195,"In that case I think it works well since the base class is an abstract class and there are only two impls, which the JVM can efficiently optimize.",0,0,1,0,0,,
196,"(For the record, most queries of the benchmark upgrade to a bitset so both impls are used.)",0,0,1,0,0,,
197,"Ah, thanks for that reference... need to update my mental models",0,0,0,0,0,,
198,"Commit 95c360d053a35486aa12498c6fd319aef0377bb8 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=95c360d ]
LUCENE-7264: Fewer conditionals in DocIdSetBuilder.",0,0,0,0,0,,
199,"Commit aa81ba8642a57181a4eaa017b52d0d3c3462544b in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aa81ba8 ]
LUCENE-7264: Fewer conditionals in DocIdSetBuilder.",0,0,0,0,0,,
200,"Commit 14b42fe10ba64bb4468ea8ef298e54a751f16dd3 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=14b42fe ]
LUCENE-7264: Fix test bug in TestReqExclBulkScorer.",0,0,0,0,0,,
201,"Commit f7b333f10583639ee3d0f2631fee41c577c60452 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f7b333f ]
LUCENE-7264: Fix test bug in TestReqExclBulkScorer.",0,0,0,0,0,,
202,Manually correcting fixVersion per Step #S5 of LUCENE-7271,0,0,0,0,1,,
206,"Another failure, on 6.x:

Suite: org.apache.lucene.spatial3d.TestGeo3DPoint
   [smoker]    [junit4] IGNOR/A 0.01s J2 | TestGeo3DPoint.testRandomBig
   [smoker]    [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())
   [smoker]    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGeo3DPoint -Dtests.method=testRandomMedium -Dtests.seed=AB1C87AA82F2EF89 -Dtests.multiplier=2 -Dtests.locale=es-VE -Dtests.timezone=Africa/Niamey -Dtests.asserts=true -Dtests.file.encoding=UTF-8
   [smoker]    [junit4] FAILURE 2.55s J2 | TestGeo3DPoint.testRandomMedium <<<
   [smoker]    [junit4]    > Throwable #1: java.lang.AssertionError: FAIL: id=8110 should have matched but did not
   [smoker]    [junit4]    >   shape=GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}
   [smoker]    [junit4]    >   point=[X=-0.20200947969927532, Y=-0.4709330291599749, Z=0.8571474154523655]
   [smoker]    [junit4]    >   docID=7962 deleted?=false
   [smoker]    [junit4]    >   query=PointInGeo3DShapeQuery: field=point: Shape: GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=0.7742755548509384, lon=1.4555370543705286], [lat=-0.8402448215271041, lon=-3.1033465832913087]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.27564487296038953, lon=2.5713811980617303], [lat=-0.8678809123816704, lon=1.4382377289499255]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.8711205032246575, lon=-2.816839332961], [lat=-0.14706304488488503, lon=2.5605745305340144]], internalEdges={2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.8402448215271041, lon=-3.1033465832913087], [lat=-0.8678809123816704, lon=1.4382377289499255], [lat=0.018587409980192347, lon=0.2620880396809507], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 1, 4}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.8467492502622816, lon=1.0798875031514328], [lat=-0.14706304488488503, lon=2.5605745305340144], [lat=-0.8532322566987428, lon=-2.8534929144969876], [lat=-0.9594629553682933, lon=-2.2151935508754383]], internalEdges={0, 2}, holes=[GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.5066843301617248, lon=0.8615401840455207], [lat=1.0964870141435539, lon=2.5943881214521642], [lat=-0.8070142540769046, lon=-0.5601543821665136]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=1.254943600040169, lon=-2.455701672749609], [lat=0.8399838756142208, lon=-0.6197091219150923], [lat=0.6595204508888761, lon=-0.0777586651449774]], internalEdges={}, holes=[]}]}, GeoCompositeMembershipShape: {[GeoConvexPolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=1.0804215040880678, lon=-2.120999400452007], [lat=0.8759401563733757, lon=-2.0560515158834556]], internalEdges={2}, holes=[]}, GeoConcavePolygon: {planetmodel=PlanetModel.WGS84, points=[[lat=0.9975360448095447, lon=-1.947231901773322], [lat=0.8759401563733757, lon=-2.0560515158834556], [lat=1.0593274653908336, lon=-1.9878461917050225]], internalEdges={0}, holes=[]}]}]}]}
   [smoker]    [junit4]    >   explanation:
   [smoker]    [junit4]    >     target is in leaf _1(6.1.0):C20016 of full reader StandardDirectoryReader(segments:5:nrt _1(6.1.0):C20016)
   [smoker]    [junit4]    >     full BKD path to target doc:
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.01197272304385482 TO 0.35595394953475784); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-1.0010902294672446 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.3559539490685756 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.6956355809209895); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.6956355813871717 TO 0.10900276736191666); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.10900276689573439 TO 0.7189540192817846); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.6330869561449444 TO -0.3492819569300436 y=-1.0007728907882345 TO -0.0014295405465486804 z=0.7189540188156024 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.68605594687831 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-0.6860559473444922 TO -0.32059454789968617 z=-0.9977622808698339 TO -0.0069162292381555225); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=-0.006916229704337817 TO 0.6718162758370178); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true
   [smoker]    [junit4]    >       Cell(x=-0.3492819573962258 TO 0.006012160982458459 y=-1.0007728907882345 TO -0.32059454789968617 z=0.6718162753708355 TO 0.9977622920582089); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true
   [smoker]    [junit4]    >     on cell Cell(x=-1.0010902294672446 TO -0.7975667675246342 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = DISJOINT; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_OUTSIDE_QUERY  on cell Cell(x=-0.7975667679908164 TO -0.6330869556787622 y=-1.0007728907882345 TO -0.0014295405465486804 z=-0.9977622808698339 TO -0.011972722577672528); Shape relationship = OVERLAPS; Point within cell = false; Point within shape = true, wrapped visitor returned CELL_CROSSES_QUERY
   [smoker]    [junit4]    > 	at __randomizedtesting.SeedInfo.seed([AB1C87AA82F2EF89:16C2B002C3978CEF]:0)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.verify(TestGeo3DPoint.java:796)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.doTestRandom(TestGeo3DPoint.java:518)
   [smoker]    [junit4]    > 	at org.apache.lucene.spatial3d.TestGeo3DPoint.testRandomMedium(TestGeo3DPoint.java:445)
   [smoker]    [junit4]    > 	at java.lang.Thread.run(Thread.java:745)
   [smoker]    [junit4]   2> NOTE: test params are: codec=Asserting(Lucene60): {id=Lucene50(blocksize=128)}, docValues:{id=DocValuesFormat(name=Memory)}, maxPointsInLeafNode=421, maxMBSortInHeap=6.865543172299073, sim=ClassicSimilarity, locale=es-VE, timezone=Africa/Niamey
   [smoker]    [junit4]   2> NOTE: Linux 3.13.0-52-generic amd64/Oracle Corporation 1.8.0_74 (64-bit)/cpus=4,threads=1,free=292169440,total=351797248
   [smoker]    [junit4]   2> NOTE: All tests run in this JVM: [TestGeo3DPoint]",1,0,0,0,0,,
207,"Looking at the first failure, I have a simple unit test that demonstrates it, with some forensics in place:


   [junit4] Suite: org.apache.lucene.spatial3d.geom.GeoPolygonTest
   [junit4]   2>  Edge point [X=0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()?",1,0,0,0,0,,
208,"false
   [junit4]   2>  Edge point [X=-0.17319681511746315, Y=0.007099080851812687, Z=0.9826918170382273] path.isWithin()?",0,0,0,0,0,,
209,"false
   [junit4]   2>  Edge point [X=0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()?",0,0,0,0,0,,
210,"false
   [junit4]   2>  Edge point [X=-0.08178122448583161, Y=0.007099080851812687, Z=0.9944024015823575] path.isWithin()?",0,0,0,0,0,,
211,"false
   [junit4]   2>  Edge point [X=-0.17334224446387358, Y=0.0, Z=0.9826918170382273] path.isWithin()?",0,0,0,0,0,,
212,"false
   [junit4]   2>  Edge point [X=-0.08208876675491342, Y=0.0, Z=0.9944024015823575] path.isWithin()?",0,0,0,0,0,,
213,"false
   [junit4]   2>  path edge point [lat=0.022713796927720124, lon=-0.5815768716211268] isWithin()?",0,0,0,0,0,,
214,"false minx=1.837423958880354 maxx=-0.16451112733444273 miny=0.4509495179589238 maxy=-0.5569131482163469 minz=-0.9599546014156208 maxz=-0.9716651859597512
   [junit4] FAILURE 0.04s | GeoPolygonTest.testPolygonFactoryCase2 <<<
   [junit4]    > Throwable #1: java.lang.AssertionError
   [junit4]    > 	at org.apache.lucene.spatial3d.geom.GeoPolygonTest.testPolygonFactoryCase2(GeoPolygonTest.java:480)


Basically what this means is that nothing seems to be inside anything else – and yet there are no edge intersections anywhere either.",1,0,0,0,0,,
215,I'm wondering if this is a result of granularity?,1,0,0,0,0,,
216,"Theoretically, it is possible for a point that is not on the surface to be inside an XYZSolid and come up as inside the shape as well, even if there is no overlap between the solid and the surface shape, because the shape is described by planes that go through the center of the earth and are therefore not perpendicular to the XYZSolid's boundaries.",1,0,0,0,0,,
217,"It does seem odd that all failures we've seen have been for very complex polygons, though.",1,0,0,0,0,,
218,I'll verify that all polygon edges that should be checked for intersection actually are being checked before resorting to that explanation.,1,0,0,0,0,,
219,"I verified that the shape appears to be properly built, with the right internal edges, and that when looking for edge intersections it examines the correct number and type of edges.",0,0,1,0,0,,
220,The last thing to verify is whether the edge points for the XYZSolid are correct.,1,0,0,0,0,,
221,Added more diagnostics to figure out if scaling the point to the surface would result in it leaving the cell.,0,1,0,0,0,,
222,"Short answer: no:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true; Scaled point within cell = true; Scaled point within shape = true


So something is definitely wrong with the intersection logic, but I'm not yet clear what that is.",0,1,0,1,0,,
223,"When trying to figure out which part of the five polygon tiles that the point was in, I discovered something odd about this poly:


   [junit4]   2> Is point within p1 section?",1,0,0,0,0,,
224,"false
   [junit4]   2> Is point within p2 section?",0,0,0,0,0,,
225,"true
   [junit4]   2> Is point within p3 section?",0,0,0,0,0,,
226,"false
   [junit4]   2> Is point within concave section?",0,0,0,0,0,,
227,"false
   [junit4]   2> Is point within p5 section?",0,0,0,0,0,,
228,"true


The point is within TWO parts of the composite polygon.",1,0,0,0,0,,
229,"This is, of course, nonsense unless the point happens to be on a shared edge between the two.",0,0,0,1,0,,
230,But these two don't even share an edge.,1,0,0,0,0,,
231,I'm forced to conclude that the polygon itself is broken in some subtle way that is undetected at the time the poly is being constructed.,1,0,0,0,0,,
232,I'll look at that next.,0,0,0,0,0,,
233,"Ok, found the problem.",0,0,0,0,0,,
234,Cut and paste error in complex polygon building.,1,0,0,0,0,,
235,"Resolves the first problem, confirming the second.",0,0,0,0,1,,
236,"Commit d377e7fd34b4ace829ee6d4ba0486500aaef506b in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d377e7f ]
LUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.",0,0,0,0,0,,
237,"Commit 65a69b9757a6cd0a8e8be0ed08797643054634b1 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=65a69b9 ]
LUCENE-7197: Fix two test failures and add more forensics that helped resolve the issue.",0,0,0,0,0,,
203,"I augmented Michael McCandless's explain output to include relationship information, which then leads to the following line in the forensics dump from this failure:


   [junit4]    >       Cell(x=-1.0011088352687925 TO 1.0008262509460042 y=-1.000763585323458 TO 0.007099080851812687 z=0.9826918170382273 TO 0.9944024015823575); Shape relationship = DISJOINT; Point within cell = true; Point within shape = true


From this it looked plausible that the cell is off the top of the world in z, and thus did not intersect with it for that reason.",1,0,0,0,0,,
204,"But:


   [junit4]    >   world bounds=( minX=-1.0011188539924791 maxX=1.0011188539924791 minY=-1.0011188539924791 maxY=1.0011188539924791 minZ=-0.9977622920221051 maxZ=0.9977622920221051


So I'll have to work a bit harder to see why no intersection is detected.",1,0,0,0,0,,
205,Yay forensics!,0,0,0,0,0,,
238,"+1, we should roll our own, hopefully correctly this time",0,0,1,0,1,,
239,It does seem buggy that even StrictMath shows this difference between versions.,1,0,0,0,0,,
240,WTF is the point of StrictMath then?,1,0,0,0,0,,
241,"This is a simple multiplication, computed using two well-known constants: pi and 180.0.",0,0,0,0,0,,
242,How can this possibly be inexact??,1,0,0,0,0,,
243,See the linked openjdk issue.,0,0,0,0,0,,
244,Looks like they changed it from division to inverse multiplication.,0,1,0,0,0,,
245,IMO at least StrictMath should not have this optimization...,0,0,0,1,0,,
246,"IMO at least StrictMath should not have this optimization...
+1 for that.",0,0,1,0,0,,
247,"StrictMath doesn't say anything about being constant in time, all it says is, basically:
To help ensure portability of Java programs, the definitions of some of the numeric functions in this package require that they produce the same results as certain published algorithms.",0,1,1,0,0,,
248,[...] The Java math library is defined with respect to fdlibm version 5.3.,0,0,0,0,0,,
249,"And fdlibm doesn't have these conversion methods, so it's not violating its spec?",0,0,0,0,0,,
250,Then why does StrictMath.java have a separate toRadians method at all with strictfp?,1,0,0,0,0,,
251,That's what I'm saying – I don't know!,0,0,0,0,0,,
252,"And seriously, I think it indeed misses the point: if StrictMath does have this method and the reference (fdlibm) doesn't have it then it should at least stick to identical implementation.",0,1,0,0,0,,
253,I would file a bug.,0,0,0,0,0,,
254,strictfp is another issue,"issue, con",1,0,0,1,0,
255,"I recall we did hit it once (a long time ago) when we tested on sparcs, amds and intels in parallel – we had reference results of matrix computations (in high precision) and there was some inaccuracies in tiny digits.",1,0,0,0,0,,
256,"I know what it does, i am saying it makes no sense to have the method there at all if its result may change.",0,0,0,1,0,,
257,"Commit a4bf526a62dbf5e2c3fed6d98112c71ed33e15d6 in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a4bf526 ]
LUCENE-7194: Roll our own toRadians() method, and also make it less likely we'll need to restaple the toString() tests.",0,0,0,0,0,,
258,"Commit f6be813308133de06e08717309750e1f47dd73d1 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6be813 ]
LUCENE-7194: Roll our own toRadians() method, and also make it less likely we'll need to restaple the toString() tests.",0,0,0,0,0,,
259,+1 to rolling our own in any case.,0,0,1,0,0,,
260,Committed changes to the spatial3d module for this purpose.,0,0,0,0,1,,
261,Not sure where else it's used?,1,0,0,0,0,,
262,Thanks Karl Wright.,0,0,0,0,0,,
263,"I think we should just add this to our forbidden API list, then see what fails (because it's using these APIs), and correct them...",0,1,0,0,0,,
264,Michael McCandless How do I add these to the forbidden API list?,0,0,0,0,0,,
265,"Karl Wright Oh, I think you just add it to lucene/tools/forbiddenApis/lucene.txt?",0,0,0,0,0,,
266,And then run ant precommit and you should see failures from places using these APIs...,0,0,0,0,0,,
267,"Here's what it spits out:


[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:94)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:95)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:121)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:151)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toDegrees(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.geo.Rectangle (Rectangle.java:169)
[forbidden-apis] Forbidden method invocation: java.lang.Math#toRadians(double) [Use NIO.2 instead]
[forbidden-apis]   in org.apache.lucene.util.SloppyMath (SloppyMath.java:212)
[forbidden-apis] Scanned 2733 (and 585 related) class file(s) for forbidden API
invocations (in 2.98s), 9 error(s).",0,0,0,0,0,,
268,Robert Muir: Is this still needed?,0,0,0,0,0,,
269,"In SloppyMath.java I see the following:


  // haversin
  // TODO: remove these for java 9, they fixed Math.toDegrees()/toRadians() to work just like this.",1,0,0,0,0,,
270,"public static final double TO_RADIANS = Math.PI / 180D;
  public static final double TO_DEGREES = 180D / Math.PI;


... which leads me to wonder if Java 9 was fixed and we should instead be using Math.toDegrees()/toRadians() everywhere?",0,1,0,0,0,,
271,Maybe Uwe Schindler knows?,0,0,0,0,0,,
272,Here's the patch,0,1,0,0,0,,
273,"Commit b11e48c7553daed127b1b231641d7367a09aed1b in lucene-solr's branch refs/heads/master from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b11e48c ]
LUCENE-7194: Ban Math.toRadians and Math.toDegrees",0,0,0,0,0,,
274,"Commit 40ca6f1d64ab8ec2e873c2a6c6815ca449b046a4 in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=40ca6f1 ]
LUCENE-7194: Ban Math.toRadians and Math.toDegrees",0,0,0,0,0,,
275,"Commit 388d388c990c8d9e05ec9ba9bc881fdc921e734b in lucene-solr's branch refs/heads/branch_6x from Karl Wright
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=388d388 ]
LUCENE-7194: Ban Math.toRadians, Math.toDegrees",0,0,0,0,0,,
276,Thank you Karl Wright!,0,0,0,0,0,,
277,"Yikes, I'll take it.",0,0,0,0,0,,
278,Patch.,0,1,0,0,0,,
279,I also hit and fixed a separate bug in InetAddressPoint.newSetQuery where it hit an exception if you tried to pass more than one InetAddress in the set   Sheesh.,0,0,0,0,1,,
280,s/comparsion/comparison/ but otherwise +1 to the patch,0,0,1,1,0,,
281,nice find!,0,0,0,0,0,,
282,"Commit 770e508fd3d908e9bf7997361299af96aa437b75 in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=770e508 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal",0,0,0,0,0,,
283,"Commit 9f8fe1239afb7089b9f85432d076bdd778d3cd50 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9f8fe12 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal",0,0,0,0,0,,
284,"Commit f0ed113bb6ebed008bc9aa5954e12de98d62c951 in lucene-solr's branch refs/heads/branch_6_0 from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f0ed113 ]
LUCENE-7085: PointRangeQuery.equals sometimes returns false even if queries were in fact equal",0,0,0,0,0,,
285,"What a nice catch, thanks Adrien Grand!",0,0,0,0,0,,
318,I just want to be sure that maybe others also tested it with his/her Git installation.,0,0,0,0,0,,
319,"Commit 424a647af4d093915108221bcd4390989303b426 in lucene-solr's branch refs/heads/master from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=424a647 ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change",0,0,0,0,0,,
320,"Commit 0ef36fcdd107084a2ac3156943f01eb5f7dd9c74 in lucene-solr's branch refs/heads/branch_5x from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0ef36fc ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change",0,0,0,0,0,,
321,I committed this.,0,0,0,0,1,,
322,I think we should open another issues about the multiple build dir change.,1,0,0,0,0,,
323,"Commit b0e769c3ec598dd7398cc8df123bc4c41069e2c3 in lucene-solr's branch refs/heads/branch_5_4 from Uwe Schindler
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b0e769c ]
LUCENE-6995, LUCENE-6938: Add branch change trigger to common-build.xml to keep sane build on GIT branch change",0,0,0,0,0,,
286,Patch.,0,1,0,0,0,,
287,this is a really good idea.,0,0,1,0,0,,
288,"otherwise we are going to get bug reports that look like data corruption:

   [junit4]    > Caused by: java.lang.IllegalArgumentException: Version is too old, should be at least 2 (got 0)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.checkVersion(PackedInts.java:77)
   [junit4]    >     at org.apache.lucene.util.packed.PackedInts.getDecoder(PackedInts.java:742)
   [junit4]    >     at org.apache.lucene.codecs.lucene50.ForUtil.<clinit>(ForUtil.java:64)
   [junit4]    >     ... 50 more",0,0,0,1,0,,
289,This is interesting because I don't see this behavior.,0,0,0,0,0,,
290,"When I checkout a historical branch, it just updates timestamps on changed files in the working copy, but updates them to checkout time?",0,0,0,0,0,,
291,"$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 35579 Jan 26 14:55 build.xml

# historical hash from Sep. 5:
$ git checkout 8f5259b4ff2d5f0
$ ls -l build.xml
-rw-r--r-- 1 dweiss dweiss 25633 Jan 26 14:56 build.xml",0,0,0,0,0,,
292,"I think it's your build folders that are in an insane state, not the checked out files?",0,0,0,0,0,,
293,"I'm not against this patch, I just find it odd you're experiencing the issue because I've never had any problem with it (yes, git will not wipe out any ignored files automatically – these are ignored after all – but it'll switch any versioned files and update timestamps properly).",0,0,0,1,0,,
294,Please make this an opt-out feature.,0,1,0,0,0,,
295,"I prefer to git clean the build folder myself, actually (faster than ant).",0,0,1,0,0,,
296,It happened to me twice this morning already.,0,0,0,0,0,,
297,Just doing things like running tests and forgetting to clean when switching branches.,0,0,0,0,0,,
298,"Ok, but this isn't related to timestamps on version-controlled files, only on the fact that build artefacts coexist?",0,0,0,0,0,,
299,"If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder.",0,1,1,0,0,,
300,This way you could actually switch a branch and then continue working as usual because it should work flawlessly.,0,0,1,0,0,,
301,"...have branch-dedicated build folder
I like that idea!",0,0,1,0,0,,
302,I guess it's manually possible now with -Dbuild.dir=my_branch_dir?,0,1,0,0,0,,
303,"If so, it'd be an interesting exercise to not clean after a branch switch but have branch-dedicated build folder.",0,1,1,0,0,,
304,This way you could actually switch a branch and then continue working as usual because it should work flawlessly.,0,0,1,0,0,,
305,This is an interesting idea.,0,0,1,0,0,,
306,"But if we do this, I think it should be `build/<branch>` and ant clean still removes build entirely (means it still really cleans).",0,1,0,0,0,,
307,"I guess it will force us to fix places in the build/python scripts/whatever that might have a hardcoded `build/` somewhere, and so on.",0,1,0,0,0,,
308,"Yes, absolutely.",0,0,1,0,0,,
309,Ant clean should really clean everything.,0,1,1,0,0,,
310,"As a side note, I use this very often:


git clean -xfd


This removes everything (including all ignored files, etc.)",0,1,1,0,0,,
311,and restores a pristine checkout state.,0,0,1,0,0,,
312,Just in case somebody finds it useful.,0,0,0,0,0,,
313,+1 to close this trap ASAP.,0,0,1,0,0,,
314,The errors you hit when ant doesn't realize you've switched git branches are awful.,1,0,0,0,0,,
315,"Can we iterate by committing this solution first, then working on the separate build directory as a followup.",0,1,0,0,0,,
316,"Again, the current situation is a real problem, because the errors you see look like corruption.",1,0,0,0,0,,
317,I have no problem committing and pushing this soon.,0,0,0,0,0,,
324,Patch of 11 Nov 2015.,0,1,0,0,0,,
325,"Most of the changes are to pass numDocs down to where it is actually used:
ConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.",0,1,0,0,0,,
326,"This is incomplete, there no tests.",0,0,0,1,0,,
327,MinShouldMatchSumScorer only has the disjunctions done.,0,1,1,0,0,,
328,"For un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.",0,1,0,0,0,,
329,"SpanContaining and SpanWithin use the conjunction estimation, these should also be smaller.",0,1,0,0,0,,
330,The independence that is assumed is normally not there.,0,0,0,1,0,,
331,"However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.",0,0,1,0,0,,
332,But so would the current worst-case approach?,1,0,0,0,0,,
333,That one is actually solved nowadays by the two phase approach.,0,1,0,0,0,,
334,I'll think of a better example later.,0,0,0,0,0,,
335,"There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates.",0,1,1,0,0,,
336,"However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.",0,0,0,1,0,,
337,"It comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries.",0,0,1,1,0,,
338,"However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries.",0,1,1,0,0,,
339,"Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.",0,1,1,0,0,,
340,I think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.,0,1,0,0,0,,
341,"For the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.",0,0,1,0,0,,
342,Would it be worthwhile to open a separate issue for that?,0,0,0,0,0,,
343,Patch of 15 Nov 2015.,0,1,0,0,0,,
344,Resolve conflicts after LUCENE-6276.,0,0,0,0,1,,
345,I tried this patch with the wikimedium5m benchmark.,0,1,0,0,0,,
346,"This showed no significant differences to current trunk, the differences where never bigger than 2.1\% either way, and well within the standard deviations.",0,0,0,1,0,,
347,This could be because the patch here should have influence for more complex queries than the one in the benchmark.,0,0,0,0,0,,
348,I might try to add more complex queries to the benchmark later.,0,1,0,0,0,,
349,"Here is the benchmark output, it might be good for future reference:


                    TaskQPS baseline      StdDevQPS my_modified_version      StdDev                Pct diff
                HighTerm      178.20      (1.8\%)      174.50      (5.6\%)   -2.1\% (  -9\% -    5\%)
                 MedTerm      641.36      (1.6\%)      630.32      (4.9\%)   -1.7\% (  -8\% -    4\%)
              OrHighHigh       57.02      (5.5\%)       56.32      (6.6\%)   -1.2\% ( -12\% -   11\%)
               OrHighMed      107.80      (5.2\%)      106.89      (6.2\%)   -0.8\% ( -11\% -   11\%)
             AndHighHigh      100.02      (2.3\%)       99.34      (0.7\%)   -0.7\% (  -3\% -    2\%)
                 LowTerm     2477.28      (3.0\%)     2463.27      (5.5\%)   -0.6\% (  -8\% -    8\%)
              AndHighMed      627.58      (1.5\%)      625.22      (1.2\%)   -0.4\% (  -3\% -    2\%)
              HighPhrase       81.21      (4.2\%)       80.98      (4.3\%)   -0.3\% (  -8\% -    8\%)
               OrHighLow      136.70      (3.1\%)      136.35      (2.1\%)   -0.3\% (  -5\% -    5\%)
               LowPhrase      181.55      (2.2\%)      181.09      (2.0\%)   -0.3\% (  -4\% -    4\%)
         MedSloppyPhrase       56.03      (2.9\%)       55.93      (3.1\%)   -0.2\% (  -5\% -    5\%)
             MedSpanNear       52.77      (1.7\%)       52.68      (2.6\%)   -0.2\% (  -4\% -    4\%)
         LowSloppyPhrase      106.15      (2.9\%)      106.01      (3.1\%)   -0.1\% (  -6\% -    6\%)
               MedPhrase       39.38      (3.8\%)       39.36      (3.3\%)   -0.1\% (  -6\% -    7\%)
                  Fuzzy1      137.14      (2.1\%)      137.06      (1.5\%)   -0.1\% (  -3\% -    3\%)
                  Fuzzy2       79.28      (1.9\%)       79.25      (1.5\%)   -0.0\% (  -3\% -    3\%)
             LowSpanNear       94.38      (1.7\%)       94.35      (2.8\%)   -0.0\% (  -4\% -    4\%)
            OrNotHighMed      444.12      (1.7\%)      444.36      (1.2\%)    0.1\% (  -2\% -    2\%)
              AndHighLow     1878.59      (2.0\%)     1880.20      (1.9\%)    0.1\% (  -3\% -    4\%)
                 Respell      106.47      (1.9\%)      106.62      (1.7\%)    0.1\% (  -3\% -    3\%)
            OrNotHighLow     1831.85      (1.7\%)     1834.68      (1.3\%)    0.2\% (  -2\% -    3\%)
           OrNotHighHigh       69.75      (1.6\%)       69.91      (1.4\%)    0.2\% (  -2\% -    3\%)
            HighSpanNear       36.38      (2.8\%)       36.47      (3.8\%)    0.3\% (  -6\% -    7\%)
        HighSloppyPhrase       45.58      (3.6\%)       45.70      (3.5\%)    0.3\% (  -6\% -    7\%)
            OrHighNotLow       65.78      (7.0\%)       66.03      (8.4\%)    0.4\% ( -14\% -   16\%)
                 Prefix3      448.85      (3.5\%)      450.67      (3.8\%)    0.4\% (  -6\% -    8\%)
                Wildcard      114.35      (4.8\%)      115.02      (4.6\%)    0.6\% (  -8\% -   10\%)
                  IntNRQ       23.48      (7.4\%)       23.71      (7.7\%)    1.0\% ( -13\% -   17\%)
                PKLookup      360.70      (1.7\%)      364.91      (3.1\%)    1.2\% (  -3\% -    6\%)
            OrHighNotMed      178.99      (7.2\%)      181.91      (8.2\%)    1.6\% ( -12\% -   18\%)
           OrHighNotHigh       39.78      (7.1\%)       40.63      (7.5\%)    2.1\% ( -11\% -   18\%)",0,0,0,0,0,,
363,Won't this change have the prospect of increasing the amount of GC due to all these extra objects?,0,0,0,1,0,,
364,"The patch also removes some calls to BytesRef.deepCopyOf, so I'm not sure here.",0,0,1,0,0,,
365,"have an alternative constructor that doesn't clone
This has bitten a few people at least, including me, and I'd rather have it work correctly in the case when the BytesRef comes directly from a TermsEnum.",0,1,0,0,0,,
366,"The Term(String field, String text) constructor makes a new BytesRef anyway.",0,1,0,0,0,,
367,so that users like Solr can exploit the fact that their code won't be making any further use of the input term?,0,0,1,0,0,,
368,"There are at least a few places in Solr with a BytesRef.deepCopyOf result directly passed to a Term constructor, for example in FacetField and SolrIndexSearcher.",0,1,0,0,0,,
369,There this call could be removed also.,0,1,0,0,0,,
370,"I'm not familiar with Solr code, so in case there is another impact there, please holler.",0,0,0,0,0,,
371,2nd patch of 3 October 2015.,0,1,0,0,0,,
372,"In addition to the previous patch, this also

deletes the Term cloning in PhraseQuery,
adds a Term constructor from a BytesRefBuilder, and
removes BytesRef copying at Term construction from Solr's FieldType, SolrIndexSearcher, FacetField and SimpleMLTQParser.",0,1,0,0,0,,
373,I like how the patch makes things simpler.,0,0,1,0,0,,
374,I'll wait a bit before committing to give other people a chance to comment.,0,0,0,0,0,,
375,Can you also remove the explicit cloning that we added in LUCENE-6435?,0,1,0,0,0,,
376,Can you also remove the explicit cloning that we added in LUCENE-6435?,0,1,0,0,0,,
377,"I tried, but then a test fails.",0,0,0,1,0,,
378,Perhaps this is because a BytesRef is passed ClassificationResult there.,0,0,0,1,0,,
379,do you mean the BytesRef.deepCopyOf at https://github.com/apache/lucene-solr/blob/trunk/lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java#L154 ?,0,0,0,0,0,,
380,"If yes, that's because the reference is updated and used in the ClassificationResult.",0,0,0,1,0,,
381,I'll see if I can simplify that.,1,0,0,0,0,,
382,"after a quick look it doesn't seem removing the deep copy in favour of creating new BytesRef would improve anything, actually it'd be slightly worse.",0,1,0,1,0,,
383,I would say let's keep that.,0,1,0,0,0,,
384,"One could also create the Term in the loop and pass that, or its Term.bytes(), around to the other methods.",0,1,0,0,0,,
385,Term.bytes() can also be passed to the ClassificationResult.,0,1,0,0,0,,
386,"The patch here has this javadoc at Term.bytes():
/** Returns the bytes of this term, these should not be modified.",0,1,0,0,0,,
387,*/,0,0,0,0,0,,
388,"One could also create the Term in the loop and pass that
good point indeed, in the end the underlying methods all create a new Term from the same BytesRef and field name, so that should be better than the current solution, so we should pass the Term created from within the loop to the methods to calculate prior and likelihood in SimpleNaiveBayesClassifier.",0,1,1,0,0,,
389,attached the Paul Elschot's patch modified to pass the Term instead of the BytesRef in SimpleNaiveBayesClassifier.,0,1,0,0,0,,
390,from my perspective we can proceed committing this patch.,0,0,0,0,1,,
391,"Adrien, Paul what do you think?",0,0,0,0,0,,
392,"The patch of 14 October LGTM, and tests pass here.",0,1,1,0,0,,
393,+1 on my end as well,0,0,1,0,0,,
394,I will run another round of testing and inspections and commit the latest patch if no issues come up.,0,0,0,0,1,,
395,"Commit 1709576 from Tommaso Teofili in branch 'dev/trunk'
[ https://svn.apache.org/r1709576 ]
LUCENE-6821 - TermQuery's constructors should clone the incoming term",0,0,0,0,0,,
396,"committed and resolved, thanks Paul Elschot for your patch and Adrien Grand for your help.",0,0,0,0,1,,
397,Do we want to backport it to 5.x?,0,1,0,0,0,,
398,"I don't have a strong opinion but I like to get changes into the hands of our users as soon as possible, and I don't see how this one could break existing applications, it might just perform a few extra copies?",0,0,1,0,0,,
399,Do we want to backport it to 5.x?,0,1,0,0,0,,
400,"+1, I consider this a bug fix, which should certainly go back to 5.x.",0,0,1,0,0,,
401,"agreed, I've reopened and will backport it.",0,0,1,0,1,,
402,"Commit 1709683 from Yonik Seeley in branch 'dev/trunk'
[ https://svn.apache.org/r1709683 ]
LUCENE-6821: remove unnecessary term clones",0,0,0,0,0,,
403,"Commit 1709780 from Tommaso Teofili in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1709780 ]
LUCENE-6821 - TermQuery's constructors should clone the incoming term (backport branch 5.x)",0,0,0,0,0,,
404,backported to branch 5.x,0,0,0,0,1,,
350,See (also?),0,0,0,0,0,,
351,LUCENE-4483.,0,0,0,0,0,,
352,"I had a look at the core code for the use of the TermQuery constructors, and I agree that it would be better to do the clone in the constructor.",0,1,1,0,0,,
353,Shall I give this a try?,0,1,0,0,0,,
354,Patch of 3 Oct 2015.,0,1,0,0,0,,
355,"This

adds a call to BytesRef.deepCopyOf in the Term constructor,
removes such calls where the Term constructor is used, and
documents that the result of Term.bytes() should not be modified.",0,1,0,0,0,,
356,"The patch also removes a call to the term constructor in BlendedTermQuery, which was actually making a clone.",0,1,0,0,0,,
357,"This might have gone too far, but I think it should work because the boost is in BoostQuery now.",0,0,1,0,0,,
358,Won't this change have the prospect of increasing the amount of GC due to all these extra objects?,0,1,1,0,0,,
359,Maybe might it be advisable to have an alternative constructor that doesn't clone so that users like Solr can exploit the fact that their code won't be making any further use of the input term?,0,1,1,0,0,,
360,"I don't think we should bother at all: executing a term query already performs I/O operations and allocates several objects per segment to create terms enums, scorers, iterators, leaf collectors, etc.",0,1,0,1,0,,
361,so adding two extra object allocations to clone the incoming term is very unlikely to have noticeable impact on gc activity.,0,1,0,1,0,,
362,"There is another Term constructor call that is a clone at PhraseQuery line 104, this could also be removed.",0,1,0,0,0,,
405,Initial patch fixing the comparator to take into account the payload when everything else is same.,0,1,0,0,0,,
406,"Commit 1691282 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1691282 ]
LUCENE-6680: don't lose a suggestion that differs only in payload from another suggestion",0,0,0,0,0,,
407,"Thanks Arcadius Ahouansou, I just committed your patch with a small tweak to the if statement logic in the comparator...",0,1,0,0,1,,
408,"Commit 1691283 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1691283 ]
LUCENE-6680: don't lose a suggestion that differs only in payload from another suggestion",0,0,0,0,0,,
409,Thank you very much Michael McCandless for helping get this issue fixed.,0,0,0,0,0,,
410,Bulk close for 5.3.0 release,0,0,0,0,1,,
411,"We should define a complete standard
Lucene uses Sun's java coding conventions, apparently moved here: http://www.oracle.com/technetwork/java/codeconvtoc-136057.html
With one exception: 2 space indent, not 4.",0,1,0,0,0,,
412,Currently by default the Codestyle is consistent with spaces.,0,0,1,0,0,,
413,The problem was actually with un-consistent  classes already committed that were causing the confusion.,1,0,0,0,0,,
414,We can close this now.,0,0,0,0,1,,
415,"And thanks Mike for the explanation, actually I missed the comments !",0,0,0,0,0,,
416,You're welcome Alessandro Benedetti!,0,0,0,0,0,,
417,Thank you for raising this.,0,0,0,0,0,,
418,Patch guards the Files.createDirectories call with a Files.exists check.,0,1,0,0,0,,
419,"Files.exists only demands read access, so it succeeds if you only have read access.",0,0,1,0,0,,
420,And your second patch: Please don't do this.,0,0,0,1,0,,
421,This is all not worth a test.,0,0,0,1,0,,
422,It just compromises our security manager.,0,0,0,1,0,,
423,A SecurityManager that allows to override/delete itsself just makes itsself broken.,0,0,0,1,0,,
424,"Yes, if you really want to do this in a test:
Guideline 9-4 / ACCESS-4: Know how to restrict privileges through doPrivileged
http://www.oracle.com/technetwork/java/seccodeguide-139067.html#9",0,1,0,0,0,,
425,Good tip!,0,0,1,0,0,,
426,I have the test working here.,0,0,0,0,0,,
427,Will post updated patch.,0,1,0,0,0,,
428,Here is the test.,0,1,0,0,0,,
429,"I had to f*ck with the last Permission, because to run the code with restricted permissions you need the additional permission, otherwise it runs with no permissions at all  (see javadocs)
I also added a helper method, we may move this to LTC later",0,1,0,0,0,,
430,Slightly improved generics in the patch to make the helper method more universal (to move to LTC).,0,1,1,0,0,,
431,"I also added a helper method, we may move this to LTC later
+1
This patch looks great Uwe, generalizing that method into LTC sounds good to me.",0,0,1,0,0,,
432,More neat helper method (Java 8).,0,0,1,0,0,,
433,I will blow this up to ten times its size when backporting to 5.x,0,0,0,0,0,,
434,"This patch looks great Uwe, generalizing that method into LTC sounds good to me.",0,0,1,0,0,,
435,Will look into this tomorrow.,0,0,0,0,0,,
436,"Have to sleep now, its already 4 am.",0,0,0,0,0,,
437,The time when Mike McCandless starts coding,0,0,0,0,0,,
438,Moved the runWithLowerPermissions to LTC.,0,0,0,0,1,,
439,"I also fixed the FSDirectory() ctor to do a mich simpler Files.isDirectory() check, because otherwise the basic TestDirectory test fails on Windows (of couse it fails...).",0,1,1,0,0,,
440,Could somebody with Linux or MacOSX test the patch or state any other complaints with it?,0,0,0,0,0,,
441,More tests that we cannot escape our sandbox!,0,0,0,0,0,,
442,"After discussion with Hoss Man, I added an assume to the runWithRestrictedPermissions, so it cancels test execution if no security manager is available.",0,1,0,0,0,,
443,"Running those tests without a security manager makes no sense, because they would assert nothing (because they have all permissions).",0,0,0,1,0,,
444,"Commit 1688537 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1688537 ]
LUCENE-6542: FSDirectory's ctor now works with security policies or file systems that restrict write access",0,0,0,0,0,,
445,"Commit 1688541 from Uwe Schindler in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1688541 ]
Merged revision(s) 1688537 from lucene/dev/trunk:
LUCENE-6542: FSDirectory's ctor now works with security policies or file systems that restrict write access",0,0,0,0,0,,
446,Committed and backported the lambdas to Java 7 in branch_5x.,0,0,0,0,1,,
447,Bulk close for 5.3.0 release,0,0,0,0,1,,
448,Here is a patch.,0,1,0,0,0,,
449,1,0,0,1,0,0,,
450,"Commit 1683734 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1683734 ]
LUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.",0,0,0,0,0,,
451,"Commit 1683744 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1683744 ]
LUCENE-6526: Revert some changes that were committed by mistake.",0,0,0,0,0,,
452,"Commit 1683745 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1683745 ]
LUCENE-6526: Asserting(Query|Weight|Scorer) now ensure scores are not computed if they are not needed.",0,0,0,0,0,,
453,Bulk close for 5.3.0 release,0,0,0,0,1,,
454,"Commit 1672281 from Dawid Weiss in branch 'dev/trunk'
[ https://svn.apache.org/r1672281 ]
LUCENE-6413: Test runner should report the number of suites completed/ remaining.",0,0,0,0,0,,
455,"Commit 1672282 from Dawid Weiss in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1672282 ]
LUCENE-6413: Test runner should report the number of suites completed/ remaining.",0,0,0,0,0,,
456,Woohoo!,0,0,1,0,0,,
457,Thanks Dawid!,0,0,0,0,0,,
458,You're very welcome although I am so far behind with other changes I'd like to make to the RR that there's hardly any reason to celebrate  (yet .,0,0,0,0,0,,
459,This is awesome.,0,0,1,0,0,,
460,"It won't make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.",0,0,1,0,0,,
461,Bulk close for 5.3.0 release,0,0,0,0,1,,
462,Here is an initial shotgun approach.,0,1,0,0,0,,
463,I verified it finds things like file leaks by introducing them to old codecs.,0,1,0,0,0,,
464,But it currently sometimes trips assertions during IW.commit().,1,0,0,0,0,,
465,I'm not sure everything is ok here.,0,0,0,1,0,,
466,I'm not trying to test IW so I'm gonna neuter it and keep working.,0,0,0,0,0,,
467,False alarm (fake IOE -> slowFileExists == false -> scary assertion).,1,0,0,0,0,,
468,I disable it during commit (with a comment) as a workaround.,0,1,0,0,0,,
469,But its still on during reopen which does not have this check.,1,0,0,0,0,,
470,I think its good enough.,0,0,1,0,0,,
471,"False alarm 
Phew!",0,0,0,0,0,,
472,"Commit 1671900 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671900 ]
LUCENE-6405: add shotgun test for exception handling",0,0,0,0,0,,
473,"Commit 1671902 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671902 ]
LUCENE-6405: add shotgun test for exception handling",0,0,0,0,0,,
474,"I'll close the issue, but we should improve it more later.",0,0,0,0,1,,
475,Really each of these tests should have unit tests (using a MDW that failOn's openInput/createOutput/whereever).,0,1,0,0,0,,
476,I think it would even better.,0,0,1,0,0,,
477,But for now this is an improvement over the IW tests and fills missing coverage for backwards-codecs/,0,0,1,0,1,,
478,"Commit 1671995 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671995 ]
LUCENE-6405: add infos exc unit tests.",0,0,0,0,0,,
479,fix TestTransactions to handle exceptions on openInput and let MDW do it,0,0,0,0,0,,
480,"Commit 1671996 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671996 ]
LUCENE-6405: add infos exc unit tests.",0,0,0,0,0,,
481,fix TestTransactions to handle exceptions on openInput and let MDW do it,0,0,0,0,0,,
482,Bulk close for 5.2.0.,0,0,0,0,1,,
483,"
Report after iter 10:
Chart saved to out.png... (wd: /home/rmuir/workspace/util/src/python)
                    Task   QPS trunk      StdDev   QPS patch      StdDev                Pct diff
             MedSpanNear       75.69      (2.0\%)       80.58      (3.9\%)    6.5\% (   0\% -   12\%)
             LowSpanNear      233.30      (3.8\%)      259.44      (6.5\%)   11.2\% (   0\% -   22\%)
            HighSpanNear        9.43      (3.6\%)       10.76      (7.5\%)   14.0\% (   2\% -   25\%)",0,0,0,0,0,,
484,"Oops, I thought by now ArrayList would be JIT-ed away, thanks.",0,0,0,0,0,,
485,"Also the UOE's in the NearSpansOrdered payload methods have gone in this patch, I had put these in to check the tests.",0,1,0,0,0,,
486,I removed the UOE because now the no-payload impl is used if a segment doesn't happen to have any payloads.,0,1,1,0,0,,
487,"But this is valid, the documents might just not have any.",0,0,1,0,0,,
488,+1 too bad we can't expect ArrayList to always perform like a plain array,0,0,1,0,0,,
489,"Commit 1671078 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1671078 ]
LUCENE-6388: Optimize SpanNearQuery",0,0,0,0,0,,
490,"Commit 1671081 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1671081 ]
LUCENE-6388: Optimize SpanNearQuery",0,0,0,0,0,,
491,For now the check is implemented via Terms.getPayloads() until LUCENE-6390 is fixed.,0,0,0,0,1,,
492,Bulk close for 5.3.0 release,0,0,0,0,1,,
493,I had started seeing if we could implicitely wrap with ConstantScorer but there seem to be a couple of places that call score() when needsScores is false.,1,1,0,0,0,,
494,Here is another alternative that still stops calling matches() after the first matching clause is found but now also pretends there is a single matching clause so that score() won't fail.,0,1,0,0,0,,
495,This is the smallest viable fix I can think of for https://builds.apache.org/job/Lucene-Solr-NightlyTests-5.x/775/.,0,1,1,0,0,,
496,But we should probably think about better fixes for the future and avoid calling score() when it's not needed.,0,1,0,0,0,,
497,I opened LUCENE-6330 to tackle BooleanScorer (and maybe others).,1,0,0,0,0,,
498,"Commit 1663756 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1663756 ]
LUCENE-6329: Calling score() should be ok even if needsScores is false.",0,0,0,0,0,,
499,"Commit 1663757 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1663757 ]
LUCENE-6329: Calling score() should be ok even if needsScores is false.",0,0,0,0,0,,
500,Bulk close after 5.1 release,0,0,0,0,1,,
501,Here was my first patch (moved from LUCENE-6320).,0,1,0,0,0,,
502,"Maybe we can add some fieldinfos tests to exercise this directly, and maybe there is a way to simplify the scary logic too.",0,1,0,1,0,,
503,"+1, this looks like an easy, possibly high-impact win.",0,0,1,0,0,,
504,"I think we can be more aggressive about using the array: TreeMap has much more than 50\% overhead, since it also needs Integer key and pointer to that key, and object overhead holding that key pointer and value pointer.",0,1,1,0,0,,
505,I think we can safely do this opto when it's > 10\% of the space?,0,1,0,0,0,,
506,"Mike, if you have time, can you do calculations and adjust the patch?",0,0,0,0,0,,
507,"You can take the issue too, i had basically given up on this.",0,0,0,0,0,,
508,OK I can try to update this & commit...,0,0,0,0,1,,
509,"New patch, using dense array when > 1/16th of the numbers are used:
Each TreeMap$Entry has object header (8 or 16 bytes), 5 pointers (4 or
8 bytes), and a boolean (likely rounded up to 4 bytes), times 2 for
all the inner nodes of the tree, plus the overhead of Integer (object
header, int), so net/net each entry in the TreeMap costs 68 - 124 bytes.",0,1,0,0,0,,
510,The array is 4 or 8 bytes per int.,0,1,0,0,0,,
511,I think the patch is ready...,0,0,1,0,0,,
512,"Thanks, can you change 16 to 16L?",0,1,0,0,0,,
513,I don't want to think about overflowing.,0,0,0,1,0,,
514,"Thanks, can you change 16 to 16L?",0,1,0,0,0,,
515,"Oh, good catch!",0,0,1,0,0,,
516,I'll fix ...,0,0,0,0,1,,
517,"+1 to commit with that modification, thanks for doing the calculations here.",0,0,1,0,0,,
518,"Commit 1687789 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1687789 ]
LUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases",0,0,0,0,0,,
519,"Commit 1687792 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1687792 ]
LUCENE-6325: use array for number -> FieldInfo lookup, except in very sparse cases",0,0,0,0,0,,
520,Thanks Robert Muir!,0,0,0,0,0,,
521,Bulk close for 5.3.0 release,0,0,0,0,1,,
522,Patch that adds the MatchNoDocsQuery and uses it for empty SimpleQueryParser queries as well as when a BooleanQuery is rewritten and has no clauses.,0,1,0,0,0,,
523,I think its confusing we have MatchAll but not MatchNone.,0,0,0,1,0,,
524,I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses?,0,1,0,0,0,,
525,Then it wouldn't need a weight and scorer.,0,0,1,0,0,,
526,I think the 0x1AA71190 of MatchAllDocsQuery is here to avoid that all Query impls that only wrap a boost end up with the same hash code.,0,1,1,0,0,,
527,Maybe a cleaner way to do it would be to return Float.floatToIntBits(getBoost()) ^ getClass().hashCode().,0,1,1,0,0,,
528,I wonder if it should just be sugar and rewrite() to a booleanquery with no clauses?,0,1,0,0,0,,
529,+1 to rewrite to an empty BooleanQuery,0,0,1,0,0,,
530,1,0,0,1,0,0,,
531,New patch that changes MatchNoDocsQuery to rewrite to an empty BooleanQuery.,0,1,0,0,0,,
532,Also removes the nocommit as per Adrien's suggestion,0,1,0,0,0,,
533,is the hashcode/equals stuff needed here or can the superclass impls in Query be used?,0,1,0,0,0,,
534,They seem to already have this logic.,0,0,1,0,0,,
535,"In the tests, i would add a call to QueryUtils.check(q) to one of your matchnodocsqueries.",0,1,0,0,0,,
536,This will do some tests on hashcode/equals.,0,1,0,0,0,,
537,Feels wrong to me to override hashCode but not equals.,0,0,0,1,0,,
538,I think we should move this class part of hashCode() to Query.hashCode()?,0,1,0,0,0,,
539,(ie.,0,0,0,0,0,,
540,"return Float.floatToIntBits(getBoost()) ^ getClass().hashCode()
If it is controversial then I'm happy with the previous patch that overrides both equals and hashCode().",0,1,1,0,0,,
541,+1 Adrien.,0,0,1,0,0,,
542,Adrien: I agree about having the hashCode.,0,0,1,0,0,,
543,Here is a new patch that doesn't override equals or hashCode and changes Query to use the class in the hashCode method as Adrien suggested.,0,1,0,0,0,,
544,"Thanks Lee, I like this.",0,0,1,0,0,,
545,"When i see code overriding hashcode/equals and not calling super.hashcode/super.equals, its a bad sign.",0,0,0,1,0,,
546,"We should commit this one, and remove duplicated logic and hardcoded constants in e.g.",0,0,0,0,1,,
547,TermQuery and all other places in a followup.,0,0,0,0,1,,
548,"Robert: +1, I opened LUCENE-6333 for this, I'll work on a patch.",0,0,1,0,0,,
549,"Commit 1663899 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1663899 ]
LUCENE-6304: Add MatchNoDocsQuery.",0,0,0,0,0,,
550,Committed.,0,0,0,0,1,,
551,Thanks Lee!,0,0,0,0,0,,
552,"Commit 1663901 from Adrien Grand in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1663901 ]
LUCENE-6304: Add MatchNoDocsQuery.",0,0,0,0,0,,
553,Bulk close after 5.1 release,0,0,0,0,1,,
556,1,0,0,1,0,0,,
557,I added two more timings to the patch.,0,1,0,0,0,,
558,"here is the output on one of my wiki10m segments:

size (MB)=624.591
    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_40-ea, lucene.version=6.0.0, mergeMaxNumSegments=-1, os.arch=amd64, source=merge, mergeFactor=10, os.version=3.13.0-43-generic, timestamp=1423097209630}
    has deletions [delGen=6]
    test: open reader.........OK [took 0.075 sec]
    test: check integrity.....OK [took 1.515 sec]
    test: check live docs.....OK [90031 deleted docs]
    test: field infos.........OK [8 fields] [took 0.000 sec]
    test: field norms.........OK [2 fields] [took 0.046 sec]
    test: terms, freq, prox...OK [6844227 terms; 170452948 terms/docs pairs; 240913350 tokens] [took 13.171 sec]
    test (ignoring deletes): terms, freq, prox...OK [7105194 terms; 179422787 terms/docs pairs; 253586353 tokens] [took 9.632 sec]
    test: stored fields.......OK [5135307 total field count; avg 3.0 fields per doc] [took 4.648 sec]
    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.036 sec]
    test: docvalues...........OK [2 docvalues fields; 0 BINARY; 1 NUMERIC; 1 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.206 sec]


Maybe check index should have a integrity-check only option as a followup.",0,1,0,0,0,,
559,"It would just be sugar to the user, but this would always be pretty fast.",0,0,1,1,0,,
560,"sorry, here is the correct patch.",0,1,0,0,0,,
561,"OK I noticed one case where live docs didn't confess how long it took 
I'll fix that and commit.",0,0,0,0,1,,
562,"Commit 1658831 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1658831 ]
LUCENE-6233: speed up CheckIndex when the index has term vectors",0,0,0,0,0,,
563,"Commit 1658832 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1658832 ]
LUCENE-6233: speed up CheckIndex when the index has term vectors",0,0,0,0,0,,
564,Bulk close after 5.1 release,0,0,0,0,1,,
554,"This was introduced with LUCENE-5610
I'll fix the nightly Lucene benchmark to plot CheckIndex time ... we could have spotted this performance regression.",0,1,0,0,0,,
555,I think CheckIndex should not check Terms.getMin/Max for TVs?,0,1,0,0,0,,
565,Patch attached + a couple of unit tests for allTermsRequired=false,0,1,0,0,0,,
566,"I didn't see this patch yet, but as I said in SOLR-6648, I think it makes sense to set those defaults in the constructor.",0,1,1,0,0,,
567,"Boon Low, could you update the patch to a recent version of trunk?",0,0,0,0,1,,
568,"Also, could you add javadocs to the newly added constructors?",0,1,0,0,0,,
569,That patch was based upon and tested with the v4.10.3 release on Dec 20.,0,0,0,0,0,,
570,But I can see that have been significant changes to AnalyzingInfixSuggester in the trunk.,0,0,0,0,0,,
571,Shall update and test the patch tomorrow.,0,0,0,0,0,,
572,patch updated w.r.t.,0,1,0,0,0,,
573,trunk 05/01/15,0,0,0,0,0,,
574,The patch looks good.,0,0,1,0,0,,
575,I added some more tests to the new code and renamed the field highlighting->highlight,0,0,0,0,1,,
576,Minor changes to the test and made the new fields private.,0,1,0,0,0,,
577,I'll commit this soon,0,0,0,0,1,,
578,"Commit 1649893 from Tomás Fernández Löbbe in branch 'dev/trunk'
[ https://svn.apache.org/r1649893 ]
LUCENE-6149: Infix suggesters' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup",0,0,0,0,0,,
579,"Commit 1649955 from Tomás Fernández Löbbe in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1649955 ]
LUCENE-6149: Infix suggesters' highlighting and allTermsRequired can be set at the constructor for non-contextual lookup",0,0,0,0,0,,
580,"Thanks Tomás, good to see the patch making into the trunk and branch_5x.",0,0,0,0,1,,
581,I shall find some time soon to update and post the v.4.10.3 patch to include your changes.,0,0,0,0,0,,
582,There's a typo the test class: testConstructorDefatuls,0,0,0,0,0,,
583,patch for v4.10.3 release,0,1,0,0,0,,
584,"Commit 1650132 from Tomás Fernández Löbbe in branch 'dev/trunk'
[ https://svn.apache.org/r1650132 ]
LUCENE-6149: Fixed typo",0,0,0,0,0,,
585,"Commit 1650134 from Tomás Fernández Löbbe in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650134 ]
LUCENE-6149: Fixed typo",0,0,0,0,0,,
586,Fixed.,0,0,0,0,1,,
587,Thanks Boon!,0,0,0,0,0,,
588,Bulk close after 5.0 release.,0,0,0,0,1,,
589,Simple patch + test.,0,1,0,0,0,,
590,Thinking about this more ... it may be better to do this entirely inside a FilterDirectory.,0,1,0,0,0,,
591,E.g.,0,0,0,0,0,,
592,"when IndexOutput is closed, and the IOContext is not MERGE, increment the bytes written ... and then that same directory instance could dynamically update the target merge throttling ... maybe.",0,1,0,0,0,,
593,I ran some tests with this approach and I think it's no good.,0,0,0,1,0,,
594,"This creates a tricky feedback system, where both CMS (via hard stalling of incoming threads) and this directory attempt to make change to let merges catch up.",0,0,0,1,0,,
595,"When CMS's hard stalls kick in, this lowers the indexing byte/sec rate, which causes this directory to (over simplistically) lower the merge IO throttling, which causes the merges to take longer.",0,0,0,1,0,,
596,"I think it's better if all throttling efforts happen in one place, e.g.",0,0,1,0,0,,
597,CMS.,0,0,0,0,0,,
598,I'l think about it ...,0,0,0,0,0,,
599,great to see progress nuking checkabort!,0,0,1,0,0,,
600,"

    // Defensive: sleep for at most 250 msec; the loop above will call us again if we should keep sleeping:
    if (curPauseNS > 250L*1000000000) {
      curPauseNS = 250L*1000000000;
    }


Did you mean 250 milliseconds or 250 seconds?",0,0,0,0,0,,
601,"+1, I really like this approach.",0,0,1,0,0,,
602,Did you mean 250 milliseconds or 250 seconds?,0,0,0,0,0,,
603,Woops!,0,0,0,0,0,,
604,"I'll fix, thanks.",0,0,0,0,0,,
605,I was never sure what a good value for the rate limiter would be so I'm very happy to see Lucene take care of it by itself.,0,0,1,0,0,,
606,+  /** true if we should rate-limit writes for each merge,NULL,0,0,0,0,0,
607,"null means use dynamic default: */
+  private boolean doAutoIOThrottle = true;


I think the comment is outdated since doAutoIOThrottle is a boolean now (instead of a Boolean)?",0,0,0,1,0,,
608,"There is a similar leftover a couple of lines below I think: if (doAutoIOThrottle == Boolean.TRUE)


+    /** Set by {@link IndexWriter} to rate limit writes and abort this merge.",0,0,0,1,0,,
609,"*/
+    public final MergeRateLimiter rateLimiter;


I think the comment is a bit confusing since this property is not actually set by the index writer?",0,0,0,1,0,,
610,"/** Returns 0 if no pause happened, 1 if pause because rate was 0.0 (merge is paused), 2 if paused with a normal rate limit.",0,0,0,0,0,,
611,"*/
  private synchronized int maybePause(long bytes, long curNS) throws MergePolicy.MergeAbortedException


Maybe having constants or an enum would make the code easier to read?",0,1,1,0,0,,
612,"Thanks Adrien Grand, here's a new patch with those fixes.",0,1,0,0,0,,
613,"Commit 1649532 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1649532 ]
LUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate",0,0,0,0,0,,
614,"Commit 1649539 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1649539 ]
LUCENE-6119: CMS dynamically rate limits IO writes of each merge depending on incoming merge rate",0,0,0,0,0,,
615,"Commit 1650025 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650025 ]
LUCENE-6119: fix just arrived merge to throttle correctly",0,0,0,0,0,,
616,"Commit 1650026 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650026 ]
LUCENE-6119: fix just arrived merge to throttle correctly",0,0,0,0,0,,
617,"Commit 1650027 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650027 ]
LUCENE-6119: fix just arrived merge to throttle correctly",0,0,0,0,0,,
618,"Commit 1650463 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650463 ]
LUCENE-6119: set initial rate for forced merge correctly",0,0,0,0,0,,
619,"Commit 1650464 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650464 ]
LUCENE-6119: set initial rate for forced merge correctly",0,0,0,0,0,,
620,"Commit 1650594 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1650594 ]
LUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping",0,0,0,0,0,,
621,"Commit 1650595 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1650595 ]
LUCENE-6119: must check merge for abort even when we are not rate limiting; don't wrap rate limiter when doing addIndexes (it's not abortable); don't leak file handle when wrapping",0,0,0,0,0,,
622,"Commit 1651305 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1651305 ]
LUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter",0,0,0,0,0,,
623,"Commit 1651307 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1651307 ]
LUCENE-6119: make sure minPauseCheckBytes is set on init of MergeRateLimiter",0,0,0,0,0,,
624,I realize this issue is closed but it'd be sweet to have this kind of adaptive heuristic to throttle the number of merging threads as well.,0,1,1,0,0,,
625,Let me explain.,0,0,0,0,0,,
626,"When you think of it, the really important measurement of quality on the surface is I/O throughput of merges combined with I/O throughput of IW additions (indexing).",0,1,1,0,0,,
627,"Essentially we want to maximize a function:


f = merge_throughput + indexing_throughput


perhaps with a bias towards indexing_throughput which can be modeled (by multiplying by a constant?).",1,1,0,0,0,,
628,"The underlying variables to adaptively tune are:

how many merge threads there are (for example having too many doesn't make sense on a spindle, with an SSD this is not a problem),
when to pause/ resume existing merge threads,
when to pause/ resume indexing threads.",1,0,0,0,0,,
629,"What's interesting is that we can tweak these variables in response to the the current value (and gradient) of function f. This means an adaptive algorithm could (examples):

react to temporary external system load (for example pausing some merge threads if it observes a drop in throughput),


find out the sweet spot of how many merge threads there can be without saturating I/O (no need to detect SSD vs. spindle; we just want to maximize f – the optimal number of merge threads would emerge by itself from looking at the data).",0,1,0,0,0,,
630,"Now the big question is what this algorithm should look like, of course.",1,0,0,0,0,,
631,The options vary from relatively simple hand-written rule-based heuristics to an advanced black-box with either pre-trained or adaptive machine learning algorithms.,0,1,0,0,0,,
632,"I have an application that has just one of the objectives of function f (we need to quickly merge a large set of segments, optimally without knowing or caring what the underlying disk hardware/ disk buffers are).",0,1,0,0,0,,
633,I'll report my impressions once I have it done.,0,0,0,0,0,,
634,I think auto-tuning merge thread count would be a great addition!,0,1,1,0,0,,
635,I know.,0,0,0,0,0,,
636,"It would take a lot of manual tuning or detection (ssd vs. non-ssd vs. hybrid vs. large mem disk buffers, etc.)",0,0,0,1,0,,
637,off the map.,0,0,1,0,0,,
638,And it could gracefully play with other components of the system without clogging everything (like ionice).,0,0,1,0,0,,
639,We'll see.,0,0,0,0,0,,
640,Bulk close after 5.0 release.,0,0,0,0,1,,
641,Patch against branch_5x,0,1,0,0,0,,
642,Oops.,0,0,0,0,0,,
643,Here is the real patch against branch_5x.,0,1,0,0,0,,
644,"+1, looks good.",0,0,1,0,0,,
645,"Commit 1636025 from Ryan Ernst in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1636025 ]
LUCENE-6043: Fix backcompat support for UAX29URLEmailTokenizer",0,0,0,0,0,,
646,"Commit 1636074 from Ryan Ernst in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1636074 ]
LUCENE-6043: Forgot to svn add the new file",0,0,0,0,0,,
647,Bulk close after 5.0 release.,0,0,0,0,1,,
648,Committed to trunk and branch_5x.,0,0,0,0,1,,
649,"Thanks again for reporting, Ilia Sretenskii!",0,0,0,0,0,,
650,"Unfortunately I can not help you with that, guys.",0,0,0,0,0,,
651,My experience is limited with Apache Maven and I honestly have no idea about how Apache Ant and Apache Ivy work at all.,0,0,0,0,0,,
652,"Isn't that just too many of different dependency managers to mess with, that are causing their own conflicts troubles?",0,0,0,1,0,,
653,+1 for backport to 4.10.x,0,0,1,0,0,,
654,Reopening to backport to the 4.10 branch,0,0,0,0,1,,
655,Committed to lucene_solr_4_10.,0,0,0,0,1,,
656,Thanks Jan. - Steve,0,0,0,0,0,,
657,updated description with work around for older sources,0,0,0,0,1,,
658,Can you post a test case which fails with this exception?,0,0,0,0,0,,
659,"I'm afraid that would be very hard as it is not easily reproducible, and I am not able to provide you with my production index.",0,0,0,0,0,,
660,Basically what I do is I'm calling DrillSideways search method with boolean query like (+field:term1 + field:term2 +field:term3 +field:term4 -field:term5) and drilldown on one facet dimension.,0,1,0,0,0,,
661,"Of 26 segments that my index have, only one is coming with NullPointerException.",1,0,0,0,0,,
662,"If I send 4 instead of 5 terms, it works properly.",1,0,0,0,0,,
663,"Also if I leave out the NOT term, it works with any number of terms.",1,0,0,0,0,,
664,I'm sorry I couldn't be of more help.,0,0,0,0,0,,
665,If you tell me where to look and how to debug I can do it and post the results.,0,0,0,0,0,,
666,"I think we just need to find out why is that scorer not initialized, or we could default the cost to 0 (in DrillSidewaysScorer) if there is null pointer exception .",1,1,0,0,0,,
667,"In any way, this unchecked exception should probably be caught somewhere.",1,0,0,0,0,,
668,"If ReqExclScorer has a null 'req' part, then something in the logic of BooleanWeight.",1,0,0,0,0,,
669,ok I see the bug.,0,0,0,0,0,,
670,"ReqExclScorer can indeed be buggy here, because it marks 'req' as null for 'exhausted'.",1,0,0,0,0,,
671,"So if you call cost() after that, you will get NPE.",0,1,0,0,0,,
672,We've been seeing this issue semi-regularly in our app.,1,0,0,0,0,,
673,We've been working around it so far by simplifying our queries to remove clauses (probably a good idea anyway) but we can never be sure our users won't find a way to break it!,0,1,1,1,0,,
674,"this bug was fixed on trunk when reqScorer was modified to no longer be set to null
this patch is against 5.0, where ReqScorer can hit NPE if cost is called after nextDoc
patch includes test and fix to call cost before nextDoc",0,0,0,0,1,,
675,"Commit 1662673 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'
[ https://svn.apache.org/r1662673 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches",0,0,0,0,0,,
676,"Commit 1662674 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662674 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches",0,0,0,0,0,,
677,Thank you Dragan and jane!,0,0,0,0,0,,
678,"Commit 1662681 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1662681 ]
LUCENE-6001: DrillSideways hits NullPointerException for some BooleanQuery searches",0,0,0,0,0,,
679,"Commit 1662728 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'
[ https://svn.apache.org/r1662728 ]
LUCENE-6001: null check was backwards",0,0,0,0,0,,
680,"Commit 1662732 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662732 ]
LUCENE-6001: null check was backwards",0,0,0,0,0,,
681,"Commit 1662733 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1662733 ]
LUCENE-6001: null check was backwards",0,0,0,0,0,,
682,Bulk close for 4.10.4 release,0,0,0,0,1,,
683,"GitHub user andyetitmoves opened a pull request:
 https://github.com/apache/lucene-solr/pull/96
    Explicitly stop beast from running on top-level modules
    Patch for LUCENE-5968
You can merge this pull request into a Git repository by running:
    $ git pull https://github.com/bloomberg/lucene-solr trunk-beast-error
Alternatively you can review and apply these changes as the patch at:
 https://github.com/apache/lucene-solr/pull/96.patch
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #96

commit 79578f7cd4825d3d0d6700c4ec5581374216ac6f
Author: Ramkumar Aiyengar <andyetitmoves@gmail.com>
Date:   2014-09-21T07:33:59Z
    Explicitly stop beast from running on top-level modules",0,0,0,0,0,,
684,"Uwe Schindler, Hoss Man: The pull request above has changes suggested by both, could you check and commit?",0,0,0,0,0,,
685,"Github user uschindler commented on the pull request:
 https://github.com/apache/lucene-solr/pull/96#issuecomment-56595072
    Hi, see comments on https://issues.apache.org/jira/browse/LUCENE-5968 !",0,0,0,0,0,,
686,Done..,0,0,0,0,0,,
687,"Hey Uwe Schindler, I have addressed your comments, could this be merged in?",0,0,0,0,0,,
688,Thanks!,0,0,0,0,0,,
689,"Sorry,
I missed this issue.",0,0,0,0,0,,
690,I will commit in a moment.,0,0,0,0,1,,
691,Sorry.,0,0,0,0,0,,
692,Uwe,0,0,0,0,0,,
693,"Commit 1642488 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1642488 ]
LUCENE-5968: Improve error message when 'ant beast' is run on top-level modules
This closes #96",0,0,0,0,0,,
694,"Commit 1642489 from Uwe Schindler in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1642489 ]
Merged revision(s) 1642488 from lucene/dev/trunk:
LUCENE-5968: Improve error message when 'ant beast' is run on top-level modules
This closes #96",0,0,0,0,0,,
695,"Github user asfgit closed the pull request at:
 https://github.com/apache/lucene-solr/pull/96",0,0,0,0,0,,
696,Bulk close after 5.0 release.,0,0,0,0,1,,
697,1,0,0,1,0,0,,
698,"Patch requiring resourceDescription (either datainput, or string).",0,1,0,0,0,,
699,We had quite a few places missing this.,1,0,0,0,0,,
700,+1 for this!,0,0,1,0,0,,
701,"+1, looks awesome.",0,0,1,0,0,,
702,"Commit 1626372 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1626372 ]
LUCENE-5965: CorruptIndexException requires a String or DataInput resource",0,0,0,0,0,,
703,"Commit 1626375 from Robert Muir in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626375 ]
LUCENE-5965: CorruptIndexException requires a String or DataInput resource",0,0,0,0,0,,
704,Bulk close after 5.0 release.,0,0,0,0,1,,
705,Patch with suggested changes.,0,1,0,0,0,,
706,"Thanks Markus, looks great, I'll commit shortly.",0,0,1,0,0,,
707,"Commit 1626241 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1626241 ]
LUCENE-5963: more efficient AnalyzingSuggester.replaceSep",0,0,0,0,0,,
708,"Commit 1626242 from Michael McCandless in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626242 ]
LUCENE-5963: more efficient AnalyzingSuggester.replaceSep",0,0,0,0,0,,
709,Thanks Markus!,0,0,0,0,0,,
710,Bulk close after 5.0 release.,0,0,0,0,1,,
711,"Patch, starting from Vitaly's test case (thank you!)",0,1,0,0,0,,
712,"and folding into Lucene's tests ... it fails with this on trunk:

1) testReopenReaderToOlderCommit(org.apache.lucene.index.TestDirectoryReaderReopen)
java.lang.IllegalStateException: same segment _0 has invalid changes; likely you are re-opening a reader after illegally removing index files yourself and building a new index in their place.",1,0,0,0,0,,
713,"Use IndexWriter.deleteAll or OpenMode.CREATE instead
	at __randomizedtesting.SeedInfo.seed([D3F22B13D5839643:931C8A9673D003F4]:0)
	at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:190)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:323)
	at org.apache.lucene.index.StandardDirectoryReader$2.doBody(StandardDirectoryReader.java:317)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromCommit(StandardDirectoryReader.java:312)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenNoWriter(StandardDirectoryReader.java:308)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:259)
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:137)
	at org.apache.lucene.index.TestDirectoryReaderReopen.testReopenReaderToOlderCommit(TestDirectoryReaderReopen.java:824)",1,0,0,0,0,,
714,Patch: I think the use-case is cool and it should be supported: its just adding an 'if' and removing the current exception (which is geared at protecting some user who manually rm -rf's files from their index.),0,1,1,0,0,,
715,I improved the test a bit to ensure that cores are shared and also tested the dv updates case.,0,1,1,0,0,,
716,"Thanks Rob, patch looks great, except: I think we can keep the illegalDocCountChange safety?",0,0,1,1,0,,
717,I think I could make a test case to trip that ...,0,1,0,0,0,,
718,Can you make a test change to trip it without manually removing files from your index?,0,0,0,0,0,,
719,Can you make a test change to trip it without manually removing files from your index?,0,0,0,0,0,,
720,"I don' t think so ... the only way I know of this happening is if an app has a reader open, then removes the index, rebuilds it, then tries to openIfChanged the reader.",0,1,0,1,0,,
721,"We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption... it's nice not to have false scares even if the app is doing something it shouldn't...",0,1,0,1,0,,
722,"Yes, I really want to.",0,0,1,0,0,,
723,"We can't let such abuse prevent real features, thats just wrong.",0,0,0,1,0,,
724,"If you want safety against deleting files, maybe look at NIO.2 WatchService.",0,1,0,0,0,,
725,This is general and would give you notification when such things happen rather than having a hack for one particular user's mistake.,0,1,1,0,0,,
726,"We can remove the defensive safety if you really want to, but we put it in last time when this happened and it sure looked like index corruption
It is index corruption though.",0,1,0,1,0,,
727,The user went and manually corrupted their index.,0,0,0,0,0,,
728,Why hide that Mike?,0,0,0,1,0,,
729,"Michael,
Your updated patch definitely fixes the issue.",0,0,1,0,0,,
730,"But I just wanted to understand why deletes are so special, in that - if I don't have any buffered deletes for the segment, but new documents only, the reused reader instance won't pick them up, even without the fix in place.",1,0,0,0,0,,
731,This is because liveDocs won't capture unflushed doc ids?,1,0,0,0,0,,
732,"Woops, this almost dropped past the event horizon of my TODO list.",0,0,0,0,0,,
733,"I modernized the patch, and was able to improve how effective its check is, by switching to comparing segment IDs (a very good check that the segments changed on disk) vs what the patch used to do, comparing maxDoc.",0,1,1,0,0,,
734,"Commit 664e39292bd0a90ed6f20debc872ab74a1d7294f in lucene-solr's branch refs/heads/master from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=664e392 ]
LUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader",0,0,0,0,0,,
735,"Commit 6b0b119074f4cd32adc2388fbcc01f2aa70c7d5d in lucene-solr's branch refs/heads/branch_6x from Mike McCandless
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6b0b119 ]
LUCENE-5931: detect when segments were (illegally) replaced when re-opening an IndexReader",0,0,0,0,0,,
736,Thanks Adrien Grand for pinging me about this almost lost issue!,0,0,0,0,0,,
737,Bulk close resolved issues after 6.2.0 release.,0,0,0,0,1,,
738,Here is a simple patch.,0,1,0,0,0,,
739,Looks good.,0,0,1,0,0,,
740,FWIW CharTermAttribute.java has some optimizations for this method.,0,1,1,0,0,,
741,Maybe if they are really useful we should see if we can pull them out into static methods.,0,1,1,0,0,,
742,"Thanks Uwe, I'll fix before committing, what a crazy requirement!",0,0,0,0,0,,
743,There are potential things to share with CharTermAttributeImpl but I'd rather leave it to another issue as it has some pitfalls as noted.,0,0,0,1,0,,
744,"Commit 1618625 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1618625 ]
LUCENE-5892: CharsRefBuilder implements Accountable.",0,0,0,0,0,,
745,"Commit 1618627 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1618627 ]
LUCENE-5892: CharsRefBuilder implements Accountable.",0,0,0,0,0,,
746,"Commit 1618628 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1618628 ]
LUCENE-5892: Missed the covariant return type.",0,0,0,0,0,,
747,"Commit 1618629 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1618629 ]
LUCENE-5892: Missed the covariant return type.",0,0,0,0,0,,
748,Thanks!,0,0,0,0,0,,
749,"BTW: Typical example is StringBuilder itsself: It implements Appendable, but also uses covariant return type: http://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html#append(java.lang.CharSequence)",0,0,0,0,0,,
750,Here is a patch.,0,1,0,0,0,,
751,Do we have a benchmark that could be used to validate this change?,1,0,0,0,0,,
752,"I just checked out luceneutil but it only seems to have tasks for queries, not sorting?",0,1,0,1,0,,
753,I dont understand how the MatchNoBits case is safe.,0,0,0,1,0,,
754,"If no document has values, then they will all return the missing value?",1,0,0,0,0,,
755,"Oops, I understand your question now, I didn't upload the latest version of my patch.",0,0,0,0,0,,
756,"i benchmarked the first version of the patch with the little benchmark in luceneutil, but saw no improvement.",0,1,0,1,0,,
757,I think the current null check is effective?,0,1,1,0,0,,
758,it has to be handled anyway.,0,0,1,0,0,,
759,And personally i would be wary of overspecialization here...,0,0,0,1,0,,
760,I dont think there is specialization needed.,0,0,0,1,0,,
761,"The null check, Robert mentions, was done like this to optimize missing values.",0,1,1,0,0,,
762,"the null check has to be done anyways by the jvm, so removing it brings nothing.",0,1,0,1,0,,
763,see the original missing values issue for discussion.,0,0,0,0,0,,
764,"Uwe, please read the issue again: the goal was not to remove the null check, but the check for missing values.",1,0,0,0,0,,
765,The reason why I came up with this issue is that I'm writing a selector in order to sort based on the values of a block of documents.,0,1,0,0,0,,
766,To make it work efficiently I need to write a NumericDocValues instance that already returns the missing value when there are either no child documents in the block or if none of them have a value.,0,1,1,0,0,,
767,So there is no need to check the missing values in the comparator.,0,0,1,0,0,,
768,I'm surprised that you think of it as a specialization as this is actually making things simpler?,0,0,1,0,0,,
769,The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance.,0,1,1,0,0,,
770,And it makes it easier (and potentially more efficient) to write selectors.,0,0,1,0,0,,
771,I'm surprised that you think of it as a specialization as this is actually making things simpler?,0,0,1,0,0,,
772,The handling of the missing value is done once for all in setNextReader and then the comparator only needs to care about the NumericDocValues instance.,0,1,1,0,0,,
773,And it makes it easier (and potentially more efficient) to write selectors.,0,0,1,0,0,,
774,"It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues.",0,0,1,0,0,,
775,"and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.",0,0,0,1,0,,
776,"the goal was not to remove the null check, but the check for missing values.",1,0,0,0,0,,
777,"In fact you are removing the null check, which is the extra branch to check for missing values - just look at the old code (this was my trick).",0,1,0,0,0,,
778,"It was done exactly like this to not slow down - hotspot can optimize that away, if it finds out that it is null - it does this very fast.",0,0,1,0,0,,
779,We checked this at the time I added this to Lucene 3.5 or like that.,0,0,0,0,0,,
780,We compared the two implementations - without missing values and the new one with missing values - and they were exactly the same speed.,0,1,1,0,0,,
781,"The same that Robert discovered here, too.",0,0,0,0,0,,
782,"In fact your patch would only work in Lucene trunk, in 4.x this cannot be done like that.",0,0,1,1,0,,
783,"It is a specialization, because instead of a branch for null, you have a branch checking class of the numericdocvalues.",0,0,1,0,0,,
784,"and if this one fails, the whole thing gets deoptimized and hotspot goes crazy.",0,0,0,1,0,,
785,Doesn't it happen already if you have two fields that have different compression?,1,0,0,0,0,,
786,I didn't see it happening with the currently assembly generated.,0,0,1,0,0,,
787,"I'm ok with the issue if we see a performance increase, just not seeing it.",0,0,0,1,0,,
788,Fair enough,0,0,1,0,0,,
789,"I don't think we should remove the default implementation for FilterScorer, as the scorer is not really changed when using this abstract class, its just wrapped?",0,1,0,1,0,,
790,"For the same reason, i think the boostingscorer (since its just an implementation detail of how the current BS2 stuff solves this case) should be transparent.",0,1,1,0,0,,
791,Thanks for taking the time to review my patch and comment on the approach.,0,0,0,0,0,,
792,"The reason that I advocated changing FilterScorer and BoostedScorer is to allow some of my custom Query implementations to use a regular BooleanQuery for recall and optionally scoring while taking advantage of the actual Scorers used on a per document, per clause basis.",0,0,1,0,0,,
793,This has been working great across quite a few Lucene releases but failed when I upgraded to 4.9 due to the two regressions in behavior for Scorer.getChildren() as described in this ticket.,1,0,0,0,0,,
794,"In this scenario, a BooleanQuery containing two TermQueries (one a miss and the other a hit) returns the following from BooleanWeight.scorer():

BoostedScorer
	
TermScorer (hit)



Calling getChildren() on this returns an empty list because the BoostedScorer just returns in.getChildren() and thus you are unable to navigate to the actual TermScorer in play.",1,0,0,0,0,,
795,This would impact any classes that extend FilterScorer and don't override getChildren().,0,0,0,1,0,,
796,"In other words, the current wiring does make the BoostedScorer transparent but with the disadvantage of hiding the actual scorer that performs the work.",0,0,1,1,0,,
797,"If this is an unsupported workflow, I'm happy to move the discussion over to the user mailing list.",0,0,0,0,0,,
798,I see: this makes sense.,0,0,1,0,0,,
799,"If you have a custom scorer you may need access to the raw one, so this makes sense to remove the transparency...",0,0,1,0,0,,
800,I'll look at the patch again and reply back if I have more questions.,0,0,0,0,0,,
801,"Commit 1608454 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1608454 ]
LUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery",0,0,0,0,0,,
802,"Commit 1608457 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1608457 ]
LUCENE-5796: Fix Scorer getChildren for two combinations of BooleanQuery",0,0,0,0,0,,
803,Thanks Terry!,0,0,0,0,0,,
804,"Commit 1603521 from Michael McCandless in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1603521 ]
LUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow",0,0,0,0,0,,
805,"Commit 1603523 from Michael McCandless in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1603523 ]
LUCENE-5775: Deprecate JaspellLookup; fix its ramBytesUsed to not StackOverflow",0,0,0,0,0,,
806,"Hi,
should we maybe also deprecate the Solr factories?",0,1,0,0,0,,
807,Because then Solr prints a warning on startup if they are used.,0,0,1,0,0,,
808, just add @deprecated / @Deprecated to JaspellLookupFactory right?,1,alternative,0,1,0,0,0
809,"I am trying to check that out, too",0,0,0,0,0,,
810,With Eclipse it looks like this should be enough to deprecate the factory you mentioned.,0,0,1,0,0,,
811,"But the default lookup for file-based stuff in Solr is still Jaspell:

./core/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookupFactory.java:public class JaspellLookupFactory extends LookupFactory {
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/LookupFactory.java:  public static String DEFAULT_FILE_BASED_DICT = JaspellLookupFactory.class.getName();
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:import org.apache.solr.spelling.suggest.jaspell.JaspellLookupFactory;
./core/src/java/org/apache/solr/spelling/suggest/Suggester.java:      lookupImpl = JaspellLookupFactory.class.getName();


So maybe we should change defaults.",0,1,0,0,0,,
812,Should we open SOLR issue?,0,0,0,0,0,,
813,So maybe we should change defaults.,0,1,0,0,0,,
814,Should we open SOLR issue?,0,0,0,0,0,,
815,OK I'll open a Solr issue.,0,0,0,0,0,,
816,I opened SOLR-6178,0,0,0,0,0,,
817,"Commit 1603542 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1603542 ]
LUCENE-5775: deprecate JaspellLookup",0,0,0,0,0,,
818,"OK I committed the deprecation to trunk; I'd really like to just remove it, but we can't do that until we address SOLR-6178 ... so I'll leave this issue open to remove Jaspell in trunk.",0,0,0,1,1,,
819,"Commit 1604122 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1604122 ]
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory",0,0,0,0,0,,
820,"Commit 1604124 from Uwe Schindler in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1604124 ]
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory",0,0,0,0,0,,
821,"Commit 1604125 from Uwe Schindler in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1604125 ]
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory",0,0,0,0,0,,
822,"Commit 1604707 from Uwe Schindler in branch 'dev/branches/lucene_solr_4_9'
[ https://svn.apache.org/r1604707 ]
Revert:
Merged revision(s) 1604122 from lucene/dev/trunk:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory",0,0,0,0,0,,
823,"Commit 1604710 from Uwe Schindler in branch 'dev/trunk'
[ https://svn.apache.org/r1604710 ]
Move changes of:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory",0,0,0,0,0,,
824,"Commit 1604711 from Uwe Schindler in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1604711 ]
Merged revision(s) 1604710 from lucene/dev/trunk:
Move changes of:
SOLR-6178, LUCENE-5775: Deprecate JaspellLookupFactory",0,0,0,0,0,,
825,1,0,0,1,0,0,,
826,"patch, I also cleaned up all remnants/conditionals about not supporting offsets.",0,1,0,0,0,,
827,"+1, nice that all codecs support offsets now!",0,0,1,0,0,,
828,"Commit 1584140 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1584140 ]
LUCENE-5563: remove sep layout",0,0,0,0,0,,
829,"I can backport if we want to, but I'm not sure its worth the trouble here.",0,0,0,1,0,,
830,"We still have 3.x codec in 4.x, as well as the fact the blockterms readers/indexes have changed in trunk and have better testing: so backporting poses some risks.",0,0,0,1,0,,
831,Bulk close after 5.0 release.,0,0,0,0,1,,
832,Here is a simple test.,0,1,0,0,0,,
833,Seems to be related to the use of index-time synonyms.,0,0,0,0,0,,
834,"Here's a patch I'm testing (all tests seem to pass, but I will see if i can add some better ones).",0,1,0,0,0,,
835,"FieldTermStack is ordered by position, but the current code doesn't really handle position increments of 0 (synonyms).",0,0,0,1,0,,
836,"In this case when we reach a dead-end (nextMap == null), we keep looking as long as the incoming position is the same until we find a match.",0,1,0,0,0,,
837,+1!,0,0,1,0,0,,
838,"Commit 1579255 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1579255 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases",0,0,0,0,0,,
839,"Commit 1579264 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1579264 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases",0,0,0,0,0,,
840,"Commit 1579269 from Robert Muir in branch 'dev/branches/lucene_solr_4_7'
[ https://svn.apache.org/r1579269 ]
LUCENE-5538: FastVectorHighlighter fails with booleans of phrases",0,0,0,0,0,,
841,Bulk close 4.7.1 issues,0,0,0,0,1,,
842,here is a patch,0,1,0,0,0,,
843,The first ensureOpen() in incRef() now seems redundant after this patch?,0,0,0,1,0,,
844,The first ensureOpen() in incRef() now seems redundant after this patch?,0,0,0,1,0,,
845,agreed - uploaded a new patch,0,1,1,0,0,,
846,"+1, looks correct.",0,0,1,0,0,,
847,SegemntCoreReaders atomic increment is also correct - @BrianGoetzSays,0,0,1,0,0,,
848,1,0,0,1,0,0,,
849,"Commit 1549012 from Simon Willnauer in branch 'dev/trunk'
[ https://svn.apache.org/r1549012 ]
LUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.",0,0,0,0,0,,
850,"Commit 1549013 from Simon Willnauer in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1549013 ]
LUCENE-5362: IndexReader and SegmentCoreReaders now throw AlreadyClosedException if the refCount in incremented but is less that 1.",0,0,0,0,0,,
851,Fix the issue by pushing boosts from parent queries to child queries when the parent queries are flattened.,0,1,0,0,0,,
852,I clone the child queries before setting their boost so I don't break anything that expects them unchanged.,0,1,1,0,0,,
853,I'm not super happy that I have to clone the queries but it seemed like the simplest solution.,0,0,1,1,0,,
854,"Thanks Nik, your fix looks good!",0,0,1,0,0,,
855,"I don't think cloning the queries is an issue, it happens all the time when doing rewrites, and it's definitely better than modifying those queries in-place.",0,0,1,0,0,,
856,I'll commit it tomorrow if there is no objection.,0,0,0,0,1,,
857,"Commit 1556483 from Adrien Grand in branch 'dev/trunk'
[ https://svn.apache.org/r1556483 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.",0,0,0,0,0,,
858,"Commit 1556484 from Adrien Grand in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1556484 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.",0,0,0,0,0,,
859,"Commit 1556485 from Adrien Grand in branch 'dev/branches/lucene_solr_4_6'
[ https://svn.apache.org/r1556485 ]
LUCENE-5361: Fixed handling of query boosts in FastVectorHighlighter.",0,0,0,0,0,,
860,"While doing a final review, I noticed that you mistakenly modified the boost of the original query instead of the clone.",1,0,0,0,0,,
861,I took the liberty to fix it before committing but please let me know if this looks wrong to you.,0,0,0,0,1,,
862,"Committed, thanks!",0,0,0,0,1,,
863,Wonderful!,0,0,1,0,0,,
864,Thanks.,0,0,0,0,0,,
865,"First patch, 17 Oct 2013, quite rough, one nocommit.",0,1,0,1,0,,
866,"The latest benchmark results for doc id sets are here: http://people.apache.org/~jpountz/doc_id_sets.html
The patch uses EliasFanoDocIdSet for caching when EliasFanoDocIdSet.sufficientlySmallerThanBitSet returns true,
which is currently when load factor is at most 1/7, at about -0.85 log10 scale in the benchmark results.",0,1,0,0,0,,
867,"Otherwise it uses WAH8DocIdSet, the current behaviour.",0,0,1,0,0,,
868,Does this choice make good use of the benchmark results?,0,1,0,0,0,,
869,"To get the number of doc ids to be put in the cache, the patch checks for the type of the actual DocIdSet that is given, and uses FixedBitSet and OpenBitSet cardinality.",0,1,0,0,0,,
870,(Perhaps a similar method should be added to EliasFanoDocIdSet.),0,1,0,0,0,,
871,"In other cases, the patch falls back to WAH8DocIdSet.",0,0,0,1,0,,
872,"I added a DocIdSet argument to cacheImpl(), there is a nocommit for that.",0,1,0,0,0,,
873,"The patch also corrects a mistake in EliasFanoDocIdSet.sufficientlySmallerThanBitSet, the arguments should be int instead of long, just like the  EliasFanoDocIdSet constructor.",0,1,1,0,0,,
874,I like the idea of using the Elias-Fano doc id set given how it behaves in the benchmarks but it is tricky that it needs to know the size of the set in advance.,0,0,1,1,0,,
875,"In practice, the cache impls that you are going to have in CWF.cacheImpl are most likely QueryWrapperFilters, not FixedBitSets or OpenBitSets, so there is no way to know the exact size in advance.",0,1,0,1,0,,
876,"We could use DocIdSetIterator.cost but although it is recommended to implement this method by returning an upper bound on the number of documents in the set, it could return any number.",0,1,1,1,0,,
877,Do you think there would be a way to relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)?,0,1,0,0,0,,
878,"relax the Elias-Fano doc id set building process so that it could be built by providing an approximation of the number of docs in the set (at the cost of some compression loss)
The approximation would have to be a lower bound, i.e.",0,1,0,0,0,,
879,it might be higher than the actual number of documents.,0,0,1,0,0,,
880,"The EliasFanoEncoder reserves all the memory it needs at construction time, so the loss in compression would be roughly as noticable as the accuracy of the bound.",0,0,1,0,0,,
881,"DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.",0,0,0,1,0,,
882,Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?,0,0,0,0,0,,
883,"That is not immediately clear from the benchmark results, but it could be so.",0,0,0,0,0,,
884,"DocIdSetIterator.cost has another purpose, so it's not worthwhile to use it here I think.",0,0,0,1,0,,
885,I was mentioning it because it is the closest thing we have to a cardinality() which is available for every DocIdSet.,0,0,1,0,0,,
886,Does the faster build time of the EF DocIdSet (compared to WAH8 and FBS) allow for an extra FBS to be built?,0,0,0,0,0,,
887,"That is not immediately clear from the benchmark results, but it could be so.",0,0,0,0,0,,
888,My guess is that it is faster to build the in-memory doc id sets compared to consuming the (eg.,0,0,1,0,0,,
889,"QueryWrapper) filter we need to cache, so indeed, it may allow for building an additional FBS.",0,0,1,0,0,,
890,"Since WAH8DocIdSet computes it size (available thorugh cardinality()), maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents  I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set.",0,1,0,0,0,,
891,"This also reminds me that I should look into the building time of the WAH8 doc id set, there are probably things to improve...",1,0,0,0,0,,
892,I'm currently thinking it may be due to the fact that it keeps resizing buffers but I may be completely wrong.,1,0,0,0,0,,
893,... the closest thing we have to a cardinality() which is available for every DocIdSet.,0,0,0,0,0,,
894,"For a single term used as a filter there is IndexReader.docFreq(Term), and that does fit in here.",0,0,1,0,0,,
895,"(Ideally in this case the posting list could be copied from the index into the cache, but we're not there yet.)",0,0,0,1,0,,
896,Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?,0,1,0,0,0,,
897,There is also a TermFilter in the queries module.,0,1,0,0,0,,
898,I have not looked at the code yet.,0,0,0,0,0,,
899,"Is it necessary to move that to core so a check for that can be used here, too?",0,1,1,0,0,,
900,... maybe we could build a WAH8DocIdSet in any case and replace it with an EF doc id set when there are not many documents???,0,1,0,0,0,,
901,"For any case in which the cardinality can not easily be determined, that indeed would make sense from the benchmark.",0,0,1,0,0,,
902,I can try to update the benchmarks to add the building of an additional FBS before the EF doc id set.,0,1,0,0,0,,
903,Adding a single FBS build to the EF DocIdSet can be visualized in the benchmark for the build times.,0,1,0,0,0,,
904,"In that case the EF DocIdSet build result never gets above log(1) = 0, so an update of the benchmarks would not be needed.",0,0,1,0,0,,
905,Shall I add a check for QueryWrapperFilter.getQuery() being a TermQuery and then use docFreq() ?,0,1,0,0,0,,
906,I don't like much having support for specific filters based on instanceof calls.,0,0,0,1,0,,
907,"I really think the only two options are to consume the filter to cache twice, or to first load into memory in another filter impl and then load it again in an EF doc id set.",0,1,0,0,0,,
908,And I would go for option 2 since option 1 is likely going to be slower?,0,0,1,1,0,,
909,I don't like much having support for specific filters based on instanceof calls.,0,0,0,1,0,,
910,"Me neither, but that can be fixed by adding a docFreq() method to DocIdSet, as another hint to be used by CachingWrapperFilter.",0,1,1,0,0,,
911,This method should return the actual number of doc ids in the set when its return value >= 0.,0,1,0,0,0,,
912,"The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.",0,1,1,0,0,,
913,"I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.",0,0,1,1,0,,
914,I'll try and make another patch for this.,0,1,0,0,0,,
915,"The existing hint in DocIdSet is isCacheable(), its javadocs will need to updated since they need not mention BitSet anymore.",0,1,1,0,0,,
916,"Good point, I fixed it.",0,0,1,0,1,,
917,"I also prefer option 2, but I'd like to avoid using the other filter impl when possible by using the docFreq hint.",0,0,1,1,0,,
918,To me this feels like a big change compared to what it gives us.,0,0,0,1,0,,
919,I would prefer having another copy rather than adding this method.,0,1,1,1,0,,
920,"It felt like a big change when I started, but it was easier than I thought, have a look at the patch of 18 Oct.",0,0,1,1,0,,
921,"There are about 35 other places that directly extend DocIdSet or create a new one from an inline subclass, I have not checked these yet.",0,0,0,0,0,,
922,"This passes the current TestCachingWrapperFilter,  but there are no tests for this change yet.",0,0,1,1,0,,
923,"For small segments, maxDoc() <= 256,  this will use WAH8, would FBS better for those cases?",0,1,0,0,0,,
924,"The last choice for using the EF after the WAH8 was built is done using sufficientlySmallerThanBitSet because that was available, but I'm not really sure whether a smaller load factor should be used there.",0,1,1,1,0,,
925,"Looking again at the benchmark on how to solve building an EF docidset without knowing the number of values in advance, one solution would be to use a PFD docidset for that because it builds quickly and it has good next() performance.",0,1,1,0,0,,
926,The next() will be used once through the set to build the final docidset to be cached.,0,1,1,0,0,,
927,"However an even better way might be to use one or more temporary long arrays to store the incoming doc ids directly in FOR format, (without forming deltas and without an index).",0,1,1,0,0,,
928,This can be done because the maximum doc id value is known.,0,0,1,0,0,,
929,"While storing the doc ids, one can switch to an FBS on the fly when the total number of doc ids becomes too high.",0,0,1,0,0,,
930,The existing PackedInts code should be a nice fit for this.,0,0,1,0,0,,
931,"Since allocating the long arrays takes time, one can start with one array of say 1/512 of the maximum needed size, and continue into another (bigger) array as long as necessary or until an FBS is preferable.",0,1,1,1,0,,
932,"After some more thought on this I think using the WA8 docidset as in the patch is the best solution for now, because I think that gives the best building time for the expected cases.",0,1,1,0,0,,
933,I might add an EliasFanoEncoder constructor with only an upperBound argument for this case.,0,1,0,0,0,,
934,This would leave some room for adding more values (as in ArrayUtil.grow) and it would reorganize the encoded sequence to always use the latest number of values.,0,0,1,0,0,,
935,"Reorganizing the encoded sequence would be needed when the number of bits for encoding the lower values changes, and this is floor(log2(upperBound/numValues)) but never negative.",0,1,0,0,0,,
936,"(In a docidset for filtering the upperBound is normally the segment size, and the values are the doc ids.)",0,0,1,0,0,,
937,"About the patch of 20 Oct 2013:
There is an EliasFanoEncoder2 with a constructor with only an upperBound.",0,1,1,0,0,,
938,"This class delegates to EliasFanoEncoder and has very much the same methods, but not yet a common interface/superclass.",0,1,1,1,0,,
939,"It works by growing by a factor of 2 as necessary, and reencoding completely.",0,0,1,0,0,,
940,"This reencoding could be optimized for the lower bits by using PackedInts to block decode/encode, but I have not taken the time to implement that.",0,1,0,0,0,,
941,The EliasFanoDocIdSet has an extra constructor that takes only an upperbound that uses EliasFanoEncoder2.,0,1,1,0,0,,
942,"I ran some tests on this, it appears to work fine here.",0,0,1,0,0,,
943,A possibly misplaced assert in TestEliasFanoDocIdSet is corrected to make the test pass.,0,0,0,0,1,,
944,"CachingWrapperFilter uses the extra EliasFanoDocIdSet constructor when the number of doc ids in the set is not known,
and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS.",0,1,1,0,0,,
945,For the rest see my comments on the patch of 18 Oct.,0,0,0,0,0,,
946,"I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.",0,0,0,1,0,,
947,"The advance/next performance is only slightly less, again as expected.",0,0,1,0,0,,
948,"I could not measure build times, I expect it to just about double for the upperbound only constructor.",0,0,0,1,0,,
949,"I also did a little bit of perfomance testing, the memory usage of the upperbound only constructor is higher, as expected.",0,0,0,1,0,,
950,"Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding?",0,1,0,0,0,,
951,This is what the other (wah8/pfor) sets do.,0,0,1,0,0,,
952,"and it reverts to FBS when this EliasFanoDocIdSet is not sufficiently smaller than an FBS
Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?",0,1,1,0,0,,
953,"Instead of keeping the oversized encoder, maybe the encoder could be resized to its actual size when finished encoding?",0,1,0,0,0,,
954,This is what the other (wah8/pfor) sets do.,0,0,1,0,0,,
955,"I suppose you mean by recomputing the actually needed sizes for the long arrays of the encoder after all encoding is done, and then reallocating them?",0,1,0,0,0,,
956,That would certainly be possible.,0,0,1,0,0,,
957,I'll have a look at wah8/pfor for this.,0,0,0,0,0,,
958,Maybe pfor/wah8 would still be useful here since they efficiently encode almost full sets?,0,1,1,0,0,,
959,The problem is that none of the compressing sets is as good as FBS at advancing far in almost full sets.,0,0,0,1,0,,
960,"I'm working on this now, another patch is slowly on its way.",0,0,0,0,0,,
961,"About the patch of 21 Oct:
Adds EliasFanoEncoderUpperBound (instead of EliasFanoEncoder2) with only an upperBound argument to the constructor.",0,1,0,0,0,,
962,Adds a freeze() method to EliasFanoEncoder to reallocate to actual size.,0,1,0,0,0,,
963,"Both are used in EliasFanoDocIdSet, which now also reverts to using an FBS as needed.",0,1,1,0,0,,
964,Adapted TestEliasFanoDocIdSet to use EliasFanoDocIdSet randomly half of the time with a -1  numValues so it may use EliasFanoEncoderUpperBound instead of EliasFanoEncoder at first.,0,1,1,0,0,,
965,For the rest see my remarks about the patch of 20 Oct.,0,0,0,0,0,,
966,"On the patch of 22 Oct:
The previous patch contains a bug in the freeze() code, it allocates more than an FBS size to one of the encoding arrays.",1,0,0,0,0,,
967,This should fix it.,0,0,1,0,0,,
968,I've done some more performance measurements of this EliasFanoDocIdSet that allows only an upperBound to its constructor.,0,1,0,0,0,,
969,No surprising results but I'd like add an APL2 to  the benchmark program that was derived from the (Test)DocIdSetBenchmark at LUCENE-5236 and LUCENE-5101 and post it.,0,1,1,0,0,,
970,"Adrien, can I assume an APL 2 on the first DocIdSetBenchmark at LUCENE-5101?",0,1,0,0,0,,
971,"The patch here is getting a little overloaded, so shall I open a separate issue for the EliasFanoDocIdSet that allows only an upperBound to its constructor?",0,1,0,0,0,,
972,"I opened a github pull request for the latest patch:
https://github.com/apache/lucene-solr/pull/19",0,0,0,0,0,,
973,Since LUCENE-5983 CachingWrapperFilter uses RoaringDocIdSet.,0,0,0,0,0,,
974,Are there any advantages for EliasFanoDocIdSet left for use there?,1,0,0,0,0,,
975,"Although the elias-fano set is smaller in the sparse case, it's true that RoaringDocIdSet tends to be faster to build and to iterate on.",0,0,1,0,0,,
976,So overall I think the RoaringDocIdSet has a better trade-off indeed.,0,0,1,0,0,,
977,"Github user PaulElschot closed the pull request at:
 https://github.com/apache/lucene-solr/pull/19",0,0,0,0,0,,
978, it turned out to be easier than I expected: I just tapped into the existing logic that ShingleFilter has for handling holes between tokens.,1,"alternative, pro",0,1,1,0,0
979,"+1, patch looks good.",0,0,1,0,0,,
980,"+1 to your suggestion about ShingleFilterTest.TestTokenStream:
// TODO: merge w/ CannedTokenStream?",0,0,1,0,0,,
981,Thanks Steve!,0,0,0,0,0,,
982,Here's a new patch w/ that TODO done ...,0,1,0,0,0,,
983,I think it's ready.,0,0,1,0,0,,
984,"Commit 1524117 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1524117 ]
LUCENE-5180: ShingleFilter creates shingles from trailing holes",0,0,0,0,0,,
985,"Commit 1524120 from Michael McCandless in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1524120 ]
LUCENE-5180: ShingleFilter creates shingles from trailing holes",0,0,0,0,0,,
986,"Commit 1524122 from Michael McCandless in branch 'dev/trunk'
[ https://svn.apache.org/r1524122 ]
LUCENE-5180: move CHANGES entry",0,0,0,0,0,,
987,"java.lang.IndexOutOfBoundsException: start 9999, end 10004, s.length() 10000
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:453)
	at java.lang.StringBuilder.append(StringBuilder.java:179)
	at org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.append(DefaultPassageFormatter.java:135)
	at org.apache.lucene.search.postingshighlight.DefaultPassageFormatter.format(DefaultPassageFormatter.java:79)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightField(PostingsHighlighter.java:435)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:353)
	at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:268)",0,0,0,0,0,,
988,Can you show a real usecase for a document matching beyond content.length()?,0,1,0,0,0,,
989,"Your patch artificially creates an out-of-bound Passage, but I think it's better if we can see a real usecase, e.g.",0,1,1,0,0,,
990,maybe a combination of TokenFilters may cause that?,1,0,0,0,0,,
991,But if e.g.,0,0,0,0,0,,
992,"the app indexed content1 but then tries to highlight content2, I don't think that's a supported usecase...",1,0,0,0,0,,
993,Please find attached another test case.,0,1,0,0,0,,
994,It is sort of bad luck to run into this in a real use case but it actually happened to me.,1,0,0,0,0,,
995,Interesting.,0,0,1,0,0,,
996,"FYI, I will not be available in the next 2 weeks, and haven't reproduced it yet.",0,0,0,0,0,,
997,"If no one assigns himself to the issue, I will when I'm back.",0,0,0,0,0,,
998,I have reproduced it with Manuel's test,0,0,0,0,0,,
999,Attached is just a combined patch of Manuel's 2 patches.,0,0,0,0,0,,
1000,There is definitely bug(s) here.,1,0,0,0,0,,
1001,"As far as the fix, to me the big question (I put it in a nocommit to his test case), is if formatter classes should really have to deal with these cases.",1,0,0,0,0,,
1002,OK here's a patch.,0,1,0,0,0,,
1003,"the cause of the bug is that we only know startOffsets are always increasing (the algorithm relies on this, and merges them across terms).",1,0,0,0,0,,
1004,"So we cannot safely terminate when end >= limit (only start >= limit), but we don't have to confuse the formatter with the cases of terms that 'span' the limit.",1,0,0,0,0,,
1005,Hmm so this means we may pick a truncated passage to present?,0,1,0,0,0,,
1006,I suppose it's unlikely to score well ... just seems bad though.,0,0,0,1,0,,
1007,"Wait, couldn't we fix passageQueue.offer(current) to not offer it if current.endOffset == contentLength?",0,1,0,0,0,,
1008,"Improved patch, thank you Mike",0,1,0,0,0,,
1009,OK let's not try to address that on this issue ...,0,0,0,0,1,,
1010,I'm not even sure it needs fixing.,0,0,0,1,0,,
1011,It ought to be rare-ish that a truncated passage is selected.,0,0,0,0,0,,
1012,There was a bug in my patch: I added another unit test for this!,1,1,0,0,0,,
1013,I think its ready.,0,0,1,0,0,,
1014,"+1
Tricky!",0,0,1,0,0,,
1015,"Commit 1513207 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1513207 ]
LUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException",0,0,0,0,0,,
1016,"Commit 1513231 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1513231 ]
LUCENE-5166: PostingsHighlighter fails with IndexOutOfBoundsException",0,0,0,0,0,,
1017,Thank you Manuel!,0,0,0,0,0,,
1018,Thank you for the quick help!,0,0,0,0,0,,
1019,"I just found another problem here: If we have both, matches that do and matches that don't span the content boundary the formatter is asked to highlight the spanning match.",1,0,0,0,0,,
1020,Please find attached additional tests and a possible fix for this.,0,1,0,0,0,,
1021,Hi Manuel: thank you!,0,0,0,0,0,,
1022,"Another bug, or a bug in my fix to the other bug 
I'll investigate deeper in a bit.",0,0,0,0,0,,
1023,"Manuel: your fix is correct, thank you.",0,0,1,0,0,,
1024,"To explain: I had totally forgotten about this little loop on tf within the passage (i had removed this optimization in LUCENE-4909, which didnt turn out to work that great, so wasn't committed).",0,1,0,1,0,,
1025,"We might at some point want to still just remove the optimization just based on the reason that it makes this thing more complicated, it was just intended to speed up the worst case (where someone has very common stopwords and stuff like that).",0,1,0,1,0,,
1026,"But for now to complete the bugfix, we should commit your patch (LUCENE-5166-revisited.patch).",0,0,0,0,1,,
1027,"Commit 1514367 from Robert Muir in branch 'dev/trunk'
[ https://svn.apache.org/r1514367 ]
LUCENE-5166: also fix and test this case where tf > 1 within the passage for a term",0,0,0,0,0,,
1028,"Commit 1514379 from Robert Muir in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1514379 ]
LUCENE-5166: also fix and test this case where tf > 1 within the passage for a term",0,0,0,0,0,,
1029,Thank you again!,0,0,0,0,0,,
1030,4.5 release -> bulk close,0,0,0,0,1,,
1031,"Commit 1594464 from Robert Muir in branch 'dev/branches/lucene5666'
[ https://svn.apache.org/r1594464 ]
LUCENE-5166: clear most nocommits, move ord/rord to solr (and speed them up), nuke old purging stuff",0,0,0,0,0,,
1032,First draft of patch attached.,0,1,0,0,0,,
1033,Let me know how this looks.,0,0,0,0,0,,
1034,Thank you.,0,0,0,0,0,,
1035,Thanks Tim!,0,0,0,0,0,,
1036,This looks like a great improvement: I like factoring out calcDistance from calcSimilarity.,0,0,1,0,0,,
1037,And I like that we now take raw into account when figuring out which comparison to make to accept the term or not.,0,0,1,0,0,,
1038,Maybe we could improve it a bit: if raw is true we don't need to calcSimilarity right?,0,1,0,0,0,,
1039,For my sanity ... where exactly was the bug in the original code?,0,0,0,0,0,,
1040,Thank you for your quick response!,0,0,0,0,0,,
1041,"I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.",0,1,1,0,0,,
1042,Let me know if I'm missing something.,0,0,0,0,0,,
1043,The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.,1,0,0,0,0,,
1044,"So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.",1,0,0,0,0,,
1045,"In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.",1,0,0,0,0,,
1046,Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!,0,1,0,1,0,,
1047,"I, too, was hoping to avoid calcSimilarity if raw is true, but I think we need it to calculate the boost.",0,1,1,0,0,,
1048,Let me know if I'm missing something.,0,0,0,0,0,,
1049,"Ahh, you're right ...",0,0,1,0,0,,
1050,I missed that.,0,0,0,0,0,,
1051,OK.,0,0,0,0,0,,
1052,The bug in the original code was that FilteredTermsEnum sets minSimilarity to 0 when the user-specified minSimilarity is >= 1.0f.,1,0,0,0,0,,
1053,"So, in SlowFuzzyTermsEnum, similarity (unless it was Float.NEGATIVE_INFINITY) was typically > minSimilarity no matter its value.",1,0,0,0,0,,
1054,"In other words, when the client code made the call with minSimilarity >=1.0f, that value was correctly recorded in maxEdits, but maxEdits wasn't the determining factor in whether SlowFuzzyTerms accepted a term.",1,0,0,0,0,,
1055,"Oh, I see: FuzzyTermsEnum does this in its ctor, and SlowFuzzyTermsEnum extends that.",0,0,0,0,0,,
1056,Now I understand the bug ... thanks.,0,0,0,0,0,,
1057,Doing an explicit levenshtein calculation here sort of defeats the entire purpose of having levenshtein automata at all!,0,1,0,1,0,,
1058,"But this fix only applies in cases (edit distance > 2) where automaton's don't, I think?",0,0,0,1,0,,
1059,(The fixes are to LinearFuzzyTermsEnum).,0,0,0,0,0,,
1060,"Robert,
I agree that it appears to, but if you want a distance > 2, the current levenshtein automaton doesn't allow that (http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html?showComment=1303598602291#c3051732466052117784).",0,0,1,1,0,,
1061,The classic QueryParser silently converts distances > 2 to 2.,0,0,0,1,0,,
1062,"If I understand SlowFuzzyQuery correctly, it uses the levenshtein automaton for distances <= 2, but it runs brute force if the distance is > 2.",1,0,0,0,0,,
1063,My personal preference would be to undeprecate SlowFuzzyQuery (certainly leave it in the sandbox) because it offers a capability that the current levenshtein automaton doesn't.,0,1,1,0,0,,
1064,"In cases where the indices are very large, it wouldn't make sense to expose distance > 2 capability; but on small to medium indices, there are use cases that require it.",0,0,1,1,0,,
1065,Updated patch.,0,0,0,0,1,,
1066,Added short circuits to avoid calculating similarity when not necessary.,0,1,1,0,0,,
1067,Corrected term.length to text.length in calcSimilarity call.,0,1,0,0,0,,
1068,Activated old tests that test for edit distance matches where the edit distances are greater than the query term length.,0,1,0,0,0,,
1069,"Thanks Tim, new patch looks great!",0,0,1,0,0,,
1070,Thanks Tim!,0,0,0,0,0,,
1071,"Mike,
  Thank you for your feedback and quick response!",0,0,0,0,0,,
1072,Bulk close resolved 4.4 issues,0,0,0,0,1,,
1073,patch.,0,1,0,0,0,,
1074,looks good!,0,0,1,0,0,,
1075,"New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it's not being used by an IW ...",0,1,0,0,0,,
1076,I think it's ready.,0,0,1,0,0,,
1077,+1!,0,0,1,0,0,,
1078,"[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478726
LUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk",0,0,0,0,0,,
1079,"[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478730
LUCENE-4976: use single file to hold PersistentSnapshotDeletionPolicy state on disk",0,0,0,0,0,,
1080,"[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478854
LUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit",0,0,0,0,0,,
1081,"[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1478855
LUCENE-4976: fix Solr IndexDeletionPolicy impls to handle empty commits onInit",0,0,0,0,0,,
1082,"[trunk commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1479394
LUCENE-4976: add missing sync / delete old save files",0,0,0,0,0,,
1083,"[branch_4x commit] mikemccand
http://svn.apache.org/viewvc?view=revision&revision=1479395
LUCENE-4976: add missing sync / delete old save files",0,0,0,0,0,,
1084,Bulk close resolved 4.4 issues,0,0,0,0,1,,
1086,facet collections have IntIterator interface.,0,1,1,0,0,,
1087,And so the method should be something like IntIterator getChildren(int ordinal)?,0,1,0,0,0,,
1088,"I don't know much about IntIterator, but I surmise it'll do good enough.",0,1,1,0,0,,
1089,Added TaxoReader.getChildren(int ordinal) and corresponding test.,0,1,0,0,0,,
1090,"I also migrated PrintTaxonomyStats to use getChildren, which removed all mentions of ParallelTaxonomyArrays from it.",0,1,0,0,0,,
1091,Shouldn't next() throw NoSuchElementException if child is already INVALID_ORDINAL?,1,0,0,0,0,,
1092,"It shouldn't ever return INVALID_ORDINAL, right?",1,0,0,0,0,,
1093,"Ie, caller screwed up and called next w/o calling hasNext first.",1,0,0,0,0,,
1094,"I looked at other IntIterator impls and none throw NoSuchElementException, so I thought it's best to follow.",0,0,1,0,0,,
1095,"Also, since it returns ordinals, it's kind of ok to return INVALID_ORDINAL.",0,0,1,0,0,,
1096,I wished that we had a simple IntIterator interface with only next() for this case...,0,1,0,0,0,,
1097,I don't mind throwing it though.,0,0,1,0,0,,
1098,What do you think?,0,0,0,0,0,,
1099,"Or we could return a not-Java-iterator, that just has int next() that returns INVALID_ORDINAL when it's done...",0,1,0,0,0,,
1100,"Woops, our comments crossed...",0,0,0,0,0,,
1101,"I wished that we had a simple IntIterator interface with only next() for this case...
+1, I think that's best.",0,0,1,0,0,,
1102,"I wanted to avoid introducing another class (facet collections already use this primitive IntIterator), but maybe a ChildrenIterator with next() is simplest.",0,1,1,1,0,,
1103,I'll look into it.,0,0,0,0,0,,
1104,Patch with ChildrenIterator,0,1,0,0,0,,
1105,"+1, thanks Shai!",0,0,1,0,0,,
1106,"[trunk commit] shaie
http://svn.apache.org/viewvc?view=revision&revision=1464730
LUCENE-4897: add a sugar API for traversing categories",0,0,0,0,0,,
1107,"[branch_4x commit] shaie
http://svn.apache.org/viewvc?view=revision&revision=1464743
LUCENE-4897: add a sugar API for traversing categories",0,0,0,0,0,,
1108,Committed to trunk and 4x.,0,0,0,0,1,,
1109,Closed after release.,0,0,0,0,1,,
1085,"I think that we should use a primitive iterator, e.g.",0,1,0,0,0,,
1110,"Here's a preliminary patch, implementing UninvertedFilterReader and cutting over DocTermOrdsRangeFilter to use it.",0,1,0,0,0,,
1111,"Some questions:

which package should this stuff be in?",1,0,0,0,0,,
1112,"FieldCache is in o.a.l.search, and the reader is in o.a.l.index.",1,0,0,0,0,,
1113,there are a bunch of FieldCache-specific queries and filters.,1,0,0,0,0,,
1114,Can these just be reworked to be DV-specific instead?,0,1,0,0,0,,
1115,"can we consolidate the various Ints, Floats, Shorts etc FieldCache interfaces into NumericDocValues?",0,1,0,0,0,,
1116,this still uses the global FieldCache.,0,0,0,0,0,,
1117,Should the caches be moved to the readers instead?,0,1,0,0,0,,
1118,"But I assume the plan is to remove aol.search.FieldCache entirely
I wasn't going to initially - just make it package private.",0,1,0,0,0,,
1119,"But thinking about it, can we just put a map of fieldnames->XXXDocValues in a threadlocal on UninvertingFilterReader (like on SegmentCoreReaders)?",0,1,0,0,0,,
1120,That makes things a lot cleaner.,0,0,1,0,0,,
1121,"We'd need a way to purge the map, though.",1,0,0,0,0,,
1122,"Maybe we should make a branch for this
Having just spent an hour or so trying to cut things over, yes, that's probably a good idea   It touches a lot of the codebase.",0,0,1,0,0,,
1123,Should simplify things like SortField a lot though.,0,0,1,0,0,,
1124,"
I was wondering about how to do this.",1,0,0,0,0,,
1125,"We could add an optional Map<String, DocValuesType> parameter to the UFR constructor - if it's absent, then you can uninvert any field you like, at the risk of fieldcache-insanity.",0,1,0,1,0,,
1126,Why allow this?,0,0,0,0,0,,
1127,I don't think we should do this.,0,0,0,1,0,,
1128,it also prevents it from working with anything that checks fieldinfos.,0,0,0,1,0,,
1129,Maybe for the moment we should just get FieldCache moved into UFR and worry about passing CheckIndex in another issue?,0,1,0,0,0,,
1130,Unless you think that we'll end up having to make major changes if we don't build this in from the beginning.,0,0,0,1,0,,
1131,"I'm new to a lot of this part of the codebase, so all advice is very welcome here
I think its a pretty big deal that a filterreader pass checkindex, otherwise its corrupt, and will behave in a corrupt way.",1,0,0,0,0,,
1132,there is also nothing to prevent someone from calling IW.addIndexes(IR) with it and making a truly corrupt index.,1,0,0,0,0,,
1133,"I'm willing to budge on this though, if we want to add this filterreader that doesnt pass checkindex, its ok to me as long as IndexWriter.addIndexes itself internally calls checkIndex on the incoming filterreader to prevent corruption.",0,1,1,0,0,,
1134,"New patch, copying the uninvert logic from FieldCacheImpl into a new Uninverter class (I haven't worked out how to implement the NumericDocValues uninverter yet - pointers welcome), adding a cache to the UninvertingFilterReader itself and checking uninversions against a map of fieldnames to DocValuesTypes passed in as a constructor argument.",0,1,0,0,0,,
1135,"New patch, with a NumericDocValues uninverter.",0,1,0,0,0,,
1136,Also a basic test.,0,1,0,0,0,,
1137,NumericDocValues doesn't work with 32-bit values yet (the test for IntField fails),"issue, alternative",1,1,0,0,0,
1138,Something that will uninvert a LongField or DoubleField into a NumericDocValues representation.,1,0,0,0,0,,
1139,"OK, here's a patch that attempts to solve the problem of 32-bit vs 64-bit numeric values.",0,1,0,0,0,,
1140,"I don't really like it, but it seems to work, so it's a start.",0,0,1,1,0,,
1141,Users of UninvertedFilterReader have to specify up-front the width of numeric fields they wish to uninvert.,0,1,0,0,0,,
1142,"This is hacky for any number of reasons, not the least of which is I have to define a whole new set of DocValuesTypes on Uninverter, which is a lot less elegant than just using the existing FieldInfo ones.",0,0,0,1,0,,
1143,"There doesn't seem to be a good way of detecting this at uninvert-time from the FieldInfo data, though.",1,0,0,0,0,,
1144,"We could assume 64-bit values and fall back to 32-bit if we encounter an exception, but I worry that we're getting a performance hit here for every 32-bit field.",0,1,0,1,0,,
1145,"Alan i didnt look closely, but could GrowableWriter be used to avoid the hack?",0,1,0,0,0,,
1146,that way you only take up space relevant to the bits you are actually using...,0,0,1,0,0,,
1147,Actually I've convinced myself that checking for a NumberFormatException is a better way of doing this...,0,1,1,0,0,,
1148,could GrowableWriter be used to avoid the hack?,0,1,0,0,0,,
1149,"I don't think so, because this is to do with reading the already-indexed bytes and converting them back to longs or ints.",0,0,0,1,0,,
1150,Unless I've got the wrong end of the stick here.,0,0,0,0,0,,
1151,Attached a patch that adds an inner class IntIterator that is consistent with the contract of a java.util.Iterator but next() returns a primitive int.,0,1,0,0,0,,
1152,I tested basic expected usage.,0,0,0,0,0,,
1153,"I figured being close to a standard iterator would be more familiar/friendly, although if it worked similar to DocIdSetIterator it would be faster since the caller would check for the sentinal value instead of calling hasNext() (which wouldn't exist).",0,1,1,0,0,,
1154,"I forgot to make the methods public, which I'll fix when it gets committed.",0,1,0,0,0,,
1155,I don't think we need to mimic java's Iterator?,0,0,0,0,0,,
1156,"Ie, it's fine to have only next()...
Also, the iterator is wrong if a rehash happens right?",0,0,0,1,0,,
1157,"Can you add to the javadocs that the set should not be modified while the iterator is in use, except using the iterator's remove method (like Java's HashMap)?",0,1,0,0,0,,
1158,Mike: sounds good.,0,0,1,0,0,,
1159,But you know what?,0,0,0,0,0,,
1160,"Iteration by the client code on this data structure is actually so darned easy; maybe this set iterator isn't really needed after all, or could be supplied with a comment so it's clear how to do it.",0,1,1,0,0,,
1161,"SentinalIntSet set = ...
for (int v : set.keys) {
  if (v == set.emptyVal)
    continue;
  //use v...
}


piece-o-cake",0,1,0,0,0,,
1162,"maybe this set iterator isn't really needed after all
+1",0,0,1,0,0,,
1163,So instead of adding to the API I decided to enhance the documentation to make it more clear how to use this class.,0,1,0,0,1,,
1164,"(attached)
Of note I added a warning of the potential unsuitability of the lack of hashing of the key.",0,1,0,0,0,,
1165,"I committed better javadocs, including sample code to iterate the values.",0,0,0,0,1,,
1166,Closing issue as won't fix (for now).,0,0,0,0,1,,
1167,"If at some point we want an iterator, the patch is here and it can be re-considered.",0,1,0,0,0,,
1168,Closed after release.,0,0,0,0,1,,
1205,"
Should we create a branch.",0,0,0,0,0,,
1206,"I also have some ideas to fix...
We can, or maybe just use trunk?",0,0,0,0,0,,
1207,I don't think the patch makes these factories any worse.,0,0,1,0,0,,
1208,+1 to commit Robert's patch to trunk and iterate there.,0,0,1,0,0,,
1209,Or you can assign the issue.,0,0,0,0,0,,
1210,I just have family coming into town today and won't have any time to do any of the work to help out with this one.,0,0,0,0,0,,
1211,I thought i knew what was involved with this stuff pretty well but I grossly underestimated the amount of work to even throw the exception,0,0,0,0,0,,
1212,I have no preference on how to proceed.,0,0,0,0,0,,
1213,"I just dont want to download such a large patch, modify the sources and upload it again.",0,0,0,0,0,,
1214,"especially as TortoiseSVN and other clients depend on order of files in filesystem, the order of created patches is different, too.",0,0,0,0,0,,
1215,So its impossible to see any change in comparison to earlier patches.,0,0,0,0,0,,
1216,"As we don't intend to release trunk soon: If all tests pass, can you simply commit as a first step, Robert?",0,0,0,0,0,,
1217,+1 on the current patch.,0,0,1,0,0,,
1218,"We can open further issues to unfuck ResourceLoaderAware (it should be removed, too and the ResourceLoader should be passed to the ctor, too).",0,1,0,0,0,,
1219,I committed this to trunk.,0,0,0,0,1,,
1220,Can we be extra careful to use LUCENE-4877: in all commit messages so when its time to backport its easy to find the revisions?,0,0,0,0,0,,
1221,Thanks in advance for any improvements!,0,0,0,0,0,,
1222,"getFloat() - well, only one factory (NumericPayloadTokenFilterFactory) could use it now, but maybe add it for completeness?",0,1,1,0,0,,
1223,I just remembered.,0,0,0,0,0,,
1224,I think the reversewildcardfactory in solr has one of these too.,0,0,0,0,0,,
1225,"Patch fixing these minor nits:

TestMappingCharFilterFactory's factory could switch to being instantiated using the charFilterFactory() method
EdgeNgramTokenizerFactory's gram size constants are pulled from EdgeNgramTokenFilter instead of EdgeNgramTokenizer
LimitTokenCountFilterFactory's maxTokenCount param should be required; this is a pre-existing problem though
PatternTokenizerFactory's group param should use the getInt() method with a default of -1.",0,1,0,0,0,,
1226,"Actually, I was wrong about LimitTokenCountFilterFactory - it already has a test in place to insure reporting of missing required maxTokenCount param.",0,0,1,0,0,,
1227,Committing shortly.,0,0,0,0,1,,
1228,Thanks Steve!,0,0,0,0,0,,
1229,"This patch adds more param parsing methods to AbstractAnalysisFactory, including get(), require(), getFloat(), getChar(), and getSet(), and changed all analysis factories to use them where appropriate.",0,1,1,0,0,,
1230,I don't like the 4-arg required param getXXX() methods in AbstractAnalysisFactory - 4th param as false means required???,0,0,0,1,0,,
1231,- maybe these could be converted to getRequiredXXX() ?,0,1,0,0,0,,
1232,"I implemented these as require(), requireXXX(), etc.",0,1,0,0,0,,
1233,"Tests all pass, and precommit's happy.",0,0,1,0,0,,
1234,Committing shortly.,0,0,0,0,1,,
1235,"Thanks for cleaning this up Steve, much nicer.",0,0,1,0,0,,
